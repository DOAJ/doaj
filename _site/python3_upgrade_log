RefactoringTool: Skipping optional fixer: buffer
RefactoringTool: Skipping optional fixer: idioms
RefactoringTool: Skipping optional fixer: set_literal
RefactoringTool: Skipping optional fixer: ws_comma
RefactoringTool: No changes to ./setup.py
RefactoringTool: No changes to ./deploy/doaj_gunicorn_config.py
RefactoringTool: No changes to ./deploy/doaj_test_gunicorn_config.py
RefactoringTool: No changes to ./deploy/lambda/alert_backups_missing.py
RefactoringTool: Refactored ./deploy/new_infrastructure_migration_scripts/copy_data_from_live_to_new.py
RefactoringTool: No changes to ./deploy_old/doaj_gunicorn_config.py
RefactoringTool: No changes to ./deploy_old/doaj_test_gunicorn_config.py
RefactoringTool: Refactored ./deploy_old/esbackup.py
RefactoringTool: Refactored ./deploy_old/mount_s3fs.py
--- ./deploy/new_infrastructure_migration_scripts/copy_data_from_live_to_new.py	(original)
+++ ./deploy/new_infrastructure_migration_scripts/copy_data_from_live_to_new.py	(refactored)
@@ -32,9 +32,9 @@
                 bn += json.dumps({'index':{'_index':index_name, '_type': tp, '_id': record['id']}}) + '\n'
                 bn += json.dumps(record) + '\n'
             s = requests.post(new_index + '/_bulk', data=bn)
-            print tp
-            print processed[tp]
-            print s.status_code
+            print(tp)
+            print(processed[tp])
+            print(s.status_code)
             records = []
         processed[tp] += len(res['hits']['hits'])
         for r in res.get('hits',{}).get('hits',[]):
@@ -43,4 +43,4 @@
             nxt = requests.get(old_index + '/_search/scroll?scroll=' + scroll_minutes + '&scroll_id=' + res['_scroll_id'])
             res = nxt.json()
 
-print json.dumps(processed, indent=2)
+print(json.dumps(processed, indent=2))
--- ./deploy_old/esbackup.py	(original)
+++ ./deploy_old/esbackup.py	(refactored)
@@ -64,12 +64,12 @@
     done.append('performing backups via index query')
     try:
         rs = requests.get(location + '/' + '_status')
-        indices = rs.json['indices'].keys()
+        indices = list(rs.json['indices'].keys())
         for index in indices:
             if index not in ignore:
                 try:
                     ts = requests.get(location + '/' + index + '/_mapping')
-                    types = ts.json[index].keys()
+                    types = list(ts.json[index].keys())
                     for t in types:
                         try:
                             rh = requests.get(location + '/' + index + '/' + t + '/_search?q=*&size=0')
@@ -110,6 +110,6 @@
     pass
 
 # uncomment to print when done    
-print done
+print(done)
 
 
--- ./deploy_old/mount_s3fs.py	(original)
+++ ./deploy_old/mount_s3fs.py	(refactored)
@@ -80,44 +80,44 @@
 
     if args.permanent:
         if permanently_mounted:
-            print "S3FS permanent mount for bucket {} already exists, not doing anything.\n" \
-                  'If you think the /etc/fstab entry is wrong, remove it manually and rerun this script with --permanent.'
+            print("S3FS permanent mount for bucket {} already exists, not doing anything.\n" \
+                  'If you think the /etc/fstab entry is wrong, remove it manually and rerun this script with --permanent.')
             sys.exit(0)
 
         fstab_entry = 's3fs#{} {} fuse _netdev,allow_other,{},url=https://s3.amazonaws.com 0 0'.format(bucket, app.config['S3FS_MOUNT_DIR'], credentials_arg)
         if '"' in fstab_entry:
             raise S3FSScriptException("You can't have \" (double quotes) in any of the environment variables that control the S3 mount.")
-        print
-        print 'Setting up permanent mount in /etc/fstab - you may be asked for your sudo password now.'
-        print
+        print()
+        print('Setting up permanent mount in /etc/fstab - you may be asked for your sudo password now.')
+        print()
         subprocess.check_call('echo "{}" | sudo tee -a /etc/fstab'.format(fstab_entry), shell=True)
-        print
-        print '^ the above entry has been appended to your /etc/fstab'
+        print()
+        print('^ the above entry has been appended to your /etc/fstab')
         permanently_mounted = check_permanent_mount(bucket)
 
     if args.unmount:
         try:
             subprocess.check_call(["sudo", 'umount', app.config['S3FS_MOUNT_DIR']])
         except subprocess.CalledProcessError:
-            print
-            print 'Unmounting failed. This can happen if you try to unmount too soon after you mounted. If the error is something like "device busy", wait for 5-10 seconds and rerun the unmount.'
-            print
+            print()
+            print('Unmounting failed. This can happen if you try to unmount too soon after you mounted. If the error is something like "device busy", wait for 5-10 seconds and rerun the unmount.')
+            print()
             raise
 
-       RefactoringTool: No changes to ./deploy_old/lambda/alert_backups_missing.py
RefactoringTool: No changes to ./doajtest/bootstrap.py
RefactoringTool: No changes to ./doajtest/clonelive.py
RefactoringTool: Refactored ./doajtest/helpers.py
 print 'S3FS UNmounted bucket {} from directory {} succesfully'.format(bucket, app.config['S3FS_MOUNT_DIR'])
+        print('S3FS UNmounted bucket {} from directory {} succesfully'.format(bucket, app.config['S3FS_MOUNT_DIR']))
         sys.exit(0)
 
     # let's mount then
     if permanently_mounted:
-        print
-        print "Mounting using the permanent mount in /etc/fstab.\n" \
+        print()
+        print("Mounting using the permanent mount in /etc/fstab.\n" \
               "If this script completes successfully but you can't use the mountpoint as an S3 file system, " \
-              "check your /etc/fstab and compare against your local S3FS_* DOAJ config vars, then report a bug."
+              "check your /etc/fstab and compare against your local S3FS_* DOAJ config vars, then report a bug.")
 
         subprocess.check_call(['sudo', 'mount', app.config['S3FS_MOUNT_DIR']])
     else:
-        print
-        print 'Mounting with temporary mount.'
+        print()
+        print('Mounting with temporary mount.')
         subprocess.check_call(["s3fs", bucket, app.config['S3FS_MOUNT_DIR'], '-o', credentials_arg, '-o', 'url=https://s3.amazonaws.com'])
 
-    print 'S3FS mounted bucket {} at directory {} succesfully'.format(bucket, app.config['S3FS_MOUNT_DIR'])
+    print('S3FS mounted bucket {} at directory {} succesfully'.format(bucket, app.config['S3FS_MOUNT_DIR']))
--- ./doajtest/helpers.py	(original)
+++ ./doajtest/helpers.py	(refactored)
@@ -62,39 +62,39 @@
     :return: nothing, prints results to STDOUT
     """
     differ = dictdiffer.DictDiffer(d1, d2)
-    print 'Added :: keys present in {d1} which are not in {d2}'.format(d1=d1_label, d2=d2_label)
-    print differ.added()
-    print
-    print 'Removed :: keys present in {d2} which are not in {d1}'.format(d1=d1_label, d2=d2_label)
-    print differ.removed()
-    print
-    print 'Changed :: keys which are the same in {d1} and {d2} but whose values are different'.format(d1=d1_label, d2=d2_label)
-    print differ.changed()
-    print
+    print('Added :: keys present in {d1} which are not in {d2}'.format(d1=d1_label, d2=d2_label))
+    print(differ.added())
+    print()
+    print('Removed :: keys present in {d2} which are not in {d1}'.format(d1=d1_label, d2=d2_label))
+    print(differ.removed())
+    print()
+    print('Changed :: keys which are the same in {d1} and {d2} but whose values are different'.format(d1=d1_label, d2=d2_label))
+    print(differ.changed())
+    print()
 
     if differ.changed():
-        print 'Changed values :: the values of keys which have changed. Format is as follows:'
-        print '  Key name:'
-        print '    value in {d1}'.format(d1=d1_label)
-        print '    value in {d2}'.format(d2=d2_label)
-        print
+        print('Changed values :: the values of keys which have changed. Format is as follows:')
+        print('  Key name:')
+        print('    value in {d1}'.format(d1=d1_label))
+        print('    value in {d2}'.format(d2=d2_label))
+        print()
         for key in differ.changed():
-            print ' ', key + ':'
-            print '   ', d1[key]
-            print '   ', d2[key]
-            print
-        print
+            print(' ', key + ':')
+            print('   ', d1[key])
+            print('   ', d2[key])
+            print()
+        print()
 
     if print_unchanged:
-        print 'Unchanged :: keys which are the same in {d1} and {d2} and whose values are also the same'.format(d1=d1_label, d2=d2_label)
-        print differ.unchanged()
+        print('Unchanged :: keys which are the same in {d1} and {d2} and whose values are also the same'.format(d1=d1_label, d2=d2_label))
+        print(differ.unchanged())
 
 def load_from_matrix(filename, test_ids):
     if test_ids is None:
         test_ids = []
     with open(paths.rel2abs(__file__, "matrices", filename)) as f:
         reader = csv.reader(f)
-        reader.next()   # pop the header row
+        next(reader)   # pop the header row
         cases = []
         for row in reader:
             if row[0] in test_ids or len(RefactoringTool: Refactored ./doajtest/make_matrix.py
RefactoringTool: Refactored ./doajtest/demo_scripts/async_workflow_notifications_demo.py
RefactoringTool: Refactored ./doajtest/fixtures/__init__.py
RefactoringTool: No changes to ./doajtest/fixtures/accounts.py
RefactoringTool: No changes to ./doajtest/fixtures/applications.py
RefactoringTool: Refactored ./doajtest/fixtures/article.py
RefactoringTool: No changes to ./doajtest/fixtures/background.py
RefactoringTool: No changes to ./doajtest/fixtures/bibjson.py
RefactoringTool: No changes to ./doajtest/fixtures/common.py
RefactoringTool: No changes to ./doajtest/fixtures/dois.py
RefactoringTool: No changes to ./doajtest/fixtures/editors.py
RefactoringTool: No changes to ./doajtest/fixtures/issns.py
RefactoringTool: No changes to ./doajtest/fixtures/journals.py
RefactoringTool: No changes to ./doajtest/fixtures/provenance.py
RefactoringTool: Refactored ./doajtest/fixtures/snapshots.py
test_ids) == 0:
--- ./doajtest/make_matrix.py	(original)
+++ ./doajtest/make_matrix.py	(refactored)
@@ -41,7 +41,7 @@
         cval = combo[filter["field"]]
         if cval != filter["value"]:
             continue
-        for cfield, values in filter["constraints"].iteritems():
+        for cfield, values in filter["constraints"].items():
             if combo[cfield] not in values:
                 return False
     return True
@@ -50,11 +50,11 @@
 def _add_results(results, combo):
     for result in results:
         trips = 0
-        for field, values in result["conditions"].iteritems():
+        for field, values in result["conditions"].items():
             if combo[field] in values:
                 trips += 1
-        if trips == len(result["conditions"].keys()):
-            for field, value in result["results"].iteritems():
+        if trips == len(list(result["conditions"].keys())):
+            for field, value in result["results"].items():
                 combo[field] = value
             return
 
--- ./doajtest/demo_scripts/async_workflow_notifications_demo.py	(original)
+++ ./doajtest/demo_scripts/async_workflow_notifications_demo.py	(refactored)
@@ -11,7 +11,7 @@
 # Replace the async workflow send function with this one to limit emails sent in this demo.
 def send_emails(emails_dict):
 
-    for (email, (to_name, paragraphs)) in emails_dict.iteritems():
+    for (email, (to_name, paragraphs)) in emails_dict.items():
         time.sleep(0.6)
         pre = 'Dear ' + to_name + ',\n\n'
         post = '\n\nThe DOAJ Team\n\n***\nThis is an automated message. Please do not reply to this email.'
--- ./doajtest/fixtures/__init__.py	(original)
+++ ./doajtest/fixtures/__init__.py	(refactored)
@@ -1,8 +1,8 @@
-from journals import JournalFixtureFactory
-from applications import ApplicationFixtureFactory
-from editors import EditorGroupFixtureFactory
-from accounts import AccountFixtureFactory
-from article import ArticleFixtureFactory
-from bibjson import BibJSONFixtureFactory
-from provenance import ProvenanceFixtureFactory
-from background import BackgroundFixtureFactory
+from .journals import JournalFixtureFactory
+from .applications import ApplicationFixtureFactory
+from .editors import EditorGroupFixtureFactory
+from .accounts import AccountFixtureFactory
+from .article import ArticleFixtureFactory
+from .bibjson import BibJSONFixtureFactory
+from .provenance import ProvenanceFixtureFactory
+from .background import BackgroundFixtureFactory
--- ./doajtest/fixtures/article.py	(original)
+++ ./doajtest/fixtures/article.py	(refactored)
@@ -1,6 +1,6 @@
 import os
 from lxml import etree
-from StringIO import StringIO
+from io import StringIO
 from copy import deepcopy
 
 RESOURCES = os.path.join(os.path.dirname(os.path.realpath(__file__)), "..", "unit", "resources")
--- ./doajtest/fixtures/snapshots.py	(original)
+++ ./doajtest/fixtures/snapshots.py	(refactored)
@@ -1,201 +1,201 @@
 """ Include a dump of snapshot data from the live system"""
 
 SNAPSHOTS_LIST = {
-    u"snapshots": [
-        {
-            u"duration_in_millis": 54877,
-            u"end_time": u"2018-05-06T05:00:54.937Z",
-            u"end_time_in_millis": 1525582854937,
-            u"failures": [],
-            u"indices": [
-                u"doaj_v1"
-            ],
-            u"shards": {
-                u"failed": 0,
-                u"successful": 6,
-                u"total": 6
-            },
-            u"snapshot": u"snapshot_2018-05-06_0600",
-            u"start_time": u"2018-05-06T05:00:00.060Z",
-            u"start_time_in_millis": 1525582800060,
-            u"state": u"SUCCESS",
-            u"version": u"1.7.5",
-            u"version_id": 1070599
-        },
-        {
-            u"duration_in_millis": 53051,
-            u"end_time": u"2018-05-07T05:00:53.176Z",
-            u"end_time_in_millis": 1525669253176,
-            u"failures": [],
-            u"indices": [
-                u"doaj_v1"
-            ],
-            u"shards": {
-                u"failed": 0,
-                u"successful": 6,
-                u"total": 6
-            },
-            u"snapshot": u"snapshot_2018-05-07_0600",
-            u"start_time": u"2018-05-07T05:00:00.125Z",
-            u"start_time_in_millis": 1525669200125,
-            u"state": u"SUCCESS",
-            u"version": u"1.7.5",
-            u"version_id": 1070599
-        },
-        {
-            u"duration_in_millis": 45674,
-            u"end_time": u"2018-05-08T05:00:45.990Z",
-            u"end_time_in_millis": 1525755645990,
-            u"failures": [],
-            u"indices": [
-                u"doaj_v1"
-            ],
-            u"shards": {
-                u"failed": 0,
-                u"successful": 6,
-                u"total": 6
-            },
-            u"snapshot": u"snapshot_2018-05-08_0600",
-            u"start_time": u"2018-05-08T05:00:00.316Z",
-            u"start_time_in_millis": 1525755600316,
-            u"state": u"SUCCESS",
-            u"version": u"1.7.5",
-            u"version_id": 1070599
-        },
-        {
-            u"duration_in_millis": 52140,
-            u"end_time": u"2018-05-09T05:00:51.611Z",
-            u"end_time_in_millis": 1525842051611,
-            u"failures": [],
-            u"indices": [
-                u"doaj_v1"
-            ],
-            u"shards": {
-                u"failed": 0,
-                u"successful": 6,
-                u"total": 6
-            },
-            u"snapshot": u"snapshot_2018-05-09_0600",
-            u"start_time": u"2018-05-09T04:59:59.471Z",
-            u"start_time_in_millis": 1525841999471,
-            u"state": u"SUCCESS",
-            u"version": u"1.7.5",
-            u"version_id": 1070599
-        },
-        {
-            u"duration_in_millis": 43258,
-            u"end_time": u"2018-05-10T05:00:42.828Z",
-            u"end_time_in_millis": 1525928442828,
-            u"failures": [],
-            u"indices": [
-                u"doaj_v1"
-            ],
-            u"shards": {
-                u"failed": 0,
-                u"successful": 6,
-                u"total": 6
-            },
-            u"snapshot": u"snapshot_2018-05-10_0600",
-            u"start_time": u"2018-05-10T04:59:59.570Z",
-            u"start_time_in_millis": 1525928399570,
-            u"state": u"SUCCESS",
-            u"version": u"1.7.5",
-            u"version_id": 1070599
-        },
-        {
-            u"duration_in_millis": 49055,
-            u"end_time": u"2018-05-11T05:00:48.678Z",
-            u"end_time_in_millis": 1526014848678,
-            u"failures": [],
-            u"indices": [
-                u"doaj_v1"
-            ],
-            u"shards": {
-                u"failed": 0,
-                u"successful": 6,
-                u"total": 6
-            },
-            u"snapshot": u"snapshot_2018-05-11_0600",
-            u"start_time": u"2018-05-11T04:59:59.623Z",
-            u"start_time_in_millis": 1526014799623,
-            u"state": u"SUCCESS",
-            u"version": u"1.7.5",
-            u"version_id": 1070599
-        },
-        {
-            u"duration_in_millis": 42105,
-            u"end_time": u"2018-05-12T05:00:42.006Z",
-            u"end_time_in_millis": 1526101242006,
-            u"failures": [],
-            u"indices": [
-                u"doaj_v1"
-            ],
-            u"shards": {
-                u"failed": 0,
-                u"successful": 6,
-                u"total": 6
-            },
-            u"snapshot": u"snapshot_2018-05-12_0600",
-            u"start_time": u"2018-05-12T04:59:59.901Z",
-            u"start_time_in_millis": 1526101199901,
-            u"state": u"SUCCESS",
-            u"version": u"1.7.5",
-            u"version_id": 1070599
-        },
-        {
-            u"duration_in_millis": 36901,
-            u"end_time": u"2018-05-13T05:00:36.604Z",
-            u"end_time_in_millis": 1526187636604,
-            u"failures": [
+    "snapshots": [
+        {
+            "duration_in_millis": 54877,
+            "end_time": "2018-05-06T05:00:54.937Z",
+            "end_time_in_millis": 1525582854937,
+            "failures": [],
+            "indices": [
+                "doaj_v1"
+            ],
+            "shards": {
+                "failed": 0,
+                "successful": 6,
+                "total": 6
+            },
+            "snapshot": "snapshot_2018-05-06_0600",
+            "start_time": "2018-05-06T05:00:00.060Z",
+            "start_time_in_millis": 1525582800060,
+            "state": "SUCCESS",
+            "version": "1.7.5",
+            "version_id": 1070599
+        },
+        {
+            "duration_in_millis": 53051,
+            "end_time": "2018-05-07T05:00:53.176Z",
+            "end_time_in_millis": 1525669253176,
+            "failures": [],
+            "indices": [
+                "doaj_v1"
+            ],
+            "shards": {
+                "failed": 0,
+                "successful": 6,
+                "total": 6
+            },
+            "snapshot": "snapshot_2018-05-07_0600",
+            "start_time": "2018-05-07T05:00:00.125Z",
+            "start_time_in_millis": 1525669200125,
+            "state": "SUCCESS",
+            "version": "1.7.5",
+            "version_id": 1070599
+        },
+        {
+            "duration_in_millis": 45674,
+            "end_time": "2018-05-08T05:00:45.990Z",
+            "end_time_in_millis": 1525755645990,
+            "failures": [],
+            "indices": [
+                "doaj_v1"
+            ],
+            "shards": {
+                "failed": 0,
+                "successful": 6,
+                "total": 6
+            },
+            "snapshot": "snapshot_2018-05-08_0600",
+            "start_time": "2018-05-08T05:00:00.316Z",
+            "start_time_in_millis": 1525755600316,
+            "state": "SUCCESS",
+            "version": "1.7.5",
+            "version_id": 1070599
+        },
+        {
+            "duration_in_millis": 52140,
+            "end_time": "2018-05-09T05:00:51.611Z",
+            "end_time_in_millis": 1525842051611,
+            "failures": [],
+            "indices": [
+                "doaj_v1"
+            ],
+            "shards": {
+                "failed": 0,
+                "successful": 6,
+                "total": 6
+            },
+            "snapshot": "snapshot_2018-05-09_0600",
+            "start_time": "2018-05-09T04:59:59.471Z",
+            "start_time_in_millis": 1525841999471,
+            "state": "SUCCESS",
+            "version": "1.7.5",
+            "version_id": 1070599
+        },
+        {
+            "duration_in_millis": 43258,
+            "end_time": "2018-05-10T05:00:42.828Z",
+            "end_time_in_millis": 1525928442828,
+            "failures": [],
+            "indices": [
+                "doaj_v1"
+            ],
+            "shards": {
+                "failed": 0,
+                "successful": 6,
+                "total": 6
+            },
+            "snapshot": "snapshot_2018-05-10_0600",
+            "start_time": "2018-05-10T04:59:59.570Z",
+            "start_time_in_millis": 1525928399570,
+            "state": "SUCCESS",
+            "version": "1.7.5",
+            "version_id": 1070599
+        },
+        {
+            "duration_in_millis": 49055,
+            "end_time": "2018-05-11T05:00:48.678Z",
+            "end_time_in_millis": 1526014848678,
+            "failures": [],
+            "indices": [
+                "doaj_v1"
+            ],
+            "shards": {
+                "failed": 0,
+                "successful": 6,
+                "total": 6
+            },
+            "snapshot": "snapshot_2018-05-11_0600",
+            "start_time": "2018-05-11T04:59:59.623Z",
+            "start_time_in_millis": 1526014799623,
+            "state": "SUCCESS",
+            "version": "1.7.5",
+            "version_id": 1070599
+        },
+        {
+            "duration_in_millis": 42105,
+            "end_time": "2018-05-12T05:00:42.006Z",
+            "end_time_in_millis": 1526101242006,
+            "failures": [],
+            "indices": [
+                "doaj_v1"
+            ],
+            "shards": {
+                "failed"RefactoringTool: Refactored ./doajtest/functional/bulk_api.py
: 0,
+                "successful": 6,
+                "total": 6
+            },
+            "snapshot": "snapshot_2018-05-12_0600",
+            "start_time": "2018-05-12T04:59:59.901Z",
+            "start_time_in_millis": 1526101199901,
+            "state": "SUCCESS",
+            "version": "1.7.5",
+            "version_id": 1070599
+        },
+        {
+            "duration_in_millis": 36901,
+            "end_time": "2018-05-13T05:00:36.604Z",
+            "end_time_in_millis": 1526187636604,
+            "failures": [
                 {
-                  u"index": u"doaj_v1",
-                  u"node_id": u"RTVV05UoToubyPr9ErBlKg",
-                  u"reason": u"node shutdown",
-                  u"shard_id": 1,
-                  u"status": u"INTERNAL_SERVER_ERROR"
+                  "index": "doaj_v1",
+                  "node_id": "RTVV05UoToubyPr9ErBlKg",
+                  "reason": "node shutdown",
+                  "shard_id": 1,
+                  "status": "INTERNAL_SERVER_ERROR"
                 },
                 {
-                  u"index": u"doaj_v1",
-                  u"node_id": u"RTVV05UoToubyPr9ErBlKg",
-                  u"reason": u"node shutdown",
-                  u"shard_id": 2,
-                  u"status": u"INTERNAL_SERVER_ERROR"
+                  "index": "doaj_v1",
+                  "node_id": "RTVV05UoToubyPr9ErBlKg",
+                  "reason": "node shutdown",
+                  "shard_id": 2,
+                  "status": "INTERNAL_SERVER_ERROR"
                 }
             ],
-            u"indices": [
-                u"doaj_v1"
-            ],
-            u"shards": {
-                u"failed": 2,
-                u"successful": 0,
-                u"total": 2
-            },
-            u"snapshot": u"snapshot_2018-05-13_0600",
-            u"start_time": u"2018-05-13T04:59:59.703Z",
-            u"start_time_in_millis": 1526187599703,
-            u"state": u"PARTIAL",
-            u"version": u"1.7.5",
-            u"version_id": 1070599
-        },
-        {
-            u"duration_in_millis": 40825,
-            u"end_time": u"2018-05-14T05:00:40.714Z",
-            u"end_time_in_millis": 1526274040714,
-            u"failures": [],
-            u"indices": [
-                u"doaj_v1"
-            ],
-            u"shards": {
-                u"failed": 0,
-                u"successful": 6,
-                u"total": 6
-            },
-            u"snapshot": u"snapshot_2018-05-14_0600",
-            u"start_time": u"2018-05-14T04:59:59.889Z",
-            u"start_time_in_millis": 1526273999889,
-            u"state": u"SUCCESS",
-            u"version": u"1.7.5",
-            u"version_id": 1070599
+            "indices": [
+                "doaj_v1"
+            ],
+            "shards": {
+                "failed": 2,
+                "successful": 0,
+                "total": 2
+            },
+            "snapshot": "snapshot_2018-05-13_0600",
+            "start_time": "2018-05-13T04:59:59.703Z",
+            "start_time_in_millis": 1526187599703,
+            "state": "PARTIAL",
+            "version": "1.7.5",
+            "version_id": 1070599
+        },
+        {
+            "duration_in_millis": 40825,
+            "end_time": "2018-05-14T05:00:40.714Z",
+            "end_time_in_millis": 1526274040714,
+            "failures": [],
+            "indices": [
+                "doaj_v1"
+            ],
+            "shards": {
+                "failed": 0,
+                "successful": 6,
+                "total": 6
+            },
+            "snapshot": "snapshot_2018-05-14_0600",
+            "start_time": "2018-05-14T04:59:59.889Z",
+            "start_time_in_millis": 1526273999889,
+            "state": "SUCCESS",
+            "version": "1.7.5",
+            "version_id": 1070599
         }
     ]
 }
--- ./doajtest/functional/bulk_api.py	(original)
+++ ./doajtest/functional/bulk_api.py	(refactored)
@@ -14,10 +14,10 @@
 j = resp.json()
 assert len(j) == 10
 
-print j
+print(j)
 
 ids = [r.get("id") for r in j]
-print ids
+printRefactoringTool: Refactored ./doajtest/functional/oaipmh_client_test.py
RefactoringTool: No changes to ./doajtest/functional/stress_test.py
RefactoringTool: No changes to ./doajtest/mocks/bll_article.py
RefactoringTool: No changes to ./doajtest/mocks/model_Article.py
RefactoringTool: No changes to ./doajtest/mocks/model_Cache.py
RefactoringTool: No changes to ./doajtest/mocks/model_Journal.py
RefactoringTool: No changes to ./doajtest/mocks/models_Cache.py
RefactoringTool: No changes to ./doajtest/mocks/store.py
RefactoringTool: No changes to ./doajtest/unit/test_anon.py
RefactoringTool: No changes to ./doajtest/unit/test_api_account.py
RefactoringTool: No changes to ./doajtest/unit/test_api_bulk_application.py
RefactoringTool: No changes to ./doajtest/unit/test_api_bulk_article.py
RefactoringTool: Refactored ./doajtest/unit/test_api_crud_application.py
RefactoringTool: Refactored ./doajtest/unit/test_api_crud_article.py
RefactoringTool: No changes to ./doajtest/unit/test_api_crud_journal.py
RefactoringTool: No changes to ./doajtest/unit/test_api_crud_returnvalues.py
RefactoringTool: No changes to ./doajtest/unit/test_api_dataobj.py
RefactoringTool: No changes to ./doajtest/unit/test_api_dataobj_cast_functions.py
RefactoringTool: No changes to ./doajtest/unit/test_api_discovery.py
RefactoringTool: No changes to ./doajtest/unit/test_api_errors.py
RefactoringTool: No changes to ./doajtest/unit/test_article_cleanup_sync.py
RefactoringTool: Refactored ./doajtest/unit/test_article_match.py
RefactoringTool: No changes to ./doajtest/unit/test_background.py
RefactoringTool: No changes to ./doajtest/unit/test_bll_accept_application.py
RefactoringTool: No changes to ./doajtest/unit/test_bll_article_batch_create_article.py
RefactoringTool: No changes to ./doajtest/unit/test_bll_article_create_article.py
RefactoringTool: No changes to ./doajtest/unit/test_bll_article_discover_duplicates.py
RefactoringTool: No changes to ./doajtest/unit/test_bll_article_get_duplicates.py
RefactoringTool: No changes to ./doajtest/unit/test_bll_article_is_legitimate_owner.py
RefactoringTool: No changes to ./doajtest/unit/test_bll_article_issn_ownership_status.py
RefactoringTool: No changes to ./doajtest/unit/test_bll_authorisations.py
RefactoringTool: Refactored ./doajtest/unit/test_bll_delete_application.py
RefactoringTool: No changes to ./doajtest/unit/test_bll_getters.py
RefactoringTool: Refactored ./doajtest/unit/test_bll_journal_csv.py
(ids)
 
 time.sleep(2)
 
@@ -53,10 +53,10 @@
 j = resp.json()
 assert len(j) == 10
 
-print j
+print(j)
 
 ids = [r.get("id") for r in j]
-print ids
+print(ids)
 
 time.sleep(2)
 
--- ./doajtest/functional/oaipmh_client_test.py	(original)
+++ ./doajtest/functional/oaipmh_client_test.py	(refactored)
@@ -36,7 +36,7 @@
     if now - last_req < req_period:
         time.sleep((req_period - (now - last_req)).total_seconds())
 
-    print "harvesting " + url
+    print("harvesting " + url)
     last_req = now
     resp = requests.get(url)
     assert resp.status_code == 200, resp.text
@@ -47,20 +47,20 @@
     for record in records:
         oai_id = record.find(".//" + NS + "identifier").text
         if oai_id in IDENTS:
-            print "DUPLICATE ID: " + oai_id
+            print("DUPLICATE ID: " + oai_id)
             return None
         IDENTS.append(oai_id)
 
     rtel = xml.find(".//" + NS + "resumptionToken")
     if rtel is not None:
         if rtel.text is not None and rtel.text != "":
-            print "\tresumption token", rtel.text, "cursor", rtel.get("cursor") + "/" + rtel.get("completeListSize")
-            print "\tresults received: ", len(IDENTS)
+            print("\tresumption token", rtel.text, "cursor", rtel.get("cursor") + "/" + rtel.get("completeListSize"))
+            print("\tresults received: ", len(IDENTS))
             return rtel.text
         else:
-            print "\tno resumption token, complete. cursor", rtel.get("cursor") + "/" + rtel.get("completeListSize")
+            print("\tno resumption token, complete. cursor", rtel.get("cursor") + "/" + rtel.get("completeListSize"))
     else:
-        print "no results"
+        print("no results")
         return None
 
 
--- ./doajtest/unit/test_api_crud_application.py	(original)
+++ ./doajtest/unit/test_api_crud_application.py	(refactored)
@@ -350,7 +350,7 @@
         assert ia.bibjson.country == "BD"
         assert ia.bibjson.apc.currency == "BDT"
         assert ia.bibjson.allows_fulltext_indexing is True
-        assert isinstance(ia.bibjson.title, unicode)
+        assert isinstance(ia.bibjson.title, str)
         assert ia.bibjson.publication_time == 15
         assert "fr" in ia.bibjson.language
         assert "en" in ia.bibjson.language
@@ -610,7 +610,7 @@
         # on the wrong id
         account.set_id("test")
         with self.assertRaises(Api404Error):
-            ApplicationsCrudApi.delete(u"adfasdfhwefwef", account)
+            ApplicationsCrudApi.delete("adfasdfhwefwef", account)
 
         # on one with a disallowed workflow status
         created.set_application_status(constants.APPLICATION_STATUS_ACCEPTED)
--- ./doajtest/unit/test_api_crud_article.py	(original)
+++ ./doajtest/unit/test_api_crud_article.py	(refactored)
@@ -160,7 +160,7 @@
         data["admin"]["in_doaj"] = False
         data["created_date"] = datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ")
         ia = IncomingArticleDO(data)
-        assert isinstance(ia.bibjson.title, unicode)
+        assert isinstance(ia.bibjson.title, str)
 
         # now test some failures
 
--- ./doajtest/unit/test_article_match.py	(original)
+++ ./doajtest/unit/test_article_match.py	(refactored)
@@ -247,7 +247,7 @@
         d = articleService.get_duplicates(z)
 
         assert len(d) == 1
-        print len(d)
+        print(len(d))
         assert d[0].bibjson().title == "Example article with a fulltext url and a DOI"
 
     def test_08_many_issns(self):
--- ./doajtest/unit/test_bll_delete_application.py	(original)
+++ ./doajtest/unit/test_bll_delete_application.py	(refactored)
@@ -112,7 +112,7 @@
             application.save(blocking=True)
             application_id = application.id
         elif application_type == "not_found":
-            application_id = u"sdjfasofwefkwflkajdfasjd"
+            application_id = "sdjfasofwefkwflkajdfasjd"
 
         ###########################################################
         # Execution
--- ./doajtest/unit/test_bll_journal_csv.py	(original)
+++ ./doajtest/unit/test_bll_journal_csv.py	(refactored)
@@ -12,7 +12,7 @@
 froWARNING: couldn't encode ./doajtest/unit/test_bll_journal_csv.py's diff for your terminal
RefactoringTool: No changes to ./doajtest/unit/test_bll_object_conversions.py
RefactoringTool: No changes to ./doajtest/unit/test_bll_reject_application.py
RefactoringTool: No changes to ./doajtest/unit/test_bll_update_request.py
RefactoringTool: No changes to ./doajtest/unit/test_contact.py
RefactoringTool: Refactored ./doajtest/unit/test_crosswalks_journal2questions.py
RefactoringTool: Refactored ./doajtest/unit/test_csv_wrapper.py
WARNING: couldn't encode ./doajtest/unit/test_csv_wrapper.py's diff for your terminal
RefactoringTool: Refactored ./doajtest/unit/test_datasets.py
RefactoringTool: No changes to ./doajtest/unit/test_duplicate_report_script.py
RefactoringTool: Refactored ./doajtest/unit/test_es_snapshots_client.py
m doajtest.mocks.store import StoreMockFactory
 from doajtest.mocks.models_Cache import ModelCacheMockFactory
 from portality import store, clcsv
-from StringIO import StringIO
+from io import StringIO
 import os, shutil
 
 def load_cases():
@@ -96,7 +96,7 @@
         for i in range(len(journals)):
             journal = journals[i]
             bj = journal.bibjson()
--- ./doajtest/unit/test_crosswalks_journal2questions.py	(original)
+++ ./doajtest/unit/test_crosswalks_journal2questions.py	(refactored)
@@ -17,11 +17,11 @@
         journal = models.Journal(**JournalFixtureFactory.make_journal_source(in_doaj=True))
         journal.prep()
         q_and_a = Journal2QuestionXwalk.journal2question(journal)
-        answers = [unicode(x[1]) for x in q_and_a]
+        answers = [str(x[1]) for x in q_and_a]
         expected = JournalFixtureFactory.question_answers()
         for i in range(len(answers)):
             a = answers[i]
             if a != expected[i]:
-                print("{c} = {a} | {b}".format(a=a, b=expected[i], c=q_and_a[i]))
+                print(("{c} = {a} | {b}".format(a=a, b=expected[i], c=q_and_a[i])))
 
         assert answers == expected
--- ./doajtest/unit/test_csv_wrapper.py	(original)
+++ ./doajtest/unit/test_csv_wrapper.py	(refactored)
@@ -28,7 +28,7 @@
 
         self.gold_csv_unicode = codecs.open(os.path.join(self.PRFX, 'rescsv_gold_standard_unicode'), 'wb')
 
--- ./doajtest/unit/test_datasets.py	(original)
+++ ./doajtest/unit/test_datasets.py	(refactored)
@@ -12,7 +12,7 @@
     def test_01_countries(self):
         """ Use country information from our datasets """
         assert datasets.get_country_code('united kingdom') == 'GB'
-        assert datasets.get_country_name('GB') == u'United Kingdom'
+        assert datasets.get_country_name('GB') == 'United Kingdom'
 
         # If the country is unrecognised, we send it back unchanged.
         assert datasets.get_country_code('mordor') == 'mordor'
@@ -25,7 +25,7 @@
 
         # When we have more than one option, the first alphabetically is returned
         #assert datasets.get_country_code('united') == 'AE' # doesn't work with pycountry - has to be full text match
-        assert datasets.get_country_name('AE') == u'United Arab Emirates'
+        assert datasets.get_country_name('AE') == 'United Arab Emirates'
 
     def test_02_currencies(self):
         """ Utilise currency information from the datasets """
@@ -33,7 +33,7 @@
         assert datasets.get_currency_name('JPY') == 'JPY - Yen'
 
         assert datasets.get_currency_code('pound sterling') == 'GBP'
-        assert datasets.get_currency_name('GBP') == u'GBP - Pound Sterling'
+        assert datasets.get_currency_name('GBP') == 'GBP - Pound Sterling'
 
         assert datasets.get_currency_code('pound') is None
         assert datasets.get_currency_code('doubloons') is None
@@ -41,6 +41,6 @@
     def test_03_languages(self):
         """ Use language information from our datasets """
         #assert datasets.language_for('en') == [u'eng', u'', u'en', u'English', u'anglais'] # obsolete with pycountry
-        assert datasets.name_for_lang('en') == u'English'
-        assert datasets.name_for_lang('eng') == u'English'
+        assert datasets.name_for_lang('en') == 'English'
+        assert datasets.name_for_lang('eng') == 'English'
         #assert datasets.name_for_lang('anglais') == u'English'  # doesnt work with pycountry languages
--- ./doajtest/unit/test_es_snapshots_client.py	(original)
+++ ./doajtest/unit/test_es_snapshots_client.py	(refactored)
@@ -82,7 +82,7 @@
 
         # Set up a mock responses for deleting the snapshots
         for s in SNAPSHOTS_LIST['snapshots']:
-            responses.add(responses.DELETE, self.snapshot_url + '/' + s['snapshot'], json={u"acknowledged": True}, status=200)
+            responses.add(responses.DELETE, self.snapshot_url + '/' + s['snapshot'], json={"acknowledged": True}, status=200)
 
         # Set the time to the day of the latest backup, and request a prune
         latest_fixture_date = datetime.utcfromtimestamp(SNAPSHOTS_LISRefactoringTool: Refactored ./doajtest/unit/test_fc_assed_app_review.py
RefactoringTool: Refactored ./doajtest/unit/test_fc_assed_journal_review.py
RefactoringTool: Refactored ./doajtest/unit/test_fc_editor_app_review.py
RefactoringTool: Refactored ./doajtest/unit/test_fc_editor_journal_review.py
RefactoringTool: Refactored ./doajtest/unit/test_fc_maned_app_review.py
RefactoringTool: Refactored ./doajtest/unit/test_fc_maned_journal_review.py
RefactoringTool: No changes to ./doajtest/unit/test_fc_publisher_update_request.py
RefactoringTool: Refactored ./doajtest/unit/test_fc_readonly_journal.py
RefactoringTool: Refactored ./doajtest/unit/test_feed.py
T['snapshots'][-1]['start_time_in_millis'] / 1000)
@@ -100,7 +100,7 @@
         # Mock response for initiating a snapshot
         right_now = datetime.utcnow()
         slashtimestamp = datetime.strftime(right_now, "/%Y-%m-%d_%H%Mz")
-        responses.add(responses.PUT, self.snapshot_url + slashtimestamp, json={u"acknowledged": True}, status=200)
+        responses.add(responses.PUT, self.snapshot_url + slashtimestamp, json={"acknowledged": True}, status=200)
 
         # Request a new backup, check it has the right timestamp
         with freeze_time(right_now):
--- ./doajtest/unit/test_fc_assed_app_review.py	(original)
+++ ./doajtest/unit/test_fc_assed_app_review.py	(refactored)
@@ -25,8 +25,8 @@
     return eg
 
 mock_lcc_choices = [
-    (u'H', u'Social Sciences'),
-    (u'HB1-3840', u'--Economic theory. Demography')
+    ('H', 'Social Sciences'),
+    ('HB1-3840', '--Economic theory. Demography')
 ]
 
 def mock_lookup_code(code):
--- ./doajtest/unit/test_fc_assed_journal_review.py	(original)
+++ ./doajtest/unit/test_fc_assed_journal_review.py	(refactored)
@@ -20,8 +20,8 @@
     return eg
 
 mock_lcc_choices = [
-    (u'H', u'Social Sciences'),
-    (u'HB1-3840', u'--Economic theory. Demography')
+    ('H', 'Social Sciences'),
+    ('HB1-3840', '--Economic theory. Demography')
 ]
 
 def mock_lookup_code(code):
--- ./doajtest/unit/test_fc_editor_app_review.py	(original)
+++ ./doajtest/unit/test_fc_editor_app_review.py	(refactored)
@@ -26,8 +26,8 @@
     return eg
 
 mock_lcc_choices = [
-    (u'H', u'Social Sciences'),
-    (u'HB1-3840', u'--Economic theory. Demography')
+    ('H', 'Social Sciences'),
+    ('HB1-3840', '--Economic theory. Demography')
 ]
 
 
--- ./doajtest/unit/test_fc_editor_journal_review.py	(original)
+++ ./doajtest/unit/test_fc_editor_journal_review.py	(refactored)
@@ -22,8 +22,8 @@
     return eg
 
 mock_lcc_choices = [
-    (u'H', u'Social Sciences'),
-    (u'HB1-3840', u'--Economic theory. Demography')
+    ('H', 'Social Sciences'),
+    ('HB1-3840', '--Economic theory. Demography')
 ]
 
 def mock_lookup_code(code):
--- ./doajtest/unit/test_fc_maned_app_review.py	(original)
+++ ./doajtest/unit/test_fc_maned_app_review.py	(refactored)
@@ -25,8 +25,8 @@
     return eg
 
 mock_lcc_choices = [
-    (u'H', u'Social Sciences'),
-    (u'HB1-3840', u'--Economic theory. Demography')
+    ('H', 'Social Sciences'),
+    ('HB1-3840', '--Economic theory. Demography')
 ]
 
 def mock_lookup_code(code):
--- ./doajtest/unit/test_fc_maned_journal_review.py	(original)
+++ ./doajtest/unit/test_fc_maned_journal_review.py	(refactored)
@@ -27,8 +27,8 @@
     return eg
 
 mock_lcc_choices = [
-    (u'H', u'Social Sciences'),
-    (u'HB1-3840', u'--Economic theory. Demography')
+    ('H', 'Social Sciences'),
+    ('HB1-3840', '--Economic theory. Demography')
 ]
 
 
--- ./doajtest/unit/test_fc_readonly_journal.py	(original)
+++ ./doajtest/unit/test_fc_readonly_journal.py	(refactored)
@@ -13,8 +13,8 @@
 # Mocks required to make some of the lookups work
 #####################################################################
 mock_lcc_choices = [
-    (u'H', u'Social Sciences'),
-    (u'HB1-3840', u'--Economic theory. Demography')
+    ('H', 'Social Sciences'),
+    ('HB1-3840', '--Economic theory. Demography')
 ]
 
 def mock_lookup_code(code):
--- ./doajtest/unit/test_feed.py	(original)
+++ ./doajtest/unit/test_feed.py	(refactored)
@@ -18,7 +18,7 @@
     def test_01_object(self):
         # first try requesting a feed over the empty test index
         f = atom.get_feed("http://my.test.com")
-        assert len(f.entries.keys()) == 0
+        assert len(list(f.entries.keys())) == 0
         assert f.url == "http://my.test.com"
 
         # now populate the index and then re-get the feed
@@ -42,10 +42,10 @@
 
         with self.app_test.test_request_context('/feed'):
             f = atom.get_feed("http://my.test.com")
-        assert len(f.entries.keys()) == 5
+        assert len(list(f.entries.keys())) == 5
 
         # now go through the entries in order, and check they are as expected
-        entry_dates = f.entries.RefactoringTool: Refactored ./doajtest/unit/test_formcontext.py
RefactoringTool: Refactored ./doajtest/unit/test_formcontext_emails.py
RefactoringTool: Refactored ./doajtest/unit/test_formrender.py
RefactoringTool: No changes to ./doajtest/unit/test_index_searchbox.py
RefactoringTool: Refactored ./doajtest/unit/test_jinja_template_filters.py
keys()
+        entry_dates = list(f.entries.keys())
         entry_dates.sort()
 
         for i in range(5):
--- ./doajtest/unit/test_formcontext.py	(original)
+++ ./doajtest/unit/test_formcontext.py	(refactored)
@@ -12,11 +12,11 @@
 
 JOURNAL_SOURCE = {
     "index": {
-        "publisher": ["Centro Centroamericano de Poblaci\u00f3n"],
+        "publisher": ["Centro Centroamericano de Poblaci\\u00f3n"],
         "schema_subject": ["LCC:Social Sciences", "LCC:Economic theory. Demography"],
         "license": ["CC by-nc-sa"],
         "classification": ["Social Sciences", "Economic theory. Demography"],
-        "title": ["Poblaci\u00f3n y Salud en Mesoam\u00e9rica"],
+        "title": ["Poblaci\\u00f3n y Salud en Mesoam\\u00e9rica"],
         "country": "Costa Rica",
         "issn": ["1659-0201"],
         "language": ["Spanish"],
@@ -32,11 +32,11 @@
     "created_date": "2008-05-19T16:45:03Z",
     "id": "3f3d352f32364065ab9051b1a4b2d715",
     "bibjson": {
-        "publisher": "Centro Centroamericano de Poblaci\u00f3n",
+        "publisher": "Centro Centroamericano de Poblaci\\u00f3n",
         "author_pays": "N",
         "license": [{"type": "CC by-nc-sa", "title": "CC by-nc-sa"}],
         "language": ["Spanish"],
-        "title": "Poblaci\u00f3n y Salud en Mesoam\u00e9rica",
+        "title": "Poblaci\\u00f3n y Salud en Mesoam\\u00e9rica",
         "author_pays_url": "http://revistas.ucr.ac.cr/index.php/psm/about/submissions",
         "country": "CR",
         "alternative_title": "",
--- ./doajtest/unit/test_formcontext_emails.py	(original)
+++ ./doajtest/unit/test_formcontext_emails.py	(refactored)
@@ -1,6 +1,6 @@
 import logging
 import re
-from StringIO import StringIO
+from io import StringIO
 from copy import deepcopy
 
 from portality import constants
--- ./doajtest/unit/test_formrender.py	(original)
+++ ./doajtest/unit/test_formrender.py	(refactored)
@@ -143,7 +143,7 @@
         assert len(r.error_fields) == 0
         assert len(r.NUMBERING_ORDER) == 6
         assert len(r.NUMBERING_ORDER) == len(r.ERROR_CHECK_ORDER)
-        assert len(r.FIELD_GROUPS.keys()) == 6
+        assert len(list(r.FIELD_GROUPS.keys())) == 6
 
         # number the questions
         r.number_questions()
@@ -153,7 +153,7 @@
         for g in r.NUMBERING_ORDER:
             cfg = r.FIELD_GROUPS.get(g)
             for obj in cfg:
-                field = obj.keys()[0]
+                field = list(obj.keys())[0]
                 assert obj[field].get("q_num") == str(q), (field, obj[field].get("q_num"), q)
                 q += 1
         assert q == 59, q # checks that we checked everything (58 questions, plus an extra 1 from the end of the loop)
@@ -169,7 +169,7 @@
         for g in r.ERROR_CHECK_ORDER:
             cfg = r.FIELD_GROUPS.get(g)
             for obj in cfg:
-                field = obj.keys()[0]
+                field = list(obj.keys())[0]
                 if field == "publisher":
                     assert obj[field].get("first_error", False)
                 else:
@@ -190,7 +190,7 @@
             field_group='test'
         )
         assert len(r.FIELD_GROUPS['test']) == 4
-        assert r.FIELD_GROUPS['test'][0].keys()[0] == 'one'
-        assert r.FIELD_GROUPS['test'][1].keys()[0] == 'inserted_in_middle'
-        assert r.FIELD_GROUPS['test'][2].keys()[0] == 'two'
-        assert r.FIELD_GROUPS['test'][3].keys()[0] == 'inserted_last'
+        assert list(r.FIELD_GROUPS['test'][0].keys())[0] == 'one'
+        assert list(r.FIELD_GROUPS['test'][1].keys())[0] == 'inserted_in_middle'
+        assert list(r.FIELD_GROUPS['test'][2].keys())[0] == 'two'
+        assert list(r.FIELD_GROUPS['test'][3].keys())[0] == 'inserted_last'
--- ./doajtest/unit/test_jinja_template_filters.py	(original)
+++ ./doajtest/unit/test_jinja_template_filters.py	(refactored)
@@ -12,31 +12,31 @@
     def test_01_form_diff_table_comparison_value(self):
         filter = app.jinja_env.filters['form_diff_table_comparison_value']
 
-        assert filter(None) == ""
-        assert filter([]) == ""
-        assert filter("whatever") == RefactoringTool: No changes to ./doajtest/unit/test_lib_normalise_doi.py
RefactoringTool: No changes to ./doajtest/unit/test_lib_normalise_url.py
RefactoringTool: No changes to ./doajtest/unit/test_lock.py
RefactoringTool: No changes to ./doajtest/unit/test_models.py
RefactoringTool: No changes to ./doajtest/unit/test_oaipmh.py
RefactoringTool: No changes to ./doajtest/unit/test_prune_marvel.py
RefactoringTool: No changes to ./doajtest/unit/test_query.py
RefactoringTool: No changes to ./doajtest/unit/test_query_filters.py
RefactoringTool: No changes to ./doajtest/unit/test_regexes.py
RefactoringTool: Refactored ./doajtest/unit/test_reporting.py
WARNING: couldn't encode ./doajtest/unit/test_reporting.py's diff for your terminal
RefactoringTool: No changes to ./doajtest/unit/test_reserved_usernames.py
RefactoringTool: Refactored ./doajtest/unit/test_scripts_accounts_with_marketing_consent.py
"whatever"
-        assert filter("true") == "Yes"
-        assert filter ("True") == "Yes"
-        assert filter("TrUe") == "Yes"
-        assert filter(True) == "Yes"
-        assert filter("false") == "No"
-        assert filter("False") == "No"
-        assert filter("FaLsE") == "No"
-        assert filter(False) == "No"
-        assert filter([None]) == ""
-        assert filter(["whatever", "next"]) == "whatever, next"
-        assert filter(["true", "True", "TrUe", True]) == "Yes, Yes, Yes, Yes"
-        assert filter(["false", "False", "FaLsE", False]) == "No, No, No, No"
+        assert list(filter(None)) == ""
+        assert list(filter([])) == ""
+        assert list(filter("whatever")) == "whatever"
+        assert list(filter("true")) == "Yes"
+        assert list(filter ("True")) == "Yes"
+        assert list(filter("TrUe")) == "Yes"
+        assert list(filter(True)) == "Yes"
+        assert list(filter("false")) == "No"
+        assert list(filter("False")) == "No"
+        assert list(filter("FaLsE")) == "No"
+        assert list(filter(False)) == "No"
+        assert list(filter([None])) == ""
+        assert list(filter(["whatever", "next"])) == "whatever, next"
+        assert list(filter(["true", "True", "TrUe", True])) == "Yes, Yes, Yes, Yes"
+        assert list(filter(["false", "False", "FaLsE", False])) == "No, No, No, No"
 
     def test_02_form_diff_table_subject_expand(self):
         filter = app.jinja_env.filters['form_diff_table_subject_expand']
 
-        assert filter(None) == ""
-        assert filter([]) == ""
-        assert filter("whatever") == "whatever"
-        assert filter("JA1-92") == "Political science (General) [code: JA1-92]"
-        assert filter(["whatever", "JA1-92"]) == "whatever, Political science (General) [code: JA1-92]"
-        assert filter(["JA1-92"]) == "Political science (General) [code: JA1-92]"
-        assert filter(["JA1-92", "D"]) == "Political science (General) [code: JA1-92], History (General) and history of Europe [code: D]"
-        assert filter(["JA1-92", None]) == "Political science (General) [code: JA1-92]"
-        assert filter(["JA1-92", ""]) == "Political science (General) [code: JA1-92]"
+        assert list(filter(None)) == ""
+        assert list(filter([])) == ""
+        assert list(filter("whatever")) == "whatever"
+        assert list(filter("JA1-92")) == "Political science (General) [code: JA1-92]"
+        assert list(filter(["whatever", "JA1-92"])) == "whatever, Political science (General) [code: JA1-92]"
+        assert list(filter(["JA1-92"])) == "Political science (General) [code: JA1-92]"
+        assert list(filter(["JA1-92", "D"])) == "Political science (General) [code: JA1-92], History (General) and history of Europe [code: D]"
+        assert list(filter(["JA1-92", None])) == "Political science (General) [code: JA1-92]"
+        assert list(filter(["JA1-92", ""])) == "Political science (General) [code: JA1-92]"
--- ./doajtest/unit/test_reporting.py	(original)
+++ ./doajtest/unit/test_reporting.py	(refactored)
@@ -46,7 +46,7 @@
     ["Country", "2010", "2011", "2012", "2013", "2014", "2015"],
     ["Angola", 0, 1, 2, 3, 4, 5],
     ["Belarus", 6, 7, 8 , 9, 10, 0],
--- ./doajtest/unit/test_scripts_accounts_with_marketing_consent.py	(original)
+++ ./doajtest/unit/test_scripts_accounts_with_marketing_consent.py	(refactored)
@@ -37,12 +37,12 @@
             pubaccount.save()
         # Create accounts with marketing consent set to True
         expected_data = [[
-          u'ID',
-          u'Name',
-          u'Email',
-          u'Created',
-          u'Last Updated',
-          u'Updated Since Create?'
+          'ID',
+          'Name',
+          'Email',
+          'Created',
+          'Last Updated',
+          'Updated Since Create?'
         ]]
         for i in range(20):
             pubsource = AccountFixtureFactory.make_publisher_source()
@@ -54,12 +54,12 @@
             else:
                 pubaccount.save()
             expected_data.append([
-              unicode(pubaccount.id),
-              unicode(pubaccoRefactoringTool: No changes to ./doajtest/unit/test_sitemap.py
RefactoringTool: No changes to ./doajtest/unit/test_snapshot.py
RefactoringTool: No changes to ./doajtest/unit/test_snapshot_tasks.py
RefactoringTool: No changes to ./doajtest/unit/test_task_article_bulk_delete.py
RefactoringTool: No changes to ./doajtest/unit/test_task_journal_bulk_delete.py
RefactoringTool: Refactored ./doajtest/unit/test_task_journal_bulkedit.py
WARNING: couldn't encode ./doajtest/unit/test_task_journal_bulkedit.py's diff for your terminal
RefactoringTool: Refactored ./doajtest/unit/test_task_suggestion_bulkedit.py
RefactoringTool: Refactored ./doajtest/unit/test_tasks_ingestarticles.py
RefactoringTool: Refactored ./doajtest/unit/test_tasks_public_data_dump.py
RefactoringTool: No changes to ./doajtest/unit/test_tick.py
RefactoringTool: No changes to ./doajtest/unit/test_toc.py
RefactoringTool: No changes to ./doajtest/unit/test_util.py
RefactoringTool: No changes to ./doajtest/unit/test_withdraw_reinstate.py
RefactoringTool: Refactored ./doajtest/unit/test_workflow_emails.py
unt.name),
-              unicode(pubaccount.email),
-              unicode(pubaccount.created_date),
-              unicode(pubaccount.last_updated),
-              unicode('False')
+              str(pubaccount.id),
+              str(pubaccount.name),
+              str(pubaccount.email),
+              str(pubaccount.created_date),
+              str(pubaccount.last_updated),
+              str('False')
             ])
 
         publishers_with_consent(output_file)
--- ./doajtest/unit/test_task_journal_bulkedit.py	(original)
+++ ./doajtest/unit/test_task_journal_bulkedit.py	(refactored)
@@ -169,7 +169,7 @@
                                  doaj_seal=True,
                                  country="AF",
                                  owner="test1",
--- ./doajtest/unit/test_task_suggestion_bulkedit.py	(original)
+++ ./doajtest/unit/test_task_suggestion_bulkedit.py	(refactored)
@@ -198,10 +198,10 @@
 
         at_least_one_validation_fail = False
         for rec in job.audit:
-            if rec['message'].startswith(u'Data validation failed'):
+            if rec['message'].startswith('Data validation failed'):
                 at_least_one_validation_fail = True
-                assert u'{"application_status": ["Not a valid choice"]}' in rec['message']  # the error details
-                assert u'{"application_status": "' + expected_app_status + u'"}' in rec['message']  # the data present in the failed field
+                assert '{"application_status": ["Not a valid choice"]}' in rec['message']  # the error details
+                assert '{"application_status": "' + expected_app_status + '"}' in rec['message']  # the data present in the failed field
 
         assert at_least_one_validation_fail
 
--- ./doajtest/unit/test_tasks_ingestarticles.py	(original)
+++ ./doajtest/unit/test_tasks_ingestarticles.py	(refactored)
@@ -5,7 +5,7 @@
 from doajtest.fixtures.accounts import AccountFixtureFactory
 from portality import models
 from portality.core import app
-import os, requests, ftplib, urlparse
+import os, requests, ftplib, urllib.parse
 from portality.background import BackgroundException, RetryException
 import time
 from portality.crosswalks import article_doaj_xml
--- ./doajtest/unit/test_tasks_public_data_dump.py	(original)
+++ ./doajtest/unit/test_tasks_public_data_dump.py	(refactored)
@@ -14,7 +14,7 @@
 from portality.core import app
 
 import os, shutil, tarfile, json
-from StringIO import StringIO
+from io import StringIO
 
 def load_cases():
     return load_parameter_sets(rel2abs(__file__, "..", "matrices", "tasks.public_data_dump"), "data_dump", "test_id",
@@ -181,10 +181,10 @@
                     assert len(data) == first_article_file_records
 
                     record = data[0]
-                    for key in record.keys():
+                    for key in list(record.keys()):
                         assert key in ["admin", "bibjson", "id", "last_updated", "created_date"]
                     if "admin" in record:
-                        for key in record["admin"].keys():
+                        for key in list(record["admin"].keys()):
                             assert key in ["ticked", "seal"]
 
             if types_arg in ["-", "all", "journal"]:
@@ -202,10 +202,10 @@
                     assert len(data) == first_journal_file_records
 
                     record = data[0]
-                    for key in record.keys():
+                    for key in list(record.keys()):
                         assert key in ["admin", "bibjson", "id", "last_updated", "created_date"]
                     if "admin" in record:
-                        for key in record["admin"].keys():
+                        for key in list(record["admin"].keys()):
                             assert key in ["ticked", "seal"]
 
         else:
--- ./doajtest/unit/test_workflow_emails.py	(original)
+++ ./doajtest/unit/test_workflow_emails.py	(refactored)
@@ -120,11 +120,11 @@
 
         async_workflow_notifications.managing_editor_notifications(emails)
         assert len(emails) > 0
-        assert app.config['MANAGIRefactoringTool: No changes to ./doajtest/unit/test_xwalk.py
RefactoringTool: Refactored ./portality/app.py
RefactoringTool: Refactored ./portality/app_email.py
RefactoringTool: No changes to ./portality/authorise.py
RefactoringTool: No changes to ./portality/background.py
RefactoringTool: No changes to ./portality/blog.py
RefactoringTool: Refactored ./portality/clcsv.py
NG_EDITOR_EMAIL'] in emails.keys()
+        assert app.config['MANAGING_EDITOR_EMAIL'] in list(emails.keys())
 
         email_text_catted = " ".join(emails[app.config['MANAGING_EDITOR_EMAIL']][1])
-        assert u'1 application(s) are assigned to an Associate Editor' in email_text_catted
-        assert u"There are 1 records in status 'Ready'" in email_text_catted
+        assert '1 application(s) are assigned to an Associate Editor' in email_text_catted
+        assert "There are 1 records in status 'Ready'" in email_text_catted
         ctx.pop()
 
     def test_03_workflow_editor_notifications(self):
@@ -163,11 +163,11 @@
         async_workflow_notifications.editor_notifications(emails)
 
         assert len(emails) > 0
-        assert EDITOR_SOURCE['email'] in emails.keys()
+        assert EDITOR_SOURCE['email'] in list(emails.keys())
 
         email_text_catted = " ".join(emails[EDITOR_SOURCE['email']][1])
-        assert u'1 application(s) currently assigned to your Editor Group, "editorgroup", which have no Associate Editor' in email_text_catted
-        assert u"1 application(s) which have been assigned to an Associate Editor but have been idle" in email_text_catted
+        assert '1 application(s) currently assigned to your Editor Group, "editorgroup", which have no Associate Editor' in email_text_catted
+        assert "1 application(s) which have been assigned to an Associate Editor but have been idle" in email_text_catted
 
         ctx.pop()
 
@@ -203,10 +203,10 @@
 
         async_workflow_notifications.associate_editor_notifications(emails)
         assert len(emails) > 0
-        assert ASSED1_SOURCE['email'] in emails.keys()
+        assert ASSED1_SOURCE['email'] in list(emails.keys())
 
         email_text = emails[ASSED1_SOURCE['email']][1].pop()
-        assert u'You have 2 application(s) assigned to you' in email_text
-        assert u'including 1 which have been unchanged' in email_text
+        assert 'You have 2 application(s) assigned to you' in email_text
+        assert 'including 1 which have been unchanged' in email_text
 
         ctx.pop()
--- ./portality/app.py	(original)
+++ ./portality/app.py	(refactored)
@@ -129,7 +129,7 @@
 }
 
 # In each tier, create an ordered dictionary sorted alphabetically by sponsor name
-SPONSORS = {k: OrderedDict(sorted(v.items(), key=lambda t: t[0])) for k, v in SPONSORS.items()}
+SPONSORS = {k: OrderedDict(sorted(list(v.items()), key=lambda t: t[0])) for k, v in list(SPONSORS.items())}
 
 
 # Configure the Google Analytics tracker
@@ -264,9 +264,9 @@
             dvals.append(form_diff_table_comparison_value(v))
         return ", ".join(dvals)
     else:
-        if val is True or (isinstance(val, basestring) and val.lower() == "true"):
+        if val is True or (isinstance(val, str) and val.lower() == "true"):
             return "Yes"
-        elif val is False or (isinstance(val, basestring) and val.lower() == "false"):
+        elif val is False or (isinstance(val, str) and val.lower() == "false"):
             return "No"
         return val
 
--- ./portality/app_email.py	(original)
+++ ./portality/app_email.py	(refactored)
@@ -51,7 +51,7 @@
 
     # ensure everything is unicode
     unicode_params = {}
-    for k, v in template_params.iteritems():
+    for k, v in template_params.items():
         unicode_params[k] = to_unicode(v)
 
     # Get the body text from the msg_body parameter (for a contact form),
@@ -91,9 +91,9 @@
 
 
 def to_unicode(val):
-    if isinstance(val, unicode):
+    if isinstance(val, str):
         return val
-    elif isinstance(val, basestring):
+    elif isinstance(val, str):
         try:
             return val.decode("utf8", "replace")
         except UnicodeDecodeError:
--- ./portality/clcsv.py	(original)
+++ ./portality/clcsv.py	(refactored)
@@ -1,5 +1,5 @@
 import csv, codecs
-import cStringIO
+import io
 
 
 class ClCsv():
@@ -70,7 +70,7 @@
             if type(col_identifier) == int:
                 # get column by index
                 return self.data[col_identifier]
-            elif isinstance(col_identifieRefactoringTool: No changes to ./portality/constants.py
RefactoringTool: Refactored ./portality/core.py
r, basestring):
+            elif isinstance(col_identifier, str):
                 # get column by title
                 for col in self.data:
                     if col[0] == col_identifier:
@@ -87,7 +87,7 @@
         try:
             if type(col_identifier) == int:
                 self.data[col_identifier] = col_contents
-            elif isinstance(col_identifier, basestring):
+            elif isinstance(col_identifier, str):
                 # set column by title.
                 num = self.get_colnumber(col_identifier)
                 if num is not None and type(col_contents) == list:
@@ -99,7 +99,7 @@
             # The column isn't there already; append a new one
             if type(col_identifier) == int:
                 self.data.append(col_contents)
-            elif isinstance(col_identifier, basestring):
+            elif isinstance(col_identifier, str):
                 self.data.append((col_identifier, col_contents))
 
     def get_colnumber(self, header):
@@ -182,8 +182,8 @@
     def __iter__(self):
         return self
 
-    def next(self):
-        val = self.reader.next()
+    def __next__(self):
+        val = next(self.reader)
         raw = val.encode("utf-8")
         if raw.startswith(codecs.BOM_UTF8):
             raw = raw.replace(codecs.BOM_UTF8, '', 1)
@@ -199,9 +199,9 @@
         f = UTF8Recoder(f, encoding)
         self.reader = csv.reader(f, dialect=dialect, **kwds)
 
-    def next(self):
-        row = self.reader.next()
-        return [unicode(s, "utf-8") for s in row]
+    def __next__(self):
+        row = next(self.reader)
+        return [str(s, "utf-8") for s in row]
 
     def __iter__(self):
         return self
@@ -214,7 +214,7 @@
 
     def __init__(self, f, dialect=csv.excel, encoding="utf-8", **kwds):
         # Redirect output to a queue
-        self.queue = cStringIO.StringIO()
+        self.queue = io.StringIO()
         self.writer = csv.writer(self.queue, dialect=dialect, **kwds)
         self.stream = f
         self.encoder = codecs.getincrementalencoder(encoding)()
@@ -224,7 +224,7 @@
         for s in row:
             if s is None:
                 s = ''
-            if not isinstance(s, basestring):
+            if not isinstance(s, str):
                 s = str(s)
             encoded_row.append(s.encode("utf-8"))
         self.writer.writerow(encoded_row)
--- ./portality/core.py	(original)
+++ ./portality/core.py	(refactored)
@@ -50,16 +50,16 @@
     here = os.path.dirname(os.path.abspath(__file__))
     app.config['DOAJENV'] = get_app_env(app)
     config_path = os.path.join(os.path.dirname(here), app.config['DOAJENV'] + '.cfg')
-    print 'Running in ' + app.config['DOAJENV']  # the app.logger is not set up yet (?)
+    print('Running in ' + app.config['DOAJENV'])  # the app.logger is not set up yet (?)
     if os.path.exists(config_path):
         app.config.from_pyfile(config_path)
-        print 'Loaded environment config from ' + config_path
+        print('Loaded environment config from ' + config_path)
 
     # import from app.cfg
     config_path = os.path.join(os.path.dirname(here), 'app.cfg')
     if os.path.exists(config_path):
         app.config.from_pyfile(config_path)
-        print 'Loaded secrets config from ' + config_path
+        print('Loaded secrets config from ' + config_path)
 
 
 def get_app_env(app):
@@ -103,12 +103,12 @@
     es_version = app.config.get("ELASTIC_SEARCH_VERSION", "1.7.5")
 
     # for each mapping (a class may supply multiple), create them in the index
-    for key, mapping in mappings.iteritems():
+    for key, mapping in mappings.items():
         if not esprit.raw.type_exists(conn, key, es_version=es_version):
             r = esprit.raw.put_mapping(conn, key, mapping, es_version=es_version)
-            print "Creating ES Type + Mapping for", key, "; status:", r.status_code
+            print("Creating ES Type + Mapping for", key, "; status:", r.status_code)
         else:
-            print "ES Type + Mapping already exists for", key
+            print("ES Type + Mapping already exists for", key)
 
 
 RefactoringTool: Refactored ./portality/dao.py
def initialise_index(app):
@@ -144,7 +144,7 @@
 
     # a jinja filter that prints to the Flask log
     def jinja_debug(text):
-        print text
+        print(text)
         return ''
     app.jinja_env.filters['debug']=jinja_debug
 
--- ./portality/dao.py	(original)
+++ ./portality/dao.py	(refactored)
@@ -5,7 +5,7 @@
 import re
 
 from portality.core import app
-import urllib2
+import urllib.request, urllib.error, urllib.parse
 import json
 
 import esprit
@@ -55,19 +55,19 @@
     @classmethod
     def makeid(cls):
         """Create a new id for data object overwrite this in specific model types if required"""
-        return unicode(uuid.uuid4().hex)
+        return str(uuid.uuid4().hex)
 
     @property
     def id(self):
         rawid = self.data.get("id", None)
         if rawid is not None:
-            return unicode(rawid)
+            return str(rawid)
         return None
     
     def set_id(self, id=None):
         if id is None:
             id = self.makeid()
-        self.data["id"] = unicode(id)
+        self.data["id"] = str(id)
     
     @property
     def version(self):
@@ -130,12 +130,12 @@
             try:
                 r = requests.post(url, data=d)
                 if r.status_code > 400:
-                    raise ElasticSearchWriteException(u"Error on ES save. Response code {0}".format(r.status_code))
+                    raise ElasticSearchWriteException("Error on ES save. Response code {0}".format(r.status_code))
                 else:
                     break  # everything is OK, so r should now be assigned to the result
 
             except requests.exceptions.ConnectionError:
-                app.logger.exception(u"Failed to connect to ES")
+                app.logger.exception("Failed to connect to ES")
                 attempt += 1
             except ElasticSearchWriteException:
                 try:
@@ -146,15 +146,15 @@
                 # Retries depend on which end the error lies.
                 if 400 <= r.status_code < 500:
                     # Bad request, do not retry as it won't work. Fail with ElasticSearchWriteException.
-                    app.logger.exception(u"Bad Request to ES, save failed. Details: {0}".format(error_details))
+                    app.logger.exception("Bad Request to ES, save failed. Details: {0}".format(error_details))
                     raise
                 elif r.status_code >= 500:
                     # Server error, this could be temporary so we may want to retry
-                    app.logger.exception(u"Server Error from ES, retrying. Details: {0}".format(error_details))
+                    app.logger.exception("Server Error from ES, retrying. Details: {0}".format(error_details))
                     attempt += 1
             except Exception:
                 # if any other exception occurs, make sure it's at least logged.
-                app.logger.exception(u"Unhandled exception in save method of DAO")
+                app.logger.exception("Unhandled exception in save method of DAO")
                 raise
 
             # wait before retrying
@@ -162,8 +162,8 @@
 
         if attempt > retries:
             raise DAOSaveExceptionMaxRetriesReached(
-                u"After {attempts} attempts the record with "
-                u"id {id} failed to save.".format(
+                "After {attempts} attempts the record with "
+                "id {id} failed to save.".format(
                     attempts=attempt, id=self.data['id']))
 
         if blocking:
@@ -249,7 +249,7 @@
         keys = []
         for item in mapping:
             if 'fields' in mapping[item]:
-                for itm in mapping[item]['fields'].keys():
+                for itm in list(mapping[item]['fields'].keys()):
                     if itm != 'exact' and not itm.startswith('_'):
                         keys.append(prefix + itm + app.config['FACET_FIELD'])
             else:
@@ -300,7 +300,7 @@
         if facets:
             if 'facets' not in query:
                 query['facets'] = {}
-            for k, v in facets.items():
+            foRefactoringTool: Refactored ./portality/datasets.py
RefactoringTool: No changes to ./portality/decorators.py
RefactoringTool: No changes to ./portality/error_handler.py
RefactoringTool: Refactored ./portality/lcc.py
RefactoringTool: No changes to ./portality/lock.py
RefactoringTool: Refactored ./portality/ordereddict.py
r k, v in list(facets.items()):
                 query['facets'][k] = {"terms": v}
 
         if terms:
@@ -326,7 +326,7 @@
                 query["query"]["bool"]["must"].append({"terms": {s: should_terms[s]}})
 
         sort_specified = False
-        for k, v in kwargs.items():
+        for k, v in list(kwargs.items()):
             if k == '_from':
                 query['from'] = v
             elif k == 'sort':
@@ -721,7 +721,7 @@
                             return
             else:
                 if (datetime.now() - start_time).total_seconds() >= max_retry_seconds:
-                    raise BlockTimeOutException(u"Attempting to block until record with id {id} appears in Elasticsearch, but this has not happened after {limit}".format(id=id, limit=max_retry_seconds))
+                    raise BlockTimeOutException("Attempting to block until record with id {id} appears in Elasticsearch, but this has not happened after {limit}".format(id=id, limit=max_retry_seconds))
 
             time.sleep(sleep)
 
@@ -809,4 +809,4 @@
 
     @staticmethod
     def url_encode_query(query):
-        return urllib2.quote(json.dumps(query).replace(' ', ''))
+        return urllib.parse.quote(json.dumps(query).replace(' ', ''))
--- ./portality/datasets.py	(original)
+++ ./portality/datasets.py	(refactored)
@@ -25,7 +25,7 @@
 
 for cu in sorted(pycountry.currencies, key=lambda x: x.alpha_3):
     try:
-        currency_options.append((cu.alpha_3, u'{code} - {name}'.format(code=cu.alpha_3, name=cu.name)))
+        currency_options.append((cu.alpha_3, '{code} - {name}'.format(code=cu.alpha_3, name=cu.name)))
         currency_name_opts.append((cu.alpha_3, cu.name))
         currency_options_code_index.append(cu.alpha_3)
     except AttributeError:
@@ -59,14 +59,14 @@
 }
 
 # do not change this - the top-level keys in the licenses dict should always be == to the "type" of each license object
-for lic_type, lic_info in licenses.iteritems():
+for lic_type, lic_info in licenses.items():
     lic_info['type'] = lic_type
     lic_info['title'] = lic_type
 
-license_dict = OrderedDict(sorted(licenses.items(), key=lambda x: x[1]['type']))
+license_dict = OrderedDict(sorted(list(licenses.items()), key=lambda x: x[1]['type']))
 
 main_license_options = []
-for lic_type, lic_info in license_dict.iteritems():
+for lic_type, lic_info in license_dict.items():
     main_license_options.append((lic_type, lic_info['form_label']))
 
 
@@ -106,7 +106,7 @@
     """ get the name of a currency from its code """
     try:
         cur = pycountry.currencies.lookup(code)
-        return u'{code} - {name}'.format(code=cur.alpha_3, name=cur.name)
+        return '{code} - {name}'.format(code=cur.alpha_3, name=cur.name)
     except LookupError:
         return code  # return what was passed in if not found
 
--- ./portality/lcc.py	(original)
+++ ./portality/lcc.py	(refactored)
@@ -43,7 +43,7 @@
         else:
             cpmap[name] = parent
 
-    for child, parent in cpmap.iteritems():
+    for child, parent in cpmap.items():
         cn = nodes.get(child)
         pn = nodes.get(parent)
         if cn is None or pn is None:
--- ./portality/ordereddict.py	(original)
+++ ./portality/ordereddict.py	(refactored)
@@ -75,9 +75,9 @@
         if not self:
             raise KeyError('dictionary is empty')
         if last:
-            key = reversed(self).next()
+            key = next(reversed(self))
         else:
-            key = iter(self).next()
+            key = next(iter(self))
         value = self.pop(key)
         return key, value
 
@@ -106,7 +106,7 @@
     def __repr__(self):
         if not self:
             return '%s()' % (self.__class__.__name__,)
-        return '%s(%r)' % (self.__class__.__name__, self.items())
+        return '%s(%r)' % (self.__class__.__name__, list(self.items()))
 
     def copy(self):
         return self.__class__(self)
@@ -122,7 +122,7 @@
         if isinstance(other, OrderedDict):
             if len(self) != len(other):
                 return False
-            for p, q in  zip(self.items(), other.items()):
+      RefactoringTool: No changes to ./portality/regex.py
RefactoringTool: No changes to ./portality/settings.py
RefactoringTool: Refactored ./portality/store.py
RefactoringTool: Refactored ./portality/upgrade.py
      for p, q in  zip(list(self.items()), list(other.items())):
                 if p != q:
                     return False
             return True
--- ./portality/store.py	(original)
+++ ./portality/store.py	(refactored)
@@ -2,7 +2,7 @@
 from portality.lib import plugin
 
 import os, shutil, codecs, boto3
-from urllib import quote_plus
+from urllib.parse import quote_plus
 
 class StoreException(Exception):
     pass
@@ -245,7 +245,7 @@
     filtered = []
     if filter is not None:
         for fn in filelist:
-            if filter(fn):
+            if list(filter(fn)):
                 filtered.append(fn)
     else:
         filtered = filelist
--- ./portality/upgrade.py	(original)
+++ ./portality/upgrade.py	(refactored)
@@ -43,14 +43,14 @@
     tconn = esprit.raw.Connection(target.get("host"), target.get("index"))
 
     if verbose:
-        print "Source", source
-        print "Target", target
+        print("Source", source)
+        print("Target", target)
 
     # get the defined batch size
     batch_size = definition.get("batch", 500)
 
     for tdef in definition.get("types", []):
-        print "Upgrading", tdef.get("type")
+        print("Upgrading", tdef.get("type"))
         batch = []
         total = 0
         first_page = esprit.raw.search(sconn, tdef.get("type"))
@@ -68,7 +68,7 @@
                     try:
                         result = model_class(**result)
                     except DataStructureException as e:
-                        print "Could not create model for {0}, Error: {1}".format(result['id'], e.message)
+                        print("Could not create model for {0}, Error: {1}".format(result['id'], e.message))
                         continue
 
                 for function_path in tdef.get("functions", []):
@@ -81,7 +81,7 @@
                     # run the tasks specified with this object type
                     tasks = tdef.get("tasks", None)
                     if tasks:
-                        for func_call, kwargs in tasks.iteritems():
+                        for func_call, kwargs in tasks.items():
                             getattr(result, func_call)(**kwargs)
 
                     # run the prep routine for the record
@@ -89,7 +89,7 @@
                         result.prep()
                     except AttributeError:
                         if verbose:
-                            print tdef.get("type"), result.id, "has no prep method - no, pre-save preparation being done"
+                            print(tdef.get("type"), result.id, "has no prep method - no, pre-save preparation being done")
                         pass
 
                     data = result.data
@@ -103,12 +103,12 @@
 
                 batch.append(data)
                 if verbose:
-                    print "added", tdef.get("type"), _id, "to batch update"
+                    print("added", tdef.get("type"), _id, "to batch update")
 
                 # When we have enough, do some writing
                 if len(batch) >= batch_size:
                     total += len(batch)
-                    print datetime.now(), "writing ", len(batch), "to", tdef.get("type"), ";", total, "of", max
+                    print(datetime.now(), "writing ", len(batch), "to", tdef.get("type"), ";", total, "of", max)
                     esprit.raw.bulk(tconn, batch, idkey="doc.id", type_=tdef.get("type"), bulk_type="update")
                     batch = []
                     # do some timing predictions
@@ -117,19 +117,19 @@
                     seconds_so_far = time_so_far.total_seconds()
                     estimated_seconds_remaining = ((seconds_so_far * max) / total) - seconds_so_far
                     estimated_finish = batch_tick + timedelta(seconds=estimated_seconds_remaining)
-                    print 'Estimated finish time for this type {0}.'.format(estimated_finish)
+                    print('Estimated finish time for this type {0}.'.format(estimated_finish))
         except esprit.tasks.ScrollTimeoutException:
             # Try to write the part-batch to index
             if len(batch) >RefactoringTool: Refactored ./portality/util.py
RefactoringTool: Refactored ./portality/api/v1/common.py
 0:
                 total += len(batch)
-                print datetime.now(), "scroll timed out / writing ", len(batch), "to", tdef.get("type"), ";", total, "of", max
+                print(datetime.now(), "scroll timed out / writing ", len(batch), "to", tdef.get("type"), ";", total, "of", max)
                 esprit.raw.bulk(tconn, batch, idkey="doc.id", type_=tdef.get("type"), bulk_type="update")
                 batch = []
 
         # Write the last part-batch to index
         if len(batch) > 0:
             total += len(batch)
-            print datetime.now(), "final result set / writing ", len(batch), "to", tdef.get("type"), ";", total, "of", max
+            print(datetime.now(), "final result set / writing ", len(batch), "to", tdef.get("type"), ";", total, "of", max)
             esprit.raw.bulk(tconn, batch, idkey="doc.id", type_=tdef.get("type"), bulk_type="update")
 
 
@@ -165,22 +165,22 @@
     args = parser.parse_args()
 
     if not args.upgrade:
-        print "Please specify an upgrade package with the -u option"
+        print("Please specify an upgrade package with the -u option")
         exit()
 
     if not (os.path.exists(args.upgrade) and os.path.isfile(args.upgrade)):
-        print args.upgrade, "does not exist or is not a file"
+        print(args.upgrade, "does not exist or is not a file")
         exit()
 
-    print 'Starting {0}.'.format(datetime.now())
+    print('Starting {0}.'.format(datetime.now()))
 
     with open(args.upgrade) as f:
         try:
             instructions = json.loads(f.read(), object_pairs_hook=OrderedDict)
         except:
-            print args.upgrade, "does not parse as JSON"
+            print(args.upgrade, "does not parse as JSON")
             exit()
 
         do_upgrade(instructions, args.verbose)
 
-    print 'Finished {0}.'.format(datetime.now())
+    print('Finished {0}.'.format(datetime.now()))
--- ./portality/util.py	(original)
+++ ./portality/util.py	(refactored)
@@ -1,4 +1,5 @@
-from urllib import urlopen, urlencode
+from urllib.request import urlopen
+from urllib.parse import urlencode
 import md5
 import re, string
 from unicodedata import normalize
@@ -7,7 +8,7 @@
 from random import choice
 import json
 
-from urlparse import urlparse, urljoin
+from urllib.parse import urlparse, urljoin
 
 
 def is_safe_url(target):
@@ -48,14 +49,14 @@
 # derived from http://flask.pocoo.org/snippets/5/ (public domain)
 # changed delimiter to _ instead of - due to ES search problem on the -
 _punct_re = re.compile(r'[\t !"#$%&\'()*\-/<=>?@\[\\\]^_`{|},.]+')
-def slugify(text, delim=u'_'):
+def slugify(text, delim='_'):
     """Generates an slightly worse ASCII-only slug."""
     result = []
     for word in _punct_re.split(text.lower()):
         word = normalize('NFKD', word).encode('ascii', 'ignore')
         if word:
             result.append(word)
-    return unicode(delim.join(result))
+    return str(delim.join(result))
 
 
 # get gravatar for email address
@@ -135,7 +136,7 @@
 def unicode_dict(d):
     """ Recursively convert dictionary keys to unicode """
     if isinstance(d, dict):
-        return dict((unicode(k), unicode_dict(v)) for k, v in d.items())
+        return dict((str(k), unicode_dict(v)) for k, v in list(d.items()))
     elif isinstance(d, list):
         return [unicode_dict(e) for e in d]
     else:
@@ -194,5 +195,5 @@
 def batch_up(long_list, batch_size):
     """Yield successive n-sized chunks from l (a list)."""
     # http://stackoverflow.com/a/312464/1154882
-    for i in xrange(0, len(long_list), batch_size):
+    for i in range(0, len(long_list), batch_size):
         yield long_list[i:i + batch_size]
--- ./portality/api/v1/common.py	(original)
+++ ./portality/api/v1/common.py	(refactored)
@@ -161,10 +161,10 @@
     keys 'next', 'prev' and 'last' defined. The values are the
     corresponding pre-generated links.
     """
-    link_metadata = {k: v for k, v in metadata.iteritems() if k in LINK_HEADERS}
+    link_metadata = {k: v for k, v in metadata.items() if k in LINK_HEADERS}
 
     links = []
-    for k, v in link_metadataRefactoringTool: Refactored ./portality/api/v1/discovery.py
RefactoringTool: No changes to ./portality/api/v1/bulk/applications.py
RefactoringTool: No changes to ./portality/api/v1/bulk/articles.py
RefactoringTool: Refactored ./portality/api/v1/crud/applications.py
RefactoringTool: No changes to ./portality/api/v1/crud/articles.py
RefactoringTool: No changes to ./portality/api/v1/crud/journals.py
RefactoringTool: Refactored ./portality/api/v1/data_objects/application.py
RefactoringTool: Refactored ./portality/api/v1/data_objects/article.py
RefactoringTool: No changes to ./portality/api/v1/data_objects/common_journal_application.py
RefactoringTool: No changes to ./portality/api/v1/data_objects/journal.py
RefactoringTool: No changes to ./portality/bll/doaj.py
RefactoringTool: No changes to ./portality/bll/exceptions.py
RefactoringTool: Refactored ./portality/bll/services/application.py
.iteritems():
+    for k, v in link_metadata.items():
         links.append(Link(v, rel=k))  # e.g. Link("http://example.com/foo", rel="next")
 
     return str(LinkHeader(links))  # RFC compliant headers e.g.
--- ./portality/api/v1/discovery.py	(original)
+++ ./portality/api/v1/discovery.py	(refactored)
@@ -21,19 +21,19 @@
 
 
 def query_substitute(query, substitutions):
-    if len(substitutions.keys()) == 0:
+    if len(list(substitutions.keys())) == 0:
         return query
 
     # apply the regex escapes to the substitutions, so we know they
     # are ready to be matched
     escsubs = {}
-    for k, v in substitutions.iteritems():
+    for k, v in substitutions.items():
         escsubs[k.replace(":", "\\:")] = v
 
     # define a function which takes the match group and returns the
     # substitution if there is one
     def rep(match):
-        for k, v in escsubs.iteritems():
+        for k, v in escsubs.items():
             if k == match.group(1):
                 return v
         return match.group(1)
--- ./portality/api/v1/crud/applications.py	(original)
+++ ./portality/api/v1/crud/applications.py	(refactored)
@@ -323,7 +323,7 @@
     def _validation_message(cls, fc):
         errors = fc.errors
         msg = "The following validation errors were received:\n"
-        for fieldName, errorMessages in errors.iteritems():
+        for fieldName, errorMessages in errors.items():
             fieldName = xwalk.SuggestionFormXWalk.formField2objectField(fieldName)
             msg += fieldName + " : " + "; ".join(errorMessages) + "\n"
         return msg
--- ./portality/api/v1/data_objects/application.py	(original)
+++ ./portality/api/v1/data_objects/application.py	(refactored)
@@ -326,7 +326,7 @@
 
     def custom_validate(self):
         # only attempt to validate if this is not a blank object
-        if len(self.data.keys()) == 0:
+        if len(list(self.data.keys())) == 0:
             return
 
         # at least one of print issn / e-issn, and they must be different
--- ./portality/api/v1/data_objects/article.py	(original)
+++ ./portality/api/v1/data_objects/article.py	(refactored)
@@ -170,7 +170,7 @@
 
     def custom_validate(self):
         # only attempt to validate if this is not a blank object
-        if len(self.data.keys()) == 0:
+        if len(list(self.data.keys())) == 0:
             return
 
         # at least one of print issn / e-issn, and they must be different
--- ./portality/bll/services/application.py	(original)
+++ ./portality/bll/services/application.py	(refactored)
@@ -29,7 +29,7 @@
             {"arg": application, "instance" : models.Suggestion, "allow_none" : False, "arg_name" : "application"},
             {"arg" : account, "instance" : models.Account, "allow_none" : False, "arg_name" : "account"},
             {"arg" : provenance, "instance" : bool, "allow_none" : False, "arg_name" : "provenance"},
-            {"arg" : note, "instance" : basestring, "allow_none" : True, "arg_name" : "note"},
+            {"arg" : note, "instance" : str, "allow_none" : True, "arg_name" : "note"},
             {"arg" : manual_update, "instance" : bool, "allow_none" : False, "arg_name" : "manual_update"}
         ], exceptions.ArgumentException)
 
@@ -173,7 +173,7 @@
         """
         # first validate the incoming arguments to ensure that we've got the right thing
         argvalidate("update_request_for_journal", [
-            {"arg": journal_id, "instance" : basestring, "allow_none" : False, "arg_name" : "journal_id"},
+            {"arg": journal_id, "instance" : str, "allow_none" : False, "arg_name" : "journal_id"},
             {"arg" : account, "instance" : models.Account, "allow_none" : True, "arg_name" : "account"},
             {"arg" : lock_timeout, "instance" : int, "allow_none" : True, "arg_name" : "lock_timeout"}
         ], exceptions.ArgumentException)
@@ -393,7 +393,7 @@
         """
         # first validate the incoming arguments to ensure that we've got the right thing
         argvalidate("delete_application", [
-            {"arg": application_id, "instance" : unicoRefactoringTool: Refactored ./portality/bll/services/article.py
de, "allow_none" : False, "arg_name" : "application_id"},
+            {"arg": application_id, "instance" : str, "allow_none" : False, "arg_name" : "application_id"},
             {"arg" : account, "instance" : models.Account, "allow_none" : False, "arg_name" : "account"}
         ], exceptions.ArgumentException)
 
--- ./portality/bll/services/article.py	(original)
+++ ./portality/bll/services/article.py	(refactored)
@@ -190,7 +190,7 @@
         # first validate the incoming arguments to ensure that we've got the right thing
         argvalidate("is_legitimate_owner", [
             {"arg": article, "instance" : models.Article, "allow_none" : False, "arg_name" : "article"},
-            {"arg" : owner, "instance" : unicode, "allow_none" : False, "arg_name" : "owner"}
+            {"arg" : owner, "instance" : str, "allow_none" : False, "arg_name" : "owner"}
         ], exceptions.ArgumentException)
 
         # get all the issns for the article
@@ -252,7 +252,7 @@
         # first validate the incoming arguments to ensure that we've got the right thing
         argvalidate("issn_ownership_status", [
             {"arg": article, "instance" : models.Article, "allow_none" : False, "arg_name" : "article"},
-            {"arg" : owner, "instance" : unicode, "allow_none" : False, "arg_name" : "owner"}
+            {"arg" : owner, "instance" : str, "allow_none" : False, "arg_name" : "owner"}
         ], exceptions.ArgumentException)
 
         # get all the issns for the article
@@ -278,10 +278,10 @@
                         seen_issns[issn].add(j.owner)
 
         for issn in issns:
-            if issn not in seen_issns.keys():
+            if issn not in list(seen_issns.keys()):
                 unmatched.append(issn)
 
-        for issn, owners in seen_issns.iteritems():
+        for issn, owners in seen_issns.items():
             owners = list(owners)
             if len(owners) == 0:
                 unowned.append(issn)
@@ -310,7 +310,7 @@
         # first validate the incoming arguments to ensure that we've got the right thing
         argvalidate("get_duplicate", [
             {"arg": article, "instance" : models.Article, "allow_none" : False, "arg_name" : "article"},
-            {"arg" : owner, "instance" : unicode, "allow_none" : True, "arg_name" : "owner"}
+            {"arg" : owner, "instance" : str, "allow_none" : True, "arg_name" : "owner"}
         ], exceptions.ArgumentException)
 
         dup = self.get_duplicates(article, owner, max_results=2)
@@ -334,7 +334,7 @@
         # first validate the incoming arguments to ensure that we've got the right thing
         argvalidate("get_duplicates", [
             {"arg": article, "instance" : models.Article, "allow_none" : False, "arg_name" : "article"},
-            {"arg" : owner, "instance" : unicode, "allow_none" : True, "arg_name" : "owner"}
+            {"arg" : owner, "instance" : str, "allow_none" : True, "arg_name" : "owner"}
         ], exceptions.ArgumentException)
 
         possible_articles_dict = self.discover_duplicates(article, owner, max_results)
@@ -342,7 +342,7 @@
             return []
 
         # We don't need the details of duplicate types, so flatten the lists.
-        all_possible_articles = [article for dup_type in possible_articles_dict.values() for article in dup_type]
+        all_possible_articles = [article for dup_type in list(possible_articles_dict.values()) for article in dup_type]
 
         # An article may fulfil more than one duplication criteria, so needs to be de-duplicated
         ids = []
@@ -370,7 +370,7 @@
         # first validate the incoming arguments to ensure that we've got the right thing
         argvalidate("discover_duplicates", [
             {"arg": article, "instance" : models.Article, "allow_none" : False, "arg_name" : "article"},
-            {"arg" : owner, "instance" : unicode, "allow_none" : True, "arg_name" : "owner"}
+            {"arg" : owner, "instance" : str, "allow_none" : True, "arg_name" : "owner"}
         ], exceptions.ArgumentException)
 
         # Get the owner's ISSNs
@@ -393,7 +393,7 RefactoringTool: No changes to ./portality/bll/services/authorisation.py
RefactoringTool: Refactored ./portality/bll/services/journal.py
RefactoringTool: Refactored ./portality/bll/services/query.py
RefactoringTool: Refactored ./portality/crosswalks/article_doaj_xml.py
RefactoringTool: No changes to ./portality/crosswalks/article_form.py
RefactoringTool: No changes to ./portality/crosswalks/exceptions.py
RefactoringTool: No changes to ./portality/crosswalks/journal_questions.py
RefactoringTool: Refactored ./portality/formcontext/choices.py
RefactoringTool: No changes to ./portality/formcontext/emails.py
RefactoringTool: Refactored ./portality/formcontext/fields.py
RefactoringTool: Refactored ./portality/formcontext/formcontext.py
RefactoringTool: Refactored ./portality/formcontext/formhelper.py
@@
         # dois = b.get_identifiers(b.DOI)
         doi = article.get_normalised_doi()
         if doi is not None:
-            if isinstance(doi, basestring) and doi != '':
+            if isinstance(doi, str) and doi != '':
                 articles = models.Article.duplicates(issns=issns, doi=doi, size=results_per_match_type)
                 if len(articles) > 0:
                     possible_articles['doi'] = [a for a in articles if a.id != article.id]
--- ./portality/bll/services/journal.py	(original)
+++ ./portality/bll/services/journal.py	(refactored)
@@ -153,7 +153,7 @@
                 article_kvs = _get_article_kvs(j)
                 cols[issn] = kvs + meta_kvs + article_kvs
 
-            issns = cols.keys()
+            issns = list(cols.keys())
             issns.sort()
 
             csvwriter = clcsv.UnicodeWriter(file_object)
--- ./portality/bll/services/query.py	(original)
+++ ./portality/bll/services/query.py	(refactored)
@@ -109,7 +109,7 @@
         # check that the request values permit a query to this endpoint
         required_parameters = cfg.get("required_parameters")
         if required_parameters is not None:
-            for k, vs in required_parameters.iteritems():
+            for k, vs in required_parameters.items():
                 val = additional_parameters.get(k)
                 if val is None or val not in vs:
                     raise exceptions.AuthoriseException()
@@ -166,7 +166,7 @@
         if "query" in self.q:
             current_query = deepcopy(self.q["query"])
             del self.q["query"]
-            if len(current_query.keys()) == 0:
+            if len(list(current_query.keys())) == 0:
                 current_query = None
 
         self.filtered = True
--- ./portality/crosswalks/article_doaj_xml.py	(original)
+++ ./portality/crosswalks/article_doaj_xml.py	(refactored)
@@ -5,7 +5,7 @@
 from portality.crosswalks.exceptions import CrosswalkException
 from portality import models
 from datetime import datetime
-from urlparse import urlparse
+from urllib.parse import urlparse
 
 class DOAJXWalk(object):
     format_name = "doaj"
--- ./portality/formcontext/choices.py	(original)
+++ ./portality/formcontext/choices.py	(refactored)
@@ -402,7 +402,7 @@
         role_choices = cls.application_status(role)
 
         # Get the full tuple of application status for the current source
-        [full_current_status] = filter(lambda x: status in x, role_choices)
+        [full_current_status] = [x for x in role_choices if status in x]
 
         # Admins can set any role at any time
         if role == 'admin':
--- ./portality/formcontext/fields.py	(original)
+++ ./portality/formcontext/fields.py	(refactored)
@@ -39,9 +39,9 @@
 
     def _value(self):
         if self.data:
-            return u', '.join(self.data)
+            return ', '.join(self.data)
         else:
-            return u''
+            return ''
 
     def get_list(self):
         return self.data
@@ -66,7 +66,7 @@
             self.data = assumed_scheme + val
         else:
             if val == assumed_scheme:  # just to prevent http:// from showing up on its own in all the URL fields when you make a mistake elsewhere
-                self.data = u''
+                self.data = ''
             else:
                 self.data = val
 
--- ./portality/formcontext/formcontext.py	(original)
+++ ./portality/formcontext/formcontext.py	(refactored)
@@ -553,7 +553,7 @@
 
     def _form_diff(self, journal_form, application_form):
         diff = []
-        for k, v in application_form.iteritems():
+        for k, v in application_form.items():
             try:
                 q = self.form[k].label
             except KeyError:
--- ./portality/formcontext/formhelper.py	(original)
+++ ./portality/formcontext/formhelper.py	(refactored)
@@ -60,7 +60,7 @@
             if render_subfields_horizontal and not (subfield.type == 'CSRFTokenField' and not subfield.value):
                 subfield_width = "3"
                 remove = []
-                for kwarg, val in kwargs.iteritems():
+                for kwRefactoringTool: No changes to ./portality/formcontext/forms.py
RefactoringTool: Refactored ./portality/formcontext/render.py
arg, val in kwargs.items():
                     if kwarg == 'subfield_display-' + subfield.short_name:
                         subfield_width = val
                         remove.append(kwarg)
@@ -138,7 +138,7 @@
                 kwargs["class"] = clazz
             render_args = {}
             # filter anything that shouldn't go in as a field attribute
-            for k, v in kwargs.iteritems():
+            for k, v in kwargs.items():
                 if k in ["class", "style", "disabled"] or k.startswith("data-"):
                     render_args[k] = v
             frag += field(**render_args) # FIXME: this is probably going to do some weird stuff
@@ -168,7 +168,7 @@
         frag += field(**kwargs)
         frag += '<span class="label-text">' + field.label.text + '</span>'
 
-        if field.label.text in extra_input_fields.keys():
+        if field.label.text in list(extra_input_fields.keys()):
             frag += "&nbsp;" + extra_input_fields[field.label.text](**{"class" : "extra_input_field"})
 
         frag += "</label>"
@@ -183,7 +183,7 @@
         frag += field(**kwargs)
         frag += '<label class="control-label" for="' + field.short_name + '">' + field.label.text + '</label>'
 
-        if field.label.text in extra_input_fields.keys():
+        if field.label.text in list(extra_input_fields.keys()):
             eif = extra_input_fields[field.label.text]
             if not isinstance(eif, UnboundField):
                 frag += "&nbsp;" + extra_input_fields[field.label.text](**{"class" : "extra_input_field"})
@@ -252,7 +252,7 @@
             if render_subfields_horizontal and not (subfield.type == 'CSRFTokenField' and not subfield.value):
                 subfield_width = "3"
                 remove = []
-                for kwarg, val in kwargs.iteritems():
+                for kwarg, val in kwargs.items():
                     if kwarg == 'subfield_display-' + subfield.short_name:
                         subfield_width = val
                         remove.append(kwarg)
@@ -349,7 +349,7 @@
         frag += field(**kwargs)
         frag += '<span class="label-text">' + field.label.text + '</span>'
 
-        if field.label.text in extra_input_fields.keys():
+        if field.label.text in list(extra_input_fields.keys()):
             frag += "&nbsp;" + extra_input_fields[field.label.text](**{"class" : "extra_input_field"})
 
         frag += "</label>"
@@ -363,7 +363,7 @@
         frag += field(**kwargs)
         frag += '<label for="' + field.short_name + '">' + field.label.text + '</label>'
 
-        if field.label.text in extra_input_fields.keys():
+        if field.label.text in list(extra_input_fields.keys()):
             eif = extra_input_fields[field.label.text]
             if not isinstance(eif, UnboundField):
                 frag += "&nbsp;" + extra_input_fields[field.label.text](**{"class" : "extra_input_field"})
--- ./portality/formcontext/render.py	(original)
+++ ./portality/formcontext/render.py	(refactored)
@@ -31,7 +31,7 @@
         # build the frag
         frag = ""
         for entry in group_def:
-            field_name = entry.keys()[0]
+            field_name = list(entry.keys())[0]
             config = entry.get(field_name)
             config = deepcopy(config)
 
@@ -72,7 +72,7 @@
     def _rewrite_extra_fields(self, form_context, config):
         if "extra_input_fields" in config:
             config = deepcopy(config)
-            for opt, field_ref in config.get("extra_input_fields").iteritems():
+            for opt, field_ref in config.get("extra_input_fields").items():
                 extra_field = form_context.form[field_ref]
                 config["extra_input_fields"][opt] = extra_field
         return config
@@ -248,7 +248,7 @@
         for g in self.NUMBERING_ORDER:
             cfg = self.FIELD_GROUPS.get(g)
             for obj in cfg:
-                field = obj.keys()[0]
+                field = list(obj.keys())[0]
                 obj[field]["q_num"] = str(q)
                 q += 1
 
@@ -256,7 +256,7 @@
         for g in self.FIELD_GROUPS:
      RefactoringTool: Refactored ./portality/formcontext/validate.py
RefactoringTool: Refactored ./portality/formcontext/xwalk.py
RefactoringTool: No changes to ./portality/lib/analytics.py
RefactoringTool: No changes to ./portality/lib/anon.py
RefactoringTool: No changes to ./portality/lib/argvalidate.py
RefactoringTool: Refactored ./portality/lib/dataobj.py
       cfg = self.FIELD_GROUPS.get(g)
             for obj in cfg:
-                f = obj.keys()[0]
+                f = list(obj.keys())[0]
                 if f == field and "q_num" in obj[f]:
                     return obj[f]["q_num"]
         return ""
@@ -273,7 +273,7 @@
             # it for errors - there are no fields to check.
             if cfg:
                 for obj in cfg:
-                    field = obj.keys()[0]
+                    field = list(obj.keys())[0]
                     if field in self.error_fields:
                         obj[field]["first_error"] = True
                         found = True
@@ -485,7 +485,7 @@
         if field_group_name == "old_journal_fields":
             display_old_journal_fields = False
             for old_field_def in self.FIELD_GROUPS["old_journal_fields"]:
-                old_field_name = old_field_def.keys()[0]
+                old_field_name = list(old_field_def.keys())[0]
                 old_field = getattr(form_context.form, old_field_name)
                 if old_field:
                     if old_field.data and old_field.data != 'None':
--- ./portality/formcontext/validate.py	(original)
+++ ./portality/formcontext/validate.py	(refactored)
@@ -281,7 +281,7 @@
         return self.__validate(field.data)
 
     def __validate(self, username):
-        if not isinstance(username, basestring):
+        if not isinstance(username, str):
             raise validators.ValidationError('Invalid username (not a string) passed to ReservedUsernames validator.')
 
         if username.lower() in [u.lower() for u in app.config.get('RESERVED_USERNAMES', [])]:
--- ./portality/formcontext/xwalk.py	(original)
+++ ./portality/formcontext/xwalk.py	(refactored)
@@ -13,7 +13,7 @@
     for cv in current_values:
         if cv not in allowed_values:
             foreign_values[current_values.index(cv)] = cv
-    ps = foreign_values.keys()
+    ps = list(foreign_values.keys())
     ps.sort()
 
     # FIXME: if the data is broken, just return it as is
@@ -31,7 +31,7 @@
 
 def interpret_special(val):
     # if you modify this, make sure to modify reverse_interpret_special as well
-    if isinstance(val, basestring):
+    if isinstance(val, str):
         if val.lower() == Choices.TRUE.lower():
             return True
         elif val.lower() == Choices.FALSE.lower():
@@ -98,7 +98,7 @@
         More technically: the value which triggers considering and adding the data in other_field to value.
     '''
     # if you modify this, make sure to modify reverse_interpret_other too
-    if isinstance(value, basestring):
+    if isinstance(value, str):
         if value == other_value:
             return other_field_data
     elif isinstance(value, list):
@@ -128,7 +128,7 @@
     # if you modify this, make sure to modify interpret_other too
     other_field_val = ''
 
-    if isinstance(interpreted_value, basestring):
+    if isinstance(interpreted_value, str):
         # A special case first: where the value is the empty string.
         # In that case, the main field was never submitted (e.g. if it was
         # a choice of "Yes", "No" and "Other", none of those were submitted
--- ./portality/lib/dataobj.py	(original)
+++ ./portality/lib/dataobj.py	(refactored)
@@ -3,7 +3,7 @@
 from portality.lib import dates
 from portality.datasets import get_country_code, get_currency_code
 from copy import deepcopy
-import locale, json, urlparse, warnings
+import locale, json, urllib.parse, warnings
 from datetime import date, datetime
 
 #########################################################
@@ -29,15 +29,15 @@
 
 def to_unicode():
     def to_utf8_unicode(val):
-        if isinstance(val, unicode):
+        if isinstance(val, str):
             return val
-        elif isinstance(val, basestring):
+        elif isinstance(val, str):
             try:
                 return val.decode("utf8", "strict")
             except UnicodeDecodeError:
-                raise ValueError(u"Could not decode string")
+                raise ValueError("Could not decode string")
         else:
-            return unicode(val)
+            return str(val)
 
     return to_utf8_unicode
 
@@ -51,7 +51,7 @@
     def intify(val):
         # strip any characters that are outside the ascii range - they won't make up the int anyway
         # and this will get rid of things like strange currency marks
-        if isinstance(val, unicode):
+        if isinstance(val, str):
             val = val.encode("ascii", errors="ignore")
 
         # try the straight cast
@@ -72,7 +72,7 @@
         except ValueError:
             pass
 
-        raise ValueError(u"Could not convert string to int: {x}".format(x=val))
+        raise ValueError("Could not convert string to int: {x}".format(x=val))
 
     return intify
 
@@ -80,7 +80,7 @@
     def floatify(val):
         # strip any characters that are outside the ascii range - they won't make up the float anyway
         # and this will get rid of things like strange currency marks
-        if isinstance(val, unicode):
+        if isinstance(val, str):
             val = val.encode("ascii", errors="ignore")
 
         # try the straight cast
@@ -101,7 +101,7 @@
         except ValueError:
             pass
 
-        raise ValueError(u"Could not convert string to float: {x}".format(x=val))
+        raise ValueError("Could not convert string to float: {x}".format(x=val))
 
     return floatify
 
@@ -158,8 +158,8 @@
     return isolang
 
 def to_url(val):
-    if not isinstance(val, basestring):
-        raise ValueError(u"Argument passed to to_url was not a string, but type '{t}': '{val}'".format(t=type(val),val=val))
+    if not isinstance(val, str):
+        raise ValueError("Argument passed to to_url was not a string, but type '{t}': '{val}'".format(t=type(val),val=val))
 
     val = val.strip()
 
@@ -167,14 +167,14 @@
         return val
 
     # parse with urlparse
-    url = urlparse.urlparse(val)
+    url = urllib.parse.urlparse(val)
 
     # now check the url has the minimum properties that we require
     if url.scheme and url.scheme.startswith("http"):
         uc = to_unicode()
         return uc(val)
     else:
-        raise ValueError(u"Could not convert string {val} to viable URL".format(val=val))
+        raise ValueError("Could not convert string {val} to viable URL".format(val=val))
 
 def to_bool(val):
     """Conservative boolean cast - don't cast lists and objects to True, just existing booleans and strings."""
@@ -183,14 +183,14 @@
     if val is True or val is False:
         return val
 
-    if isinstance(val, basestring):
+    if isinstance(val, str):
         if val.lower() == 'true':
             return True
         elif val.lower() == 'false':
             return False
-        raise ValueError(u"Could not convert string {val} to boolean. Expecting string to either say 'true' or 'false' (not case-sensitive).".format(val=val))
-
-    raise ValueError(u"Could not convert {val} to boolean. Expect either boolean or string.".format(val=val))
+        raise ValueError("Could not convert string {val} to boolean. Expecting string to either say 'true' or 'false' (not case-sensitive).".format(val=val))
+
+    raise ValueError("Could not convert {val} to boolean. Expect either boolean or string.".format(val=val))
 
 def string_canonicalise(canon, allow_fail=False):
     normalised = {}
@@ -417,7 +417,7 @@
         pass
 
     def populate(self, fields_and_values):
-        for k, v in fields_and_values.iteritems():
+        for k, v in fields_and_values.items():
             setattr(self, k, v)
 
     def clone(self):
@@ -564,7 +564,7 @@
         props = []
         try:
             # props = og(self, 'properties').keys()
-            props = self._properties.keys()
+            props = list(self._properties.keys())
         except AttributeError:
             pass
 
@@ -574,7 +574,7 @@
                 if self._struct:
                     data_attrs = construct_data_keys(self._struct)
                 else:
-                    data_attrs = self.data.keys()
+                    data_attrs = list(self.data.keys())
         except AttributeError:
             pass
 
@@ -644,10 +644,10 @@
                         pass
 
                 matches = 0
-                for k, v in matchsub.iteritems():
+                for k, v in matchsub.items():
                     if entry.get(k) == v:
                         matches += 1
-                if matches == len(matchsub.keys()):
+                if matches == len(list(matchsub.keys())):
                     removes.append(i)
             i += 1
 
@@ -679,8 +679,8 @@
         while len(stack) > 0:
             context = stack.pop()
             todelete = []
-            for k, v in context.iteritems():
-                if isinstance(v, dict) and len(v.keys()) == 0:
+            for k, v in context.items():
+                if isinstance(v, dict) and len(list(v.keys())) == 0:
                     todelete.append(k)
             for d in todelete:
                 del context[d]
@@ -693,7 +693,7 @@
         except (ValueError, TypeError):
             if accept_failure:
                 return val
-            raise DataSchemaException(u"Cast with {x} failed on '{y}' of type {z}".format(x=cast, y=val, z=type(val)))
+            raise DataSchemaException("Cast with {x} failed on '{y}' of type {z}".format(x=cast, y=val, z=type(val)))
 
     def _get_single(self, path, coerce=None, default=None, allow_coerce_failure=True):
         # get the value at the point in the object
@@ -722,7 +722,7 @@
 
         # check that the val is actually a list
         if not isinstance(val, list):
-            raise DataSchemaException(u"Expecting a list at {x} but found {y}".format(x=path, y=val))
+            raise DataSchemaException("Expecting a list at {x} but found {y}".format(x=path, y=val))
 
         # if there is a value, do we want to coerce each of them
         if coerce is not None:
@@ -743,14 +743,14 @@
             return
 
         if val is None and not allow_none:
-            raise DataSchemaException(u"NoneType is not allowed at {x}".format(x=path))
+            raise DataSchemaException("NoneType is not allowed at {x}".format(x=path))
 
         # first see if we need to coerce the value (and don't coerce None)
         if coerce is not None and val is not None:
             val = self._coerce(val, coerce, accept_failure=allow_coerce_failure)
 
         if allowed_values is not None and val not in allowed_values:
-            raise DataSchemaException(u"Value {x} is not permitted at {y}".format(x=val, y=path))
+            raise DataSchemaException("Value {x} is not permitted at {y}".format(x=val, y=path))
 
         if allowed_range is not None:
             lower, upper = allowed_range
@@ -771,7 +771,7 @@
         for v in val:
             if v is None and not allow_none:
                 if not ignore_none:
-                    raise DataSchemaException(u"NoneType is not allowed at {x}".format(x=path))
+                    raise DataSchemaException("NoneType is not allowed at {x}".format(x=path))
 
         # now coerce each of the values, stripping out Nones if necessary
         val = [self._coerce(v, coerce, accept_failure=allow_coerce_failure) for v in val if v is not None or not ignore_none]
@@ -784,7 +784,7 @@
                 return
             elif not allow_none:
                 # if we are not ignoring nones, and not allowing them, raise an error
-                raise DataSchemaException(u"Empty array not permitted at {x}".format(x=path))
+                raise DataSchemaException("Empty array not permitted at {x}".format(x=path))
 
         # now set it on the path
         self._set_path(path, val)
@@ -794,7 +794,7 @@
             return
 
         if val is None and not allow_none:
-            raise DataSchemaException(u"NoneType is not allowed in list at {x}".format(x=path))
+            raise DataSchemaException("NoneType is not allowed in list at {x}".format(x=path))
 
         # first coerce the value
         if coerce is not None:
@@ -831,7 +831,7 @@
     def _add_to_list_with_struct(self, path, val):
         type, struct, instructions = construct_lookup(path, self._struct)
         if type != "list":
-            raise DataStructureException(u"Attempt to add to list {x} failed - it is not a list element".format(x=path))
+            raise DataStructureException("Attempt to add to list {x} failed - it is not a list element".format(x=path))
         if struct is not None:
             val = construct(val, struct, self._coerce_map)
         kwargs = construct_kwargs(type, "set", instructions)
@@ -883,7 +883,7 @@
     # all fields
     allowed = schema.get("bools", []) + schema.get("fields", []) + schema.get("lists", []) + schema.get("objects", [])
 
-    for k, v in obj.iteritems():
+    for k, v in obj.items():
         # is k allowed at all
         if k not in allowed:
             raise ObjectSchemaValidationError("object contains key " + k + " which is not permitted by schema")
@@ -895,7 +895,7 @@
 
         # check that the fields are plain old strings
         if k in schema.get("fields", []):
-            if type(v) != str and type(v) != unicode and type(v) != int and type(v) != float:
+            if type(v) != str and type(v) != str and type(v) != int and type(v) != float:
                 raise ObjectSchemaValidationError("object contains " + k + " = " + str(v) + " but expected string, unicode or a number")
 
         # check that the lists are really lists
@@ -907,7 +907,7 @@
             if entry_schema is None:
                 # validate the entries as fields
                 for e in v:
-                    if type(e) != str and type(e) != unicode and type(e) != int and type(e) != float:
+                    if type(e) != str and type(e) != str and type(e) != int and type(e) != float:
                         raise ObjectSchemaValidationError("list in object contains " + str(type(e)) + " but expected string, unicode or a number in " + k)
             else:
                 # validate each entry against the schema
@@ -958,60 +958,60 @@
     }
     """
     # check that only the allowed keys are present
-    keys = struct.keys()
+    keys = list(struct.keys())
     for k in keys:
         if k not in ["fields", "objects", "lists", "required", "structs"]:
             c = context if context != "" else "root"
-            raise ConstructException(u"Key '{x}' present in struct at '{y}', but is not permitted".format(x=k, y=c))
+            raise ConstructException("Key '{x}' present in struct at '{y}', but is not permitted".format(x=k, y=c))
 
     # now go through and make sure the fields are the right shape:
-    for field_name, instructions in struct.get("fields", {}).iteritems():
+    for field_name, instructions in struct.get("fields", {}).items():
         if "coerce" not in instructions:
             c = context if context != "" else "root"
-            raise ConstructException(u"Coerce function not listed in field '{x}' at '{y}'".format(x=field_name, y=c))
-        for k,v in instructions.iteritems():
-            if not isinstance(v, list) and not isinstance(v, basestring):
+            raise ConstructException("Coerce function not listed in field '{x}' at '{y}'".format(x=field_name, y=c))
+        for k,v in instructions.items():
+            if not isinstance(v, list) and not isinstance(v, str):
                 c = context if context != "" else "root"
-                raise ConstructException(u"Argument '{a}' in field '{b}' at '{c}' is not a string or list".format(a=k, b=field_name, c=c))
+                raise ConstructException("Argument '{a}' in field '{b}' at '{c}' is not a string or list".format(a=k, b=field_name, c=c))
 
     # then make sure the objects are ok
     for o in struct.get("objects", []):
-        if not isinstance(o, basestring):
+        if not isinstance(o, str):
             c = context if context != "" else "root"
-            raise ConstructException(u"There is a non-string value in the object list at '{y}'".format(y=c))
+            raise ConstructException("There is a non-string value in the object list at '{y}'".format(y=c))
 
     # make sure the lists are correct
-    for field_name, instructions in struct.get("lists", {}).iteritems():
+    for field_name, instructions in struct.get("lists", {}).items():
         contains = instructions.get("contains")
         if contains is None:
             c = context if context != "" else "root"
-            raise ConstructException(u"No 'contains' argument in list definition for field '{x}' at '{y}'".format(x=field_name, y=c))
+            raise ConstructException("No 'contains' argument in list definition for field '{x}' at '{y}'".format(x=field_name, y=c))
         if contains not in ["object", "field"]:
             c = context if context != "" else "root"
-            raise ConstructException(u"'contains' argument in list '{x}' at '{y}' contains illegal value '{z}'".format(x=field_name, y=c, z=contains))
-        for k,v in instructions.iteritems():
-            if not isinstance(v, list) and not isinstance(v, basestring):
+            raise ConstructException("'contains' argument in list '{x}' at '{y}' contains illegal value '{z}'".format(x=field_name, y=c, z=contains))
+        for k,v in instructions.items():
+            if not isinstance(v, list) and not isinstance(v, str):
                 c = context if context != "" else "root"
-                raise ConstructException(u"Argument '{a}' in list '{b}' at '{c}' is not a string or list".format(a=k, b=field_name, c=c))
+                raise ConstructException("Argument '{a}' in list '{b}' at '{c}' is not a string or list".format(a=k, b=field_name, c=c))
 
     # make sure the requireds are correct
     for o in struct.get("required", []):
-        if not isinstance(o, basestring):
+        if not isinstance(o, str):
             c = context if context != "" else "root"
-            raise ConstructException(u"There is a non-string value in the required list at '{y}'".format(y=c))
+            raise ConstructException("There is a non-string value in the required list at '{y}'".format(y=c))
 
     # now do the structs, which will involve some recursion
     substructs = struct.get("structs", {})
 
     # first check that there are no previously unknown keys in there
-    possibles = struct.get("objects", []) + struct.get("lists", {}).keys()
+    possibles = struct.get("objects", []) + list(struct.get("lists", {}).keys())
     for s in substructs:
         if s not in possibles:
             c = context if context != "" else "root"
-            raise ConstructException(u"struct contains key '{a}' which is not listed in object or list definitions at '{x}'".format(a=s, x=c))
+            raise ConstructException("struct contains key '{a}' which is not listed in object or list definitions at '{x}'".format(a=s, x=c))
 
     # now recurse into each struct
-    for k,v in substructs.iteritems():
+    for k,v in substructs.items():
         nc = context
         if nc == "":
             nc = k
@@ -1053,10 +1053,10 @@
 
     # check that all the required fields are there
     try:
-        keys = obj.keys()
+        keys = list(obj.keys())
     except:
         c = context if context != "" else "root"
-        raise DataStructureException(u"Expected an object at {c} but found something else instead".format(c=c))
+        raise DataStructureException("Expected an object at {c} but found something else instead".format(c=c))
 
     for r in struct.get("required", []):
         if r not in keys:
@@ -1067,7 +1067,7 @@
     # Note that since the construct mechanism copies fields explicitly, silent_prune literally just turns off this
     # check
     if not silent_prune:
-        allowed = struct.get("fields", {}).keys() + struct.get("objects", []) + struct.get("lists", {}).keys()
+        allowed = list(struct.get("fields", {}).keys()) + struct.get("objects", []) + list(struct.get("lists", {}).keys())
         for k in keys:
             if k not in allowed:
                 c = context if context != "" else "root"
@@ -1078,7 +1078,7 @@
     constructed = DataObj()
 
     # now check all the fields
-    for field_name, instructions in struct.get("fields", {}).iteritems():
+    for field_name, instructions in struct.get("fields", {}).items():
         val = obj.get(field_name)
         if val is None:
             continue
@@ -1091,7 +1091,7 @@
         try:
             constructed._set_single(field_name, val, coerce=coerce_fn, **kwargs)
         except DataSchemaException as e:
-            raise DataStructureException(u"Schema exception at '{a}', {b}".format(a=context + field_name, b=e.message))
+            raise DataStructureException("Schema exception at '{a}', {b}".format(a=context + field_name, b=e.message))
 
     # next check all the objetcs (which will involve a recursive call to this function)
     for field_name in struct.get("objects", []):
@@ -1121,12 +1121,12 @@
                 raise DataStructureException(e.message)
 
     # now check all the lists
-    for field_name, instructions in struct.get("lists", {}).iteritems():
+    for field_name, instructions in struct.get("lists", {}).items():
         vals = obj.get(field_name)
         if vals is None:
             continue
         if not isinstance(vals, list):
-            raise DataStructureException(u"Expecting list at {x} but found something else".format(x=context + field_name))
+            raise DataStructureException("Expecting list at {x} but found something else".format(x=context + field_name))
 
         # prep the keyword arguments for the setters
         kwargs = construct_kwargs("list", "set", instructions)
@@ -1138,7 +1138,7 @@
             if coerce_fn is None:
                 raise DataStructureException("No coersion function defined for type '{x}' at '{c}'".format(x=instructions.get("coerce", "unicode"), c=context + field_name))
 
-            for i in xrange(len(vals)):
+            for i in range(len(vals)):
                 val = vals[i]
                 try:
                     constructed._add_to_list(field_name, val, coerce=coerce_fn, **kwargs)
@@ -1183,7 +1183,7 @@
 def construct_merge(target, source):
     merged = deepcopy(target)
 
-    for field, instructions in source.get("fields", {}).iteritems():
+    for field, instructions in source.get("fields", {}).items():
         if "fields" not in merged:
             merged["fields"] = {}
         if field not in merged["fields"]:
@@ -1195,7 +1195,7 @@
         if obj not in merged["objects"]:
             merged["objects"].append(obj)
 
-    for field, instructions in source.get("lists", {}).iteritems():
+    for field, instructions in source.get("lists", {}).items():
         if "lists" not in merged:
             merged["lists"] = {}
         if field not in merged["lists"]:
@@ -1207,7 +1207,7 @@
         if r not in merged["required"]:
             merged["required"].append(r)
 
-    for field, struct in source.get("structs", {}).iteritems():
+    for field, struct in source.get("structs", {}).items():
         if "structs" not in merged:
             merged["structs"] = {}
         if field not in merged["structs"]:
@@ -1269,14 +1269,14 @@
 
     nk = {}
     if dir == "set":
-        for k, v in kwargs.iteritems():
+        for k, v in kwargs.items():
             # basically everything is a "set" argument unless explicitly stated to be a "get" argument
             if not k.startswith("get__"):
                 if k.startswith("set__"):    # if it starts with the set__ prefix, remove it
                     k = k[5:]
                 nk[k] = v
     elif dir == "get":
-        for k, v in kwargs.iteritems():
+        for k, v in kwargs.items():
             # must start with "get" argument
             if k.startswith("get__"):
                 nk[k[5:]] = v
@@ -1284,18 +1284,18 @@
     return nk
 
 def construct_data_keys(struct):
-    return struct.get("fields", {}).keys() + struct.get("objects", []) + struct.get("lists", {}).keys()
+    return list(struct.get("fields", {}).keys()) + struct.get("objects", []) + list(struct.get("lists", {}).keys())
 
 def merge_outside_construct(struct, target, source):
     merged = deepcopy(target)
 
-    for source_key in source.keys():
+    for source_key in list(source.keys()):
         # if the source_key is one of the struct's fields, ignore it
-        if source_key in struct.get("fields", {}).keys():
+        if source_key in list(struct.RefactoringTool: Refactored ./portality/lib/dates.py
RefactoringTool: Refactored ./portality/lib/es_data_mapping.py
RefactoringTool: No changes to ./portality/lib/es_query_http.py
RefactoringTool: No changes to ./portality/lib/isolang.py
RefactoringTool: Refactored ./portality/lib/modeldoc.py
get("fields", {}).keys()):
             continue
 
         # if the source_key is one of the struct's lists, ignore it
-        if source_key in struct.get("lists", {}).keys():
+        if source_key in list(struct.get("lists", {}).keys()):
             continue
 
         # if the source_key is one of the struct's object, we will need to go deeper
@@ -1337,16 +1337,16 @@
     :param fields_and_values:
     :return:
     """
-    for k, valtup in fields_and_values.iteritems():
+    for k, valtup in fields_and_values.items():
         if not isinstance(valtup, tuple):
             valtup = (valtup,)
         set_val = valtup[0]
         try:
             setattr(obj, k, set_val)
         except AttributeError:
-            assert False, u"Unable to set attribute {x} with value {y}".format(x=k, y=set_val)
-
-    for k, valtup in fields_and_values.iteritems():
+            assert False, "Unable to set attribute {x} with value {y}".format(x=k, y=set_val)
+
+    for k, valtup in fields_and_values.items():
         if not isinstance(valtup, tuple):
             valtup = (valtup,)
         get_val = valtup[0]
--- ./portality/lib/dates.py	(original)
+++ ./portality/lib/dates.py	(refactored)
@@ -26,7 +26,7 @@
 def format(d, format=None):
     if format is None:
         format = app.config.get("DEFAULT_DATE_FORMAT")
-    return unicode(d.strftime(format))
+    return str(d.strftime(format))
 
 
 def reformat(s, in_format=None, out_format=None):
@@ -48,11 +48,11 @@
 def random_date(fro=None, to=None):
     if fro is None:
         fro = parse("1970-01-01T00:00:00Z")
-    if isinstance(fro, basestring):
+    if isinstance(fro, str):
         fro = parse(fro)
     if to is None:
         to = datetime.utcnow()
-    if isinstance(to, basestring):
+    if isinstance(to, str):
         to = parse(to)
 
     span = int((to - fro).total_seconds())
--- ./portality/lib/es_data_mapping.py	(original)
+++ ./portality/lib/es_data_mapping.py	(refactored)
@@ -36,14 +36,14 @@
 def create_mapping(struct, mapping_opts, path=()):
     result = {"properties": {}}
 
-    for field, spec in struct.get("fields", {}).iteritems():
+    for field, spec in struct.get("fields", {}).items():
         result["properties"][field] = apply_mapping_opts(field, path, spec, mapping_opts)
 
-    for field, spec in struct.get("lists", {}).iteritems():
+    for field, spec in struct.get("lists", {}).items():
         if "coerce" in spec:
             result["properties"][field] = apply_mapping_opts(field, path, spec, mapping_opts)
 
-    for struct_name, struct_body in struct.get("structs", {}).iteritems():
+    for struct_name, struct_body in struct.get("structs", {}).items():
         result["properties"][struct_name] = create_mapping(struct_body, mapping_opts, path + (struct_name,))
 
     return result
--- ./portality/lib/modeldoc.py	(original)
+++ ./portality/lib/modeldoc.py	(refactored)
@@ -50,7 +50,7 @@
     table = "| Field | Description | Datatype | Format | Allowed Values |\n"
     table += "| ----- | ----------- | -------- | ------ | -------------- |\n"
 
-    keys = fields.keys()
+    keys = list(fields.keys())
     keys.sort()
 
     for k in keys:
@@ -69,7 +69,7 @@
         example = {}
 
         # first do all the fields at this level
-        for simple_field, instructions in struct.get('fields', {}).iteritems():
+        for simple_field, instructions in struct.get('fields', {}).items():
             example[simple_field] = type_map(instructions.get("coerce"))
             fields[path + simple_field] = (field_descriptions.get(path + simple_field, ""), datatype(instructions.get("coerce")), form(instructions.get("coerce")), values_or_range(instructions.get("allowed_values"), instructions.get("allowed_range")))
 
@@ -80,7 +80,7 @@
             example[obj] = do_document(newpath, instructions, fields)
 
         # finally do all the lists at this level
-        for l, instructions in struct.get('lists', {}).iteritems():
+        for l, instructions in struct.get('lists', {}).items():
             if instructions['contains'] == 'field':
                 eRefactoringTool: Refactored ./portality/lib/normalise.py
RefactoringTool: No changes to ./portality/lib/plugin.py
RefactoringTool: Refactored ./portality/lib/query_filters.py
RefactoringTool: No changes to ./portality/lib/report_to_csv.py
RefactoringTool: Refactored ./portality/lib/swagger.py
xample[l] = [type_map(instructions.get("coerce"))]
                 fields[path + l] = (field_descriptions.get(path + l, ""), datatype(instructions.get("coerce")), form(instructions.get("coerce")), values_or_range(instructions.get("allowed_values"), instructions.get("allowed_range")))
--- ./portality/lib/normalise.py	(original)
+++ ./portality/lib/normalise.py	(refactored)
@@ -1,4 +1,4 @@
-import urlparse
+import urllib.parse
 from portality import regex
 
 def normalise_url(url):
@@ -25,7 +25,7 @@
     if "://" not in url:
         url = "http://" + url
 
-    u = urlparse.urlparse(url)
+    u = urllib.parse.urlparse(url)
 
     if u.netloc is None or u.netloc == "":
         raise ValueError("Could not extract a normalised URL from '{x}'".format(x=url))
@@ -33,8 +33,8 @@
     if u.scheme not in schemes:
         raise ValueError("URL must be at http(s) or ftp(s), found '{x}'".format(x=u.netloc))
 
-    n = urlparse.ParseResult(None, u.netloc, u.path, u.params, u.query, u.fragment)
-    return urlparse.urlunparse(n)
+    n = urllib.parse.ParseResult(None, u.netloc, u.path, u.params, u.query, u.fragment)
+    return urllib.parse.urlunparse(n)
 
 
 def normalise_doi(doi):
--- ./portality/lib/query_filters.py	(original)
+++ ./portality/lib/query_filters.py	(refactored)
@@ -77,7 +77,7 @@
     # Dealing with single unpacked result
     if unpacked:
         if "admin" in results:
-            for k in results["admin"].keys():
+            for k in list(results["admin"].keys()):
                 if k not in ["ticked", "seal"]:
                     del results["admin"][k]
         return results
@@ -91,7 +91,7 @@
     for hit in results["hits"]["hits"]:
         if "_source" in hit:
             if "admin" in hit["_source"]:
-                for k in hit["_source"]["admin"].keys():
+                for k in list(hit["_source"]["admin"].keys()):
                     if k not in ["ticked", "seal"]:
                         del hit["_source"]["admin"][k]
 
@@ -129,7 +129,7 @@
     # Dealing with single unpacked ES result
     if unpacked:
         if "admin" in results:
-            for k in results["admin"].keys():
+            for k in list(results["admin"].keys()):
                 if k not in ["ticked", "seal", "in_doaj", "related_applications", "current_application", "current_journal", "application_status"]:
                     del results["admin"][k]
         return results
@@ -143,7 +143,7 @@
     for hit in results["hits"]["hits"]:
         if "_source" in hit:
             if "admin" in hit["_source"]:
-                for k in hit["_source"]["admin"].keys():
+                for k in list(hit["_source"]["admin"].keys()):
                     if k not in ["ticked", "seal", "in_doaj", "related_applications", "current_application", "current_journal", "application_status"]:
                         del hit["_source"]["admin"][k]
 
--- ./portality/lib/swagger.py	(original)
+++ ./portality/lib/swagger.py	(refactored)
@@ -1,5 +1,5 @@
 from copy import deepcopy
-from dataobj import DataSchemaException
+from .dataobj import DataSchemaException
 
 class SwaggerSupport(object):
     # Translation between our simple field types and swagger spec types.
@@ -67,7 +67,7 @@
         swag_properties = {}
 
         # convert simple fields
-        for simple_field, instructions in struct.get('fields', {}).iteritems():
+        for simple_field, instructions in struct.get('fields', {}).items():
             # no point adding to the path here, it's not gonna recurse any further from this field
             swag_properties[simple_field] = self._swagger_trans.get(instructions['coerce'], {"type": "string"})
 
@@ -83,7 +83,7 @@
             swag_properties[obj]['required'] = deepcopy(instructions.get('required', []))
 
         # convert lists
-        for l, instructions in struct.get('lists', {}).iteritems():
+        for l, instructions in struct.get('lists', {}).items():
             newpath = l if not path else path + '.' + l
 
             swag_properties[l] = {}
@@ -97,6 +97,6 @@
                 swag_properties[l]['items']['propertiesRefactoringTool: Refactored ./portality/migrate/1089_edit_field_by_query/regex_http_to_https.py
RefactoringTool: No changes to ./portality/migrate/1196_publisher_struct/operations.py
RefactoringTool: Refactored ./portality/migrate/1303_nonexistent_editors_assigned/nonexistent_editors_assigned.py
'] = self.__struct_to_swag_properties(struct=struct.get('structs', {}).get(l, {}), path=newpath)  # recursive call, process sub-struct(s)
                 swag_properties[l]['items']['required'] = deepcopy(struct.get('structs', {}).get(l, {}).get('required', []))
             else:
-                raise DataSchemaException(u"Instructions for list {x} unclear. Conversion to Swagger Spec only supports lists containing \"field\" and \"object\" items.".format(x=newpath))
+                raise DataSchemaException("Instructions for list {x} unclear. Conversion to Swagger Spec only supports lists containing \"field\" and \"object\" items.".format(x=newpath))
 
         return swag_properties
--- ./portality/migrate/1089_edit_field_by_query/regex_http_to_https.py	(original)
+++ ./portality/migrate/1089_edit_field_by_query/regex_http_to_https.py	(refactored)
@@ -26,19 +26,19 @@
 
         # When we have enough, do some writing
         if len(write_batch) >= batch_size:
-            print "writing ", len(write_batch)
+            print("writing ", len(write_batch))
             raw.bulk(connection, es_type, write_batch)
             write_batch = []
 
     # Write the last part-batch to index
     if len(write_batch) > 0:
-        print "writing ", len(write_batch)
+        print("writing ", len(write_batch))
         raw.bulk(connection, es_type, write_batch)
 
 
 if __name__ == "__main__":
     if app.config.get("SCRIPTS_READ_ONLY_MODE", False):
-        print "System is in READ-ONLY mode, script cannot run"
+        print("System is in READ-ONLY mode, script cannot run")
         exit(1)
 
     import argparse
@@ -50,14 +50,14 @@
     args = parser.parse_args()
 
     if not args.query:
-        print 'A query parameter is required. This can be a match_all if you really do want to edit indiscriminately.'
+        print('A query parameter is required. This can be a match_all if you really do want to edit indiscriminately.')
         exit(1)
 
     if not args.type:
-        print 'One or more type parameters are required. If supplying more than one, ensure the query is valid for both types.'
+        print('One or more type parameters are required. If supplying more than one, ensure the query is valid for both types.')
         exit(1)
 
-    print 'Starting {0}.'.format(datetime.now())
+    print('Starting {0}.'.format(datetime.now()))
 
     # Connection to the ES index
     conn = raw.Connection(host=app.config.get("ELASTIC_SEARCH_HOST"), index=app.config.get("ELASTIC_SEARCH_DB"))
@@ -65,4 +65,4 @@
     for t in args.type:
         scroll_edit(conn, t, json.loads(args.query))
 
-    print 'Finished {0}.'.format(datetime.now())
+    print('Finished {0}.'.format(datetime.now()))
--- ./portality/migrate/1303_nonexistent_editors_assigned/nonexistent_editors_assigned.py	(original)
+++ ./portality/migrate/1303_nonexistent_editors_assigned/nonexistent_editors_assigned.py	(refactored)
@@ -49,23 +49,23 @@
             journal_edited_count += 1
             write_batch.append(journal_model.data)
     except DataStructureException:
-        print "Failed to create a journal model"
-        print "no model\t{0}".format(journal_model.id)
+        print("Failed to create a journal model")
+        print("no model\t{0}".format(journal_model.id))
         journal_failed_count += 1
         fa_journal.append(journal_model.id)
 
     # When we have enough, do some writing
     if len(write_batch) >= batch_size:
-        print "writing ", len(write_batch)
+        print("writing ", len(write_batch))
         models.Journal.bulk(write_batch)
         write_batch = []
 
 # Write the last part-batch to index
 if len(write_batch) > 0:
-    print "writing ", len(write_batch)
+    print("writing ", len(write_batch))
     models.Journal.bulk(write_batch)
 
-print "{0} journals were updated, {1} were left unchanged, {2} failed.".format(journal_edited_count, journal_unchanged_count, journal_failed_count)
+print("{0} journals were updated, {1} were left unchanged, {2} failed.".format(journal_edited_count, journal_unchanged_count, journal_failed_count)RefactoringTool: Refactored ./portality/migrate/1390_lcc_regen_subjects/operations.py
RefactoringTool: Refactored ./portality/migrate/1667_remove_email_from_articles/scrub_emails.py
RefactoringTool: No changes to ./portality/migrate/20180106_1463_ongoing_updates/operations.py
RefactoringTool: Refactored ./portality/migrate/20180106_1463_ongoing_updates/sync_journals_applications.py
)
 
 write_batch = []
 suggestion_edited_count = 0
@@ -87,20 +87,20 @@
             suggestion_edited_count += 1
             write_batch.append(suggestion_model.data)
     except DataStructureException:
-        print "Failed to create a suggestion model"
-        print "no model\t{0}".format(suggestion_model.id)
+        print("Failed to create a suggestion model")
+        print("no model\t{0}".format(suggestion_model.id))
         suggestion_failed_count += 1
         fa_sug.append(suggestion_model.id)
 
     # When we have enough, do some writing
     if len(write_batch) >= batch_size:
-        print "writing ", len(write_batch)
+        print("writing ", len(write_batch))
         models.Suggestion.bulk(write_batch)
         write_batch = []
 
 # Write the last part-batch to index
 if len(write_batch) > 0:
-    print "writing ", len(write_batch)
+    print("writing ", len(write_batch))
     models.Suggestion.bulk(write_batch)
 
-print "{0} suggestions were updated, {1} were left unchanged, {2} failed.".format(suggestion_edited_count, suggestion_unchanged_count, suggestion_failed_count)
+print("{0} suggestions were updated, {1} were left unchanged, {2} failed.".format(suggestion_edited_count, suggestion_unchanged_count, suggestion_failed_count))
--- ./portality/migrate/1390_lcc_regen_subjects/operations.py	(original)
+++ ./portality/migrate/1390_lcc_regen_subjects/operations.py	(refactored)
@@ -11,16 +11,16 @@
     new_subjects = []
     for s in old_subjects:
         try:
-            sobj = {"scheme": u'LCC', "term": lcc.lookup_code(s['code']), "code": s['code']}
+            sobj = {"scheme": 'LCC', "term": lcc.lookup_code(s['code']), "code": s['code']}
             new_subjects.append(sobj)
         except KeyError:
             # Carry over the DOAJ schema subjects
             if 'scheme' in s and s['scheme'] == 'DOAJ':
                 new_subjects.append(s)
             else:
-                print "Missing code:", s
+                print("Missing code:", s)
 
     bj.set_subjects(new_subjects)
     if old_subjects != record.bibjson().subjects():
-        print '{0} Changed.\nold: {1}\nnew: {2}'.format(record['id'], old_subjects, record.bibjson().subjects())
+        print('{0} Changed.\nold: {1}\nnew: {2}'.format(record['id'], old_subjects, record.bibjson().subjects()))
     return record
--- ./portality/migrate/1667_remove_email_from_articles/scrub_emails.py	(original)
+++ ./portality/migrate/1667_remove_email_from_articles/scrub_emails.py	(refactored)
@@ -55,7 +55,7 @@
     conn = esprit.raw.make_connection(None, app.config["ELASTIC_SEARCH_HOST"], None, app.config["ELASTIC_SEARCH_DB"])
 
     # Make sure the user is super serious about doing this.
-    resp = raw_input("\nAre you sure? This is a DESTRUCTIVE OPERATION y/N: ")
+    resp = input("\nAre you sure? This is a DESTRUCTIVE OPERATION y/N: ")
     if resp.lower() == 'y':
         # Run the function to remove the field
         print("Okay, here we go...")
@@ -64,4 +64,4 @@
         print("Better safe than sorry, exiting.")
 
     end = datetime.now()
-    print(str(start) + "-" + str(end))
+    print((str(start) + "-" + str(end)))
--- ./portality/migrate/20180106_1463_ongoing_updates/sync_journals_applications.py	(original)
+++ ./portality/migrate/20180106_1463_ongoing_updates/sync_journals_applications.py	(refactored)
@@ -101,14 +101,14 @@
                     writer.writerow(row)
 
             application.save()
-            print(counter, application.id)
+            print((counter, application.id))
         else:
             row = [
                 application.id, application.created_date, application.last_updated, application.last_manual_update, "", ",".join(issns),
                 "0", "", "", "", "", "", "", "", "", "", ""
             ]
             writer.writerow(row)
-            print(counter, application.id)
+            print((counter, application.id))
 
 # let the index catch up
 time.sleep(2)
@@ -168,7 +168,7 @@
         ]
         rows.append(row)
 
-    print(counter, journal.id)
+    print((counter, journal.id))
 
 with openRefactoringTool: Refactored ./portality/migrate/972_appl_form_changes/appl_form_changes.py
("journal_2_application.csv", "wb") as f:
     writer = csv.writer(f)
--- ./portality/migrate/972_appl_form_changes/appl_form_changes.py	(original)
+++ ./portality/migrate/972_appl_form_changes/appl_form_changes.py	(refactored)
@@ -60,7 +60,7 @@
     # set it to "No"/False.
     if isinstance(
             interpret_special(model.bibjson().author_copyright.get('copyright')),
-            basestring
+            str
     ):
         model.bibjson().set_author_copyright(url='', holds_copyright='False')  # it's a string on live at the moment
         edited = True
@@ -72,7 +72,7 @@
             interpret_special(
                 model.bibjson().author_publishing_rights.get('publishing_rights')
             ),
-            basestring
+            str
     ):
         model.bibjson().set_author_publishing_rights(url='', holds_rights='False')  # it's a string on live at the moment
         edited = True
@@ -90,25 +90,25 @@
             journal_edited_count += 1
             write_batch.append(journal_model.data)
     except ValueError:
-        print "Failed to create a model"
-        print "no model\t{0}".format(journal_model.id)
+        print("Failed to create a model")
+        print("no model\t{0}".format(journal_model.id))
         journal_failed_count += 1
         fa.append(journal_model.id)
     except KeyError:
         # No license present
-        print "no license information present, unchanged\t{0}".format(journal_model.id)
+        print("no license information present, unchanged\t{0}".format(journal_model.id))
         journal_unchanged_count += 1
         un.append(journal_model.id)
 
     # When we have enough, do some writing
     if len(write_batch) >= batch_size:
-        print "writing ", len(write_batch)
+        print("writing ", len(write_batch))
         models.Journal.bulk(write_batch)
         write_batch = []
 
 # Write the last part-batch to index
 if len(write_batch) > 0:
-    print "writing ", len(write_batch)
+    print("writing ", len(write_batch))
     models.Journal.bulk(write_batch)
 
 
@@ -134,27 +134,27 @@
             suggestion_edited_count += 1
             write_batch.append(suggestion_model.data)
     except ValueError:
-        print "Failed to create a model"
-        print "no model\t{0}".format(suggestion_model.id)
+        print("Failed to create a model")
+        print("no model\t{0}".format(suggestion_model.id))
         suggestion_failed_count += 1
         fa_sug.append(suggestion_model.id)
     except KeyError:
         # No license present, pass
-        print "no license information present, unchanged\t{0}".format(suggestion_model.id)
+        print("no license information present, unchanged\t{0}".format(suggestion_model.id))
         suggestion_unchanged_count += 1
         un_sug.append(suggestion_model.id)
 
     # When we have enough, do some writing
     if len(write_batch) >= batch_size:
-        print "writing ", len(write_batch)
+        print("writing ", len(write_batch))
         models.Suggestion.bulk(write_batch)
         write_batch = []
 
 # Write the last part-batch to index
 if len(write_batch) > 0:
-    print "writing ", len(write_batch)
+    print("writing ", len(write_batch))
     models.Suggestion.bulk(write_batch)
 
-print "\nCompleted. Run scripts/journalinfo.py to update the articles with the new license data, and missed_journals.py for the missing journals."
-print "{0} journals were updated, {1} were left unchanged, and {2} failed.".format(journal_edited_count, journal_unchanged_count, journal_failed_count)
-print "{0} suggestions were updated, {1} were left unchanged, {2} failed.".format(suggestion_edited_count, suggestion_unchanged_count, suggestion_failed_count)
+print("\nCompleted. Run scripts/journalinfo.py to update the articles with the new license data, and missed_journals.py for the missing journals.")
+print("{0} journals were updated, {1} were left unchanged, and {2} failed.".format(journal_edited_count, journal_unchanged_count, journal_failed_count))
+print("{0} suggestions were updated, {1} were left unchanged, {2} failed.".format(RefactoringTool: No changes to ./portality/migrate/continuations/clean_struct.py
RefactoringTool: Refactored ./portality/migrate/continuations/extract_continuations.py
RefactoringTool: No changes to ./portality/migrate/continuations/mappings.py
RefactoringTool: Refactored ./portality/migrate/continuations/restructure_archiving_policy.py
RefactoringTool: No changes to ./portality/migrate/continuations/test_migration.py
RefactoringTool: Refactored ./portality/migrate/delete_field_from_type/scrub_field.py
RefactoringTool: Refactored ./portality/migrate/licenses/missed_journals.py
RefactoringTool: Refactored ./portality/migrate/licenses/update_licenses.py
suggestion_edited_count, suggestion_unchanged_count, suggestion_failed_count))
--- ./portality/migrate/continuations/extract_continuations.py	(original)
+++ ./portality/migrate/continuations/extract_continuations.py	(refactored)
@@ -34,7 +34,7 @@
         if created is not None:
             j.set_created(created)
 
-        j.add_note(u"Continuation automatically extracted from journal {x} during migration".format(x=data.get("id")))
+        j.add_note("Continuation automatically extracted from journal {x} during migration".format(x=data.get("id")))
         j.save()
 
     if replaces is not None:
--- ./portality/migrate/continuations/restructure_archiving_policy.py	(original)
+++ ./portality/migrate/continuations/restructure_archiving_policy.py	(refactored)
@@ -27,7 +27,7 @@
     if "policy" in ap:
         nap["known"] = []
         for p in ap["policy"]:
-            if isinstance(p, basestring):
+            if isinstance(p, str):
                 nap["known"].append(p)
             else:
                 k, v = p
--- ./portality/migrate/delete_field_from_type/scrub_field.py	(original)
+++ ./portality/migrate/delete_field_from_type/scrub_field.py	(refactored)
@@ -51,7 +51,7 @@
     conn = esprit.raw.make_connection(None, app.config["ELASTIC_SEARCH_HOST"], None, app.config["ELASTIC_SEARCH_DB"])
 
     # Make sure the user is super serious about doing this.
-    resp = raw_input("\nAre you sure? This script can be VERY DESTRUCTIVE!\n"
+    resp = input("\nAre you sure? This script can be VERY DESTRUCTIVE!\n"
                      "Confirm delete of field {0} in type {1} y/N: ".format(args.field, args.type))
     if resp.lower() == 'y':
         # Run the function to remove the field
@@ -61,4 +61,4 @@
         print("Better safe than sorry, exiting.")
 
     end = datetime.now()
-    print(str(start) + "-" + str(end))
+    print((str(start) + "-" + str(end)))
--- ./portality/migrate/licenses/missed_journals.py	(original)
+++ ./portality/migrate/licenses/missed_journals.py	(refactored)
@@ -7,7 +7,7 @@
 '''
 
 # The articles missed by the update_licenses script
-missed_articles = [u'0028-9930', u'0034-7310', u'1089-6891', u'2014-7351', u'1960-6004', u'1326-2238', u'1678-4226', u'2036-3603', u'1678-4936', u'0104-7930', u'2009-4161', u'0173-5969', u'1863-5245', u'1679-2041', u'2065-7647', u'1406-0000', u'0182-1279', u'0000-0000', u'1301-3438', u'1402-150X', u'1985-8329', u'2163-7984', u'1670-7796', u'1670-7788', u'2067-7694', u'2307-5359', u'1862-4006', u'1985-8353', u'1614-2934', u'0158-1328', u'2163-3987', u'2250-5490', u'1176-4120', u'2008-4073', u'0232-0475']
+missed_articles = ['0028-9930', '0034-7310', '1089-6891', '2014-7351', '1960-6004', '1326-2238', '1678-4226', '2036-3603', '1678-4936', '0104-7930', '2009-4161', '0173-5969', '1863-5245', '1679-2041', '2065-7647', '1406-0000', '0182-1279', '0000-0000', '1301-3438', '1402-150X', '1985-8329', '2163-7984', '1670-7796', '1670-7788', '2067-7694', '2307-5359', '1862-4006', '1985-8353', '1614-2934', '0158-1328', '2163-3987', '2250-5490', '1176-4120', '2008-4073', '0232-0475']
 
 license_correct_dict = { "CC by" : "CC BY",
                          "CC by-nc" : "CC BY-NC",
@@ -47,17 +47,17 @@
             article_model.data.get('index')['license'] = [license_correct_dict[a_license[0]]]
             write_batch.append(article_model.data)
         except ValueError:
-            print "Failed to create a model"
+            print("Failed to create a model")
         except KeyError:
-            print "No license to change"
+            print("No license to change")
 
     # When we have enough, do some writing
     if len(write_batch) >= batch_size:
-        print "writing ", len(write_batch)
+        print("writing ", len(write_batch))
         models.Article.bulk(write_batch)
         write_batch = []
 
 # Write all remaining files to index
 if len(write_batch) > 0:
-    print "writing ", len(write_batch)
+    print("writing ", len(write_batch))
     models.Article.bulk(write_batch)
--- ./portality/migrate/licenses/update_licenses.py	(original)
+++ ./portality/migrate/licenses/update_licenses.py	(refactored)
@@ -42,35 +42,35 @@
         if j_license:
             j_license['type'] = license_correct_dict[j_license['type']]
             j_license['title'] = license_correct_dict[j_license['title']]
-            print "edited\t{0}".format(journal_model.id)
+            print("edited\t{0}".format(journal_model.id))
             ed.append(journal_model.id)
             edited += 1
             journal_model.prep()
             write_batch.append(journal_model.data)
         else:
             nolicence += 1
-            print "no licence\t{0}".format(journal_model.id)
+            print("no licence\t{0}".format(journal_model.id))
             nl.append(journal_model.id)
     except ValueError:
-        print "Failed to create a model"
-        print "no model\t{0}".format(journal_model.id)
+        print("Failed to create a model")
+        print("no model\t{0}".format(journal_model.id))
         failed += 1
         fa.append(journal_model.id)
     except KeyError:
         # No license present
-        print "unchanged\t{0}".format(journal_model.id)
+        print("unchanged\t{0}".format(journal_model.id))
         unchanged += 1
         un.append(journal_model.id)
 
     # When we have enough, do some writing
     if len(write_batch) >= batch_size:
-        print "writing ", len(write_batch)
+        print("writing ", len(write_batch))
         models.Journal.bulk(write_batch)
         write_batch = []
 
 # Write the last part-batch to index
 if len(write_batch) > 0:
-    print "writing ", len(write_batch)
+    print("writing ", len(write_batch))
     models.Journal.bulk(write_batch)
 
 
@@ -96,38 +96,38 @@
         if s_license:
             s_license['type'] = license_correct_dict[s_license['type']]
             s_license['title'] = license_correct_dict[s_license['title']]
-            print "edited\t{0}".format(suggestion_model.id)
+            print("edited\t{0}".format(suggestion_model.id))
             ed_sug.append(suggestion_model.id)
             edited_sug += 1
             suggestion_model.prep()
             write_batch.append(suggestion_model.data)
         else:
             nolicence_sug += 1
-            print "no licence\t{0}".format(suggestion_model.id)
+            print("no licence\t{0}".format(suggestion_model.id))
             nl_sug.append(suggestion_model.id)
     except ValueError:
-        print "Failed to create a model"
-        print "no model\t{0}".format(suggestion_model.id)
+        print("Failed to create a model")
+        print("no model\t{0}".format(suggestion_model.id))
         failed_sug += 1
         fa_sug.append(suggestion_model.id)
     except KeyError:
         # No license present, pass
-        print "unchanged\t{0}".format(suggestion_model.id)
+        print("unchanged\t{0}".format(suggestion_model.id))
         unchanged_sug += 1
         un_sug.append(suggestion_model.id)
         #pass
 
     # When we have enough, do some writing
     if len(write_batch) >= batch_size:
-        print "writing ", len(write_batch)
+        print("writing ", len(write_batch))
         models.Suggestion.bulk(write_batch)
         write_batch = []
 
 # Write the last part-batch to index
 if len(write_batch) > 0:
-    print "writing ", len(write_batch)
+    print("writing ", len(write_batch))
     models.Suggestion.bulk(write_batch)
 
-print "\nCompleted. Run scripts/journalinfo.py to update the articles with the new license labels, and missed_journals.py for the missing journals."
-print "{0} journals were updated, {1} were left unchanged, {2} had no licence object, and {3} failed.".format(edited, unchanged, nolicence, failed)
-print "{0} suggestions were updated, {1} were left unchanged, {2} had no licence object, and {3} failed.".format(edited_sug, unchanged_sug, nolicence_sug, failed_sug)
+print("\nCompleted. Run scripts/journalinfo.py to update the articles with the new license labels, and missed_journals.py for the missing journals.")
+print("{0} journals were updated, {1} were left unchanged, {2} had no licence object, and {3} failed.".format(edited, uRefactoringTool: Refactored ./portality/migrate/p1p2/country_cleanup.py
RefactoringTool: Refactored ./portality/migrate/p1p2/emails.py
RefactoringTool: No changes to ./portality/migrate/p1p2/flushuploads.py
RefactoringTool: Refactored ./portality/migrate/p1p2/journalowners.py
nchanged, nolicence, failed))
+print("{0} suggestions were updated, {1} were left unchanged, {2} had no licence object, and {3} failed.".format(edited_sug, unchanged_sug, nolicence_sug, failed_sug))
--- ./portality/migrate/p1p2/country_cleanup.py	(original)
+++ ./portality/migrate/p1p2/country_cleanup.py	(refactored)
@@ -17,7 +17,7 @@
         elif argv[1] == '--dry-run':
             migrate(test=True)
         else:
-            print 'I only understand -t (to test the migration you\'ve run already) or --dry-run (to write out a CSV of what would happen but not change the index) as arguments, .'
+            print('I only understand -t (to test the migration you\'ve run already) or --dry-run (to write out a CSV of what would happen but not change the index) as arguments, .')
     else:
         migrate()
 
@@ -29,15 +29,15 @@
         for row in reader:
             data.append(row)
 
-    print 'Problems:'
+    print('Problems:')
     problems = False
     for row in data:
         if row[0] != row[1]:
             problems = True
-            print row[0], ',', row[1]
+            print(row[0], ',', row[1])
 
     if not problems:
-        print 'No problems'
+        print('No problems')
 
 
 def migrate(test=False):
@@ -65,10 +65,10 @@
     
     end = datetime.now()
     
-    print "Updated Journals", counter
-    print start, end
-    print 'Time taken:', end-start
-    print 'You can pass -t to test the migration you just ran.'
+    print("Updated Journals", counter)
+    print(start, end)
+    print('Time taken:', end-start)
+    print('You can pass -t to test the migration you just ran.')
 
 
 if __name__ == '__main__':
--- ./portality/migrate/p1p2/emails.py	(original)
+++ ./portality/migrate/p1p2/emails.py	(refactored)
@@ -40,7 +40,7 @@
 multi_writer = csv.writer(fmulti)
 multi_writer.writerow(["Account ID", "Email", "Name", "Publisher"])
 
-print "processing accounts"
+print("processing accounts")
 for a in Account.iterall():
     id = a.id
     name = a.name
@@ -58,13 +58,13 @@
         continue
 
     if name is not None:
-        name = unicode(name).encode("utf8", "replace")
+        name = str(name).encode("utf8", "replace")
     if name is None or name == "":
         name = "no name available"
-    publisher = unicode(publisher).encode("utf8", "replace")
+    publisher = str(publisher).encode("utf8", "replace")
     if email is not None and email != "":
         try:
-            email = unicode(email, "utf8")
+            email = str(email, "utf8")
         except: pass
         emails = [e.strip() for e in email.encode("utf8", "replace").split(",") if e is not None and e != ""]
         for e in emails:
@@ -128,7 +128,7 @@
 app_writer = csv.writer(fapp)
 app_writer.writerow(["Email", "Name", "Publisher", "Number of Applications"])
 
-print "processing suggestions"
+print("processing suggestions")
 for email, count in get_suggesters():
     suggs = get_suggestions(email)
     
@@ -149,14 +149,14 @@
     publisher = ", ".join(publishers)
     
     if name is not None:
-        name = unicode(name).encode("utf8", "replace")
+        name = str(name).encode("utf8", "replace")
     if name is None or name == "":
         name = "no name available"
     
-    publisher = unicode(publisher).encode("utf8", "replace")
+    publisher = str(publisher).encode("utf8", "replace")
     
     try:
-        email = unicode(email, "utf8")
+        email = str(email, "utf8")
     except: pass
     emails = [e.strip() for e in email.encode("utf8", "replace").split(",") if e is not None and e != ""]
     for e in emails:
--- ./portality/migrate/p1p2/journalowners.py	(original)
+++ ./portality/migrate/p1p2/journalowners.py	(refactored)
@@ -1,6 +1,6 @@
 from portality import models
 
-print "Migrating journal owners - adding account ids to journal records"
+print("Migrating journal owners - adding account ids to journal records")
 
 batch = []
 batch_size = 1000
@@ -14,10 +14,10 @@
                 batch.append(record.data)
     
     if len(batch) >= batch_size:
-        print "writing ", len(batch)
+        print("writRefactoringTool: Refactored ./portality/migrate/p1p2/journalrestructure.py
RefactoringTool: Refactored ./portality/migrate/p1p2/journals_and_suggestions_text_tweaks.py
RefactoringTool: Refactored ./portality/migrate/p1p2/loadlcc.py
RefactoringTool: Refactored ./portality/migrate/p1p2/publisheremails.py
RefactoringTool: Refactored ./portality/migrate/p1p2/remove-0000-issns.py
ing ", len(batch))
         models.Journal.bulk(batch)
         batch = []
 
 if len(batch) > 0:
-    print "writing ", len(batch)
+    print("writing ", len(batch))
     models.Journal.bulk(batch)
--- ./portality/migrate/p1p2/journalrestructure.py	(original)
+++ ./portality/migrate/p1p2/journalrestructure.py	(refactored)
@@ -27,11 +27,11 @@
     
     if len(batch) >= batch_size:
         total += len(batch)
-        print "writing", len(batch), "; total so far", total
+        print("writing", len(batch), "; total so far", total)
         models.Journal.bulk(batch)
         batch = []
 
 if len(batch) > 0:
     total += len(batch)
-    print "writing", len(batch), "; total so far", total
+    print("writing", len(batch), "; total so far", total)
     models.Journal.bulk(batch)
--- ./portality/migrate/p1p2/journals_and_suggestions_text_tweaks.py	(original)
+++ ./portality/migrate/p1p2/journals_and_suggestions_text_tweaks.py	(refactored)
@@ -13,7 +13,7 @@
     }
 
     changed = False
-    for old, new in update_deposit_policies.iteritems():
+    for old, new in update_deposit_policies.items():
         try:
             replace_index = s.bibjson().deposit_policy.index(old)
             s.bibjson().deposit_policy[replace_index] = new
@@ -27,13 +27,13 @@
     
     if len(batch) >= batch_size:
         total += len(batch)
-        print "writing suggestions", len(batch), "; total so far", total
+        print("writing suggestions", len(batch), "; total so far", total)
         models.Suggestion.bulk(batch)
         batch = []
 
 if len(batch) > 0:
     total += len(batch)
-    print "writing suggestions", len(batch), "; total so far", total
+    print("writing suggestions", len(batch), "; total so far", total)
     models.Suggestion.bulk(batch)
 
 
@@ -48,7 +48,7 @@
     }
 
     changed = False
-    for old, new in update_deposit_policies.iteritems():
+    for old, new in update_deposit_policies.items():
         try:
             replace_index = j.bibjson().deposit_policy.index(old)
             j.bibjson().deposit_policy[replace_index] = new
@@ -62,11 +62,11 @@
     
     if len(batch) >= batch_size:
         total += len(batch)
-        print "writing journals", len(batch), "; total so far", total
+        print("writing journals", len(batch), "; total so far", total)
         models.Journal.bulk(batch)
         batch = []
 
 if len(batch) > 0:
     total += len(batch)
-    print "writing journals", len(batch), "; total so far", total
+    print("writing journals", len(batch), "; total so far", total)
     models.Journal.bulk(batch)
--- ./portality/migrate/p1p2/loadlcc.py	(original)
+++ ./portality/migrate/p1p2/loadlcc.py	(refactored)
@@ -38,7 +38,7 @@
     else:
         cpmap[name] = parent
 
-for child, parent in cpmap.iteritems():
+for child, parent in cpmap.items():
     cn = nodes.get(child)
     pn = nodes.get(parent)
     if cn is None or pn is None:
--- ./portality/migrate/p1p2/publisheremails.py	(original)
+++ ./portality/migrate/p1p2/publisheremails.py	(refactored)
@@ -1,7 +1,7 @@
 import os, csv
 from portality import models, settings
 from datetime import datetime
-from StringIO import StringIO
+from io import StringIO
 
 corrections_csv = os.path.join(os.path.dirname(os.path.realpath(__file__)), "corrections.csv")
 malformed_csv = os.path.join(os.path.dirname(os.path.realpath(__file__)), "malformed.csv")
@@ -43,7 +43,7 @@
         publisher = corrections[id]
     acc = models.Account.pull(publisher)
     if acc is None:
-        print publisher, "fail - shouldn't happen"
+        print(publisher, "fail - shouldn't happen")
         continue
     
     new_row = row[:4] + [acc.email]
@@ -64,7 +64,7 @@
         publisher = corrections[id]
     acc = models.Account.pull(publisher)
     if acc is None:
-        print publisher, "fail - shouldn't happen"
+        print(publisher, "fail - shouldn't happen")
         continue
     
     new_row = row[:4] + [acc.email]
--- ./portality/migrate/p1p2/remove-0000-issns.py	(original)
+++ ./portality/migrate/p1p2/remove-0000-issns.py	(refactored)
@@ -23,7 +23,7 @@
   RefactoringTool: Refactored ./portality/migrate/p1p2/suggestionrestructure.py
RefactoringTool: No changes to ./portality/migrate/p1p2/uploadcorrections.py
RefactoringTool: No changes to ./portality/migrate/p1p2/uploadedfilenames.py
RefactoringTool: Refactored ./portality/migrate/p1p2/uploadedxml.py
  
     delete_0000 = True
     if len(issns) == 1 and issns[0] == '0000-0000':
-        print 'suggestion {0} has only 1 id, 0000-0000, not deleting it'.format(s.id)
+        print('suggestion {0} has only 1 id, 0000-0000, not deleting it'.format(s.id))
         delete_0000 = False
     
     if delete_0000:
@@ -33,13 +33,13 @@
     
     if len(batch) >= batch_size:
         total += len(batch)
-        print "writing", len(batch), "; total so far", total
+        print("writing", len(batch), "; total so far", total)
         models.Suggestion.bulk(batch)
         batch = []
 
 if len(batch) > 0:
     total += len(batch)
-    print "writing", len(batch), "; total so far", total
+    print("writing", len(batch), "; total so far", total)
     models.Suggestion.bulk(batch)
 
 
@@ -52,7 +52,7 @@
     
     delete_0000 = True
     if len(issns) == 1 and issns[0] == '0000-0000':
-        print 'journal {0} has only 1 id, 0000-0000, not deleting it'.format(j.id)
+        print('journal {0} has only 1 id, 0000-0000, not deleting it'.format(j.id))
         delete_0000 = False
     
     if delete_0000:
@@ -62,11 +62,11 @@
     
     if len(batch) >= batch_size:
         total += len(batch)
-        print "writing", len(batch), "; total so far", total
+        print("writing", len(batch), "; total so far", total)
         models.Journal.bulk(batch)
         batch = []
 
 if len(batch) > 0:
     total += len(batch)
-    print "writing", len(batch), "; total so far", total
+    print("writing", len(batch), "; total so far", total)
     models.Journal.bulk(batch)
--- ./portality/migrate/p1p2/suggestionrestructure.py	(original)
+++ ./portality/migrate/p1p2/suggestionrestructure.py	(refactored)
@@ -23,7 +23,7 @@
     id = d.get("id")
     if id is not None:
         models.Suggestion.remove_by_id(id)
-        print "removing", id
+        print("removing", id)
 
 models.Suggestion.refresh()
 time.sleep(10)
@@ -62,11 +62,11 @@
     
     if len(batch) >= batch_size:
         total += len(batch)
-        print "writing", len(batch), "; total so far", total
+        print("writing", len(batch), "; total so far", total)
         models.Suggestion.bulk(batch)
         batch = []
 
 if len(batch) > 0:
     total += len(batch)
-    print "writing", len(batch), "; total so far", total
+    print("writing", len(batch), "; total so far", total)
     models.Suggestion.bulk(batch)
--- ./portality/migrate/p1p2/uploadedxml.py	(original)
+++ ./portality/migrate/p1p2/uploadedxml.py	(refactored)
@@ -63,7 +63,7 @@
 articles_updated = 0
 articles_new = 0
 
-print "importing", total, "files from", xml_dir
+print("importing", total, "files from", xml_dir)
 
 def article_save_closure(upload_id):
     def article_callback(article):
@@ -71,7 +71,7 @@
         articles_in += 1
         article.set_upload_id(upload_id)
         article.save()
-        print "saved article", article.id
+        print("saved article", article.id)
     return article_callback
 
 """
@@ -92,7 +92,7 @@
         eissn = b.get_identifiers(b.E_ISSN)
         if title is not None:
             title = title.encode("ascii", errors="ignore")
-        print "illegitimate owner", title
+        print("illegitimate owner", title)
         failed_articles_writer.writerow([id, publisher, filename, uploaded, ", ".join(pissn), ", ".join(eissn), title])
     return fail_callback
 
@@ -113,19 +113,19 @@
     # at this point we apply a correction in the event that we have a 
     # correction for this id
     if id in corrections:
-        print t, "correcting publisher", publisher, "-", corrections[id]
+        print(t, "correcting publisher", publisher, "-", corrections[id])
         publisher = corrections[id]
     
     acc = models.Account.pull(publisher)
     if acc is None:
-        print t, "No such publisher -", publisher
+        print(t, "No such publisher -", publisher)
         orphaned += 1
         orphan_writer.writerow([id, publisher, filename, uploaded])
         continue
     
     if publisher in imports:
         if filename in imports[publisher]:
-            preup = importsRefactoringTool: Refactored ./portality/migrate/p1p2/userroles.py
RefactoringTool: Refactored ./portality/migrate/p2oe/apcdata.py
RefactoringTool: Refactored ./portality/migrate/p2oe/uncontinue.py
RefactoringTool: Refactored ./portality/migrate/st2cl/cluster.py
[publisher][filename].keys()[0]
+            preup = list(imports[publisher][filename].keys())[0]
             duplicate += 1
             if lm > preup:
                 rid = imports[publisher][filename][preup]
@@ -153,9 +153,9 @@
 # files were uploaded
 lastmods = []
 lookup = {}
-for publisher, files in imports.iteritems():
-    for filename, details in files.iteritems():
-        lm = details.keys()[0]
+for publisher, files in imports.items():
+    for filename, details in files.items():
+        lm = list(details.keys())[0]
         id = details[lm]
         
         lastmods.append(lm)
@@ -190,7 +190,7 @@
             doc = etree.parse(open(xml_file))
         except:
             failed += 1
-            print f, "Malformed XML"
+            print(f, "Malformed XML")
             malformed_writer.writerow([f, publisher, filename, uploaded, acc.email])
             upload.failed("Unable to parse file")
             upload.save()
@@ -198,7 +198,7 @@
         
         # now try and validate the file
         validates = xwalk.validate(doc)
-        print f, ("Valid" if validates else "Invalid")
+        print(f, ("Valid" if validates else "Invalid"))
         
         if validates: 
             valid += 1
@@ -233,7 +233,7 @@
 
 end = datetime.now()
 
-print "Total", total, "attempted", attempted, "valid", valid, "invalid", invalid, "failed", failed, "duplicate", duplicate, "orphaned", orphaned
-print "Created Articles", articles_in, "Failed Articles", articles_failed
-print "New Articles", articles_new, "Updated Articles", articles_updated
-print start, end
+print("Total", total, "attempted", attempted, "valid", valid, "invalid", invalid, "failed", failed, "duplicate", duplicate, "orphaned", orphaned)
+print("Created Articles", articles_in, "Failed Articles", articles_failed)
+print("New Articles", articles_new, "Updated Articles", articles_updated)
+print(start, end)
--- ./portality/migrate/p1p2/userroles.py	(original)
+++ ./portality/migrate/p1p2/userroles.py	(refactored)
@@ -1,6 +1,6 @@
 from portality import models
 
-print "Migrating user accounts - adding publisher role"
+print("Migrating user accounts - adding publisher role")
 
 batch = []
 batch_size = 1000
@@ -15,10 +15,10 @@
     if trip:
         batch.append(acc.data)
     if len(batch) >= batch_size:
-        print "writing ", len(batch)
+        print("writing ", len(batch))
         models.Account.bulk(batch)
         batch = []
 
 if len(batch) > 0:
-    print "writing ", len(batch)
+    print("writing ", len(batch))
     models.Account.bulk(batch)
--- ./portality/migrate/p2oe/apcdata.py	(original)
+++ ./portality/migrate/p2oe/apcdata.py	(refactored)
@@ -26,11 +26,11 @@
 
     if len(batch) >= batch_size:
         total += len(batch)
-        print "writing", len(batch), "; total so far", total
+        print("writing", len(batch), "; total so far", total)
         models.Journal.bulk(batch)
         batch = []
 
 if len(batch) > 0:
     total += len(batch)
-    print "writing", len(batch), "; total so far", total
+    print("writing", len(batch), "; total so far", total)
     models.Journal.bulk(batch)
--- ./portality/migrate/p2oe/uncontinue.py	(original)
+++ ./portality/migrate/p2oe/uncontinue.py	(refactored)
@@ -22,10 +22,10 @@
 new.add_contact(name, email)
 new.set_owner(owner)
 new.save()
-print "Created new record with id", new.id
+print("Created new record with id", new.id)
 
 # remove the erroneous journal from the history, add a note, and re-save the original journal
 para.remove_history(issn)
 para.add_note("CL removed journal " + issn + " from continuations history; was there erroneously")
 para.save()
-print "Removed erroneous record from", id
+print("Removed erroneous record from", id)
--- ./portality/migrate/st2cl/cluster.py	(original)
+++ ./portality/migrate/st2cl/cluster.py	(refactored)
@@ -70,21 +70,21 @@
 
 issn_clusters = []
 issn_success_register = {"prevfound" : [], "prevfail" : [], "nextfound" : [], "nextfail" : []}
-for issn, rels in issn_cluster.iteritems():
+for issn, rels in issn_cluster.items():
     register = []
    RefactoringTool: Refactored ./portality/migrate/st2cl/cluster2.py
 do_issn_cluster(issn, rels, register, issn_success_register, issn_cluster)
     issn_clusters.append(register)
 
 eissn_clusters = []
 eissn_success_register = {"prevfound" : [], "prevfail" : [], "nextfound" : [], "nextfail" : []}
-for issn, rels in eissn_cluster.iteritems():
+for issn, rels in eissn_cluster.items():
     register = []
     do_issn_cluster(issn, rels, register, eissn_success_register, eissn_cluster)
     eissn_clusters.append(register)
 
 all_clusters = []
 all_success_register = {"prevfound" : [], "prevfail" : [], "nextfound" : [], "nextfail" : []}
-for issn, rels in all_cluster.iteritems():
+for issn, rels in all_cluster.items():
     register = []
     do_issn_cluster(issn, rels, register, all_success_register, all_cluster)
     all_clusters.append(register)
--- ./portality/migrate/st2cl/cluster2.py	(original)
+++ ./portality/migrate/st2cl/cluster2.py	(refactored)
@@ -54,27 +54,27 @@
     
     id += 1
 
-print len(journals), "journal records; ", len(idtable.keys()), "join identifiers; ", len(reltable.keys()), "unique issns"
+print(len(journals), "journal records; ", len(list(idtable.keys())), "join identifiers; ", len(list(reltable.keys())), "unique issns")
 
 count_register = {}
-for id, issns in idtable.iteritems():
+for id, issns in idtable.items():
     size = len(issns)
     if size in count_register:
         count_register[size] += 1
     else:
         count_register[size] = 1
 
-print "journal record to issn count statistics: ", count_register
+print("journal record to issn count statistics: ", count_register)
 
 mapregister = {}
-for issn, ids in reltable.iteritems():
+for issn, ids in reltable.items():
     size = len(ids)
     if size in mapregister:
         mapregister[size] += 1
     else:
         mapregister[size] = 1
 
-print "issn to journal record count statistics: ", mapregister
+print("issn to journal record count statistics: ", mapregister)
 
 def process(id, register):
     if id in register:
@@ -93,7 +93,7 @@
 equiv_table = {}
 processed = []
 i = 0
-for id in idtable.keys():
+for id in list(idtable.keys()):
     if id in processed:
         continue
     
@@ -103,7 +103,7 @@
     equiv_table[i] = deepcopy(register)
     i += 1
 
-print len(processed), "join ids considered"
+print(len(processed), "join ids considered")
 
 process_register = {}
 for p in processed:
@@ -111,26 +111,26 @@
         process_register[p] += 1
     else:
         process_register[p] = 1
-multiples = [(k, v) for k, v in process_register.items() if v > 1]
-print "join ids considered more than once:", multiples
+multiples = [(k, v) for k, v in list(process_register.items()) if v > 1]
+print("join ids considered more than once:", multiples)
 
 if len(multiples) > 0:
-    print "issns associated with join ids considered more than once:"
+    print("issns associated with join ids considered more than once:")
 for k, v in multiples:
     issns = idtable.get(k)
-    print k, "->", issns
+    print(k, "->", issns)
     for issn in issns:
-        print "    ", issn, "->", reltable.get(issn, [])
+        print("    ", issn, "->", reltable.get(issn, []))
         for rel in reltable.get(issn, []):
-            print "        ", rel, "->", idtable.get(rel)
-
-print len(equiv_table.keys()), "equivalences identified"
+            print("        ", rel, "->", idtable.get(rel))
+
+print(len(list(equiv_table.keys())), "equivalences identified")
 
 equivregister = {}
 idregister = {}
 multiequiv = {}
 counter = 0
-for i, ids in equiv_table.iteritems():
+for i, ids in equiv_table.items():
     # count the size of the equivalences
     size = len(ids)
     if size in equivregister:
@@ -151,19 +151,19 @@
     
     counter += size
     
-multiids = [(k, v) for k, v in idregister.items() if v > 1] 
-
-print "equivalence register statistics: ", equivregister
-print "join ids which appear in more than one equivalence", multiids
-print counter, "total issns in equivalence table"
-
-for k, v in multiequiv.iteritems():
-    print k, "->", v
+multiids = [(k, v) for k, v in list(idregister.items()) if v > 1] 
+
+print("equivaleRefactoringTool: No changes to ./portality/migrate/st2cl/emails.py
RefactoringTool: Refactored ./portality/migrate/st2cl/migrate.py
nce register statistics: ", equivregister)
+print("join ids which appear in more than one equivalence", multiids)
+print(counter, "total issns in equivalence table")
+
+for k, v in multiequiv.items():
+    print(k, "->", v)
     for jid in v:
-        print "    ", jid, "->", idtable.get(jid)
+        print("    ", jid, "->", idtable.get(jid))
 
 ordertables = {}
-for e, jids in multiequiv.iteritems():
+for e, jids in multiequiv.items():
     ordertable = {}
     for jid in jids:
         ordertable[jid] = {"n" : [], "p": []}
@@ -196,11 +196,11 @@
 """
 
 sorttable = {}
-for e, ot in ordertables.iteritems():
+for e, ot in ordertables.items():
     first = []
     last = []
     middle = []
-    for k, r in ot.iteritems():
+    for k, r in ot.items():
         if len(r.get("n")) == 0:
             first.append(k)
         elif len(r.get("p")) == 0:
@@ -210,7 +210,7 @@
     sorttable[e] = first + middle + last
 
 canontable = {}
-for e, sort in sorttable.iteritems():
+for e, sort in sorttable.items():
     canon = None
     i = 0
     found = False
@@ -229,11 +229,11 @@
     del rest[i]
     canontable[e] = (canon, rest)
 
-print "canonicalised, ordered equivalences and the relations they are derived from"
-for k in ordertables.keys():
-    print k, "->", ordertables.get(k)
-    print "    ->", sorttable.get(k)
-    print "    ->", canontable.get(k)
+print("canonicalised, ordered equivalences and the relations they are derived from")
+for k in list(ordertables.keys()):
+    print(k, "->", ordertables.get(k))
+    print("    ->", sorttable.get(k))
+    print("    ->", canontable.get(k))
 
 
 def get_issn_cell(jid):
@@ -254,7 +254,7 @@
 f = open(OUT, "wb")    
 writer = csv.writer(f)
 writer.writerow(["Equivalence Number", "Proposed Current Title", "Proposed Current ISSNs", "Proposed History: Title/ISSNs"])
-for e, data in canontable.iteritems():
+for e, data in canontable.items():
     canon, rest = data
     cells = [e]
     canon_issn_cell = get_issn_cell(canon)
--- ./portality/migrate/st2cl/migrate.py	(original)
+++ ./portality/migrate/st2cl/migrate.py	(refactored)
@@ -83,7 +83,7 @@
     xml = etree.parse(f)
     f.close()
     journals = xml.getroot()
-    print "migrating", str(len(journals)), "journal records"
+    print("migrating", str(len(journals)), "journal records")
     
     clusters = _get_journal_clusters(journals)
     
@@ -193,14 +193,14 @@
                 reltable[issn] = [id]
         id += 1
     
-    print len(journals), "journal records; ", len(idtable.keys()), "join identifiers; ", len(reltable.keys()), "unique issns"
+    print(len(journals), "journal records; ", len(list(idtable.keys())), "join identifiers; ", len(list(reltable.keys())), "unique issns")
     
     # now calculate the equivalence table.  This groups all of the journals
     # which share issns of any kind into a single batch
     equiv_table = {}
     processed = []
     i = 0
-    for id in idtable.keys():
+    for id in list(idtable.keys()):
         if id in processed:
             continue
         
@@ -213,7 +213,7 @@
     # Next go through each equivalence, and build a table of the next/previous
     # links in each of the journals
     ordertables = {}
-    for e, jids in equiv_table.iteritems():
+    for e, jids in equiv_table.items():
         ordertable = {}
         for jid in jids:
             ordertable[jid] = {"n" : [], "p": []}
@@ -242,11 +242,11 @@
     # Now analyse the previous/next status of each cluster, and organise
     # them in an array in descending order (head of the chain first)
     sorttable = {}
-    for e, ot in ordertables.iteritems():
+    for e, ot in ordertables.items():
         first = []
         last = []
         middle = []
-        for k, r in ot.iteritems():
+        for k, r in ot.items():
             if len(r.get("n")) == 0:
                 first.append(k)
             elif len(r.get("p")) == 0:
@@ -258,7 +258,7 @@
     # finally (for the clustering algorithm), select the canonical record
     # and the older historical records
     canontable = {}
-    for e, sort in sorttable.iteritems():
+    for e, sort in sorttable.items():
         canon = None
         i = 0
         found = False
@@ -280,7 +280,7 @@
     # now, in preparation for returning to the caller, substitute everything in the canon table
     # for the xml elements they represent
     clusters = []
-    for e, data in canontable.iteritems():
+    for e, data in canontable.items():
         canon, rest = data
         celement = journaltable.get(canon)
         relements = [journaltable.get(r) for r in rest]
@@ -439,7 +439,7 @@
     xml = etree.parse(f)
     f.close()
     suggestions = xml.getroot()
-    print "migrating", str(len(suggestions)), "suggestion records"
+    print("migrating", str(len(suggestions)), "suggestion records")
 
     for element in suggestions:
         s = Suggestion()
@@ -531,7 +531,7 @@
     xml = etree.parse(f)
     f.close()
     articles = xml.getroot()
-    print "migrating", str(len(articles)), "article records from", source
+    print("migrating", str(len(articles)), "article records from", source)
     
     counter = 0
     omissions = 0
@@ -543,7 +543,7 @@
         hasjournal = _add_journal_info(a)
         
         if not hasjournal:
-            print "INFO: omitting article"
+            print("INFO: omitting article")
             omissions += 1
             continue
         
@@ -554,18 +554,18 @@
         
         if len(batch) >= batch_size:
             counter += len(batch)
-            print "Writing batch, size", len(batch)
+            print("Writing batch, size", len(batch))
             Article.bulk(batch, refresh=True)
-            print "batch written, total so far", counter
+            print("batch written, total so far", counter)
             del batch[:]
     
     if len(batch) > 0:
         counter += len(batch)
-        print "Writing final batch, size", len(batch)
+        print("Writing final batch, size", len(batch))
         Article.bulk(batch, refresh=True)
-        print "batch written, total written", counter
-    
-    print "wrote", counter, "articles, omitted", omissions
+        print("batch written, total written", counter)
+    
+    print("wrote", counter, "articles, omitted", omissions)
 
 def _created_date(element):
     cd = element.find("addedOn")
@@ -577,7 +577,7 @@
             return fudge
         except:
             # do nothing, we'll just fall back to "created now"
-            print "failed on", cd.text
+            print("failed on", cd.text)
             pass
         
     return datetime.now().strftime("%Y-%m-%dT%H:%M:%SZ")
@@ -673,7 +673,7 @@
                 journal = issnmap.get(issn)
     
     if journal is None:
-        print "WARN: no journal for ", pissns, eissns
+        print("WARN: no journal for ", pissns, eissns)
         return False
     
     # if we get to here, we have a journal record we want to pull data from
@@ -708,7 +708,7 @@
     xml = etree.parse(f)
     f.close()
     contacts = xml.getroot()
-    print "migrating", str(len(contacts)), "contact records from", source
+    print("migrating", str(len(contacts)), "contact records from", source)
     
     # first thing to do is locate all the duplicates in the logins
     record = []
@@ -730,7 +730,7 @@
         issns = element.findall("issn")
         
         if login is None or login.text is None or login.text == "":
-            print "ERROR: contact without login - providing login"
+            print("ERROR: contact without login - providing login")
             if len(issns) == 0:
                 # make a random 8 character login name
                 login = ''.join(random.choice(string.ascii_uppercase + string.digits) for x in range(8))
@@ -741,7 +741,7 @@
             login = login.text
         
         if password is None or password.text is None or password.text == "":
-            print "ERROR: contact without password", login, "- providing one"
+            print("ERROR: contact without password", login, "- providing one")
             # make a random 8 character password
             password = ''.join(random.choice(string.ascii_uppercase + string.digitsRefactoringTool: Refactored ./portality/migrate/st2cl/pages.py
RefactoringTool: Refactored ./portality/migrate/subjects/remove_duplicate_subjects.py
) for x in range(8))
         else:
@@ -750,10 +750,10 @@
         # check to see if this is a duplicate
         if login in duplicates:
             if len(issns) == 0:
-                print "INFO: duplicate detected, has no ISSNs, so skipping", login
+                print("INFO: duplicate detected, has no ISSNs, so skipping", login)
                 continue
             else:
-                print "INFO: duplicate detected, with ISSNs, so keeping", login
+                print("INFO: duplicate detected, with ISSNs, so keeping", login)
                 
         a = Account()
         a.set_id(login)
@@ -785,9 +785,9 @@
     issn = _normalise_issn(issn)
     journals = Journal.find_by_issn(issn)
     if len(journals) > 1:
-        print "WARN: issn", issn, "maps to multiple journals:", ", ".join([j.id for j in journals])
+        print("WARN: issn", issn, "maps to multiple journals:", ", ".join([j.id for j in journals]))
     if len(journals) == 0:
-        print "WARN: issn", issn, "does not map to any journals"
+        print("WARN: issn", issn, "does not map to any journals")
     if len(journals) > 0:
         return journals[0].id
 
@@ -799,7 +799,7 @@
     if len(sys.argv) > 1:
         IN_DIR = sys.argv[1]
     else:
-        print "you must specify a data directory to migrate from"
+        print("you must specify a data directory to migrate from")
         exit()
 
     JOURNALS = os.path.join(IN_DIR, "journals")
--- ./portality/migrate/st2cl/pages.py	(original)
+++ ./portality/migrate/st2cl/pages.py	(refactored)
@@ -24,17 +24,17 @@
     os.mkdir(OUT)
     
 for lang in languages:
-    for name, page in pages.iteritems():
+    for name, page in pages.items():
         url = None
         if "?" not in page:
             url = page + "?uiLanguage=" + lang
         else:
             url = page + "&uiLanguage=" + lang
-        print url,
+        print(url, end=' ')
         sys.stdout.flush()
         resp = requests.get(url)
         if resp.status_code >= 400:
-            print resp.status_code
+            print(resp.status_code)
             continue
         directory = os.path.join(OUT, lang)
         if not os.path.exists(directory):
@@ -42,4 +42,4 @@
         f = os.path.join(directory, name + ".html")
         with codecs.open(f, "wb", "utf8") as fh:
             fh.write(resp.text)
-        print "done"
+        print("done")
--- ./portality/migrate/subjects/remove_duplicate_subjects.py	(original)
+++ ./portality/migrate/subjects/remove_duplicate_subjects.py	(refactored)
@@ -48,13 +48,13 @@
 
         # When we have reached the batch limit, do some writing or deleting
         if len(write_batch) >= batch_size:
-            print "writing ", len(write_batch)
+            print("writing ", len(write_batch))
             models.Article.bulk(write_batch)
             write_batch = []
 
     # Finish the last part-batches of writes
     if len(write_batch) > 0:
-        print "writing ", len(write_batch)
+        print("writing ", len(write_batch))
         models.Article.bulk(write_batch)
 
     return updated_count, same_count
@@ -73,7 +73,7 @@
     args = parser.parse_args()
 
     if app.config.get("SCRIPTS_READ_ONLY_MODE", False):
-        print "System is in READ-ONLY mode, enforcing read-only for this script"
+        print("System is in READ-ONLY mode, enforcing read-only for this script")
         args.write = False
 
     # Connection to the ES index, rely on esprit sorting out the port from the host
@@ -82,14 +82,14 @@
     (u, s) = rem_dup_sub(conn, args.write)
 
     if args.write:
-        print "Done. {0} articles updated, {1} remain unchanged.".format(u, s)
+        print("Done. {0} articles updated, {1} remain unchanged.".format(u, s))
     else:
-        print "Not written. {0} articles to be updated, {1} to remain unchanged. Set -w to write changes.".format(u, s)
+        print("Not written. {0} articles to be updated, {1} to remain unchanged. Set -w to write changes.".format(u, s))
 
     if len(failed_articles) > 0:
-        print "Failed to create models for some articles in theRefactoringTool: Refactored ./portality/migrate/tick/check_tick.py
RefactoringTool: No changes to ./portality/models/__init__.py
RefactoringTool: No changes to ./portality/models/account.py
RefactoringTool: Refactored ./portality/models/article.py
RefactoringTool: No changes to ./portality/models/atom.py
RefactoringTool: Refactored ./portality/models/background.py
 index. Something is quite wrong."
+        print("Failed to create models for some articles in the index. Something is quite wrong.")
         for f in failed_articles:
-            print f
+            print(f)
 
     end = datetime.now()
-    print start, "-", end
+    print(start, "-", end)
--- ./portality/migrate/tick/check_tick.py	(original)
+++ ./portality/migrate/tick/check_tick.py	(refactored)
@@ -15,15 +15,15 @@
     
     if len(batch) >= batch_size:
         total += len(batch)
-        print "writing", len(batch), "; total so far", total
+        print("writing", len(batch), "; total so far", total)
         models.Journal.bulk(batch)
         batch = []
 
 if len(batch) > 0:
     total += len(batch)
-    print "writing", len(batch), "; total so far", total
+    print("writing", len(batch), "; total so far", total)
     models.Journal.bulk(batch)
 
 end = datetime.now()
 
-print start, "-", end
+print(start, "-", end)
--- ./portality/models/article.py	(original)
+++ ./portality/models/article.py	(refactored)
@@ -9,6 +9,7 @@
 
 import string
 from unidecode import unidecode
+from functools import reduce
 
 
 class NoJournalException(Exception):
@@ -22,7 +23,7 @@
     def duplicates(cls, issns=None, publisher_record_id=None, doi=None, fulltexts=None, title=None, volume=None, number=None, start=None, should_match=None, size=10):
         # some input sanitisation
         issns = issns if isinstance(issns, list) else []
-        urls = fulltexts if isinstance(fulltexts, list) else [fulltexts] if isinstance(fulltexts, str) or isinstance(fulltexts, unicode) else []
+        urls = fulltexts if isinstance(fulltexts, list) else [fulltexts] if isinstance(fulltexts, str) or isinstance(fulltexts, str) else []
 
         # make sure that we're dealing with the normal form of the identifiers
         norm_urls = []
@@ -1076,7 +1077,7 @@
         self.issns = issns if isinstance(issns, list) else []
         self.publisher_record_id = publisher_record_id
         self.doi = doi
-        self.urls = urls if isinstance(urls, list) else [urls] if isinstance(urls, str) or isinstance(urls, unicode) else []
+        self.urls = urls if isinstance(urls, list) else [urls] if isinstance(urls, str) or isinstance(urls, str) else []
         self.title = title
         self.volume = volume
         self.number = number
--- ./portality/models/background.py	(original)
+++ ./portality/models/background.py	(refactored)
@@ -11,7 +11,7 @@
 
         self._add_struct(BACKGROUND_STRUCT)
         if "status" not in kwargs:
-            kwargs["status"] = u"queued"
+            kwargs["status"] = "queued"
 
         super(BackgroundJob, self).__init__(raw=kwargs)
 
@@ -61,22 +61,22 @@
         return self._get_single("status")
 
     def start(self):
-        self._set_with_struct("status", u"processing")
+        self._set_with_struct("status", "processing")
 
     def success(self):
-        self._set_with_struct("status", u"complete")
+        self._set_with_struct("status", "complete")
 
     def fail(self):
-        self._set_with_struct("status", u"error")
+        self._set_with_struct("status", "error")
 
     def cancel(self):
-        self._set_with_struct("status", u"cancelled")
+        self._set_with_struct("status", "cancelled")
 
     def is_failed(self):
-        return self._get_single("status") == u"error"
+        return self._get_single("status") == "error"
 
     def queue(self):
-        self._set_with_struct("status", u"queued")
+        self._set_with_struct("status", "queued")
 
     def add_audit_message(self, msg, timestamp=None):
         if timestamp is None:
@@ -100,7 +100,7 @@
         "id" :{"coerce" : "unicode"},
         "created_date" : {"coerce" : "utcdatetime"},
         "last_updated" : {"coerce" : "utcdatetime"},
-        "status" : {"coerce" : "unicode", "allowed_values" : [u"queued", u"processing", u"complete", u"error", u"cancelled"]},
+        "status" : {"coerce" : "unicode", "allowed_values" : ["queued", "processing", "complete", "error", "cancelled"]},
         "user" : {"coerce" : "unicode"},
   RefactoringTool: No changes to ./portality/models/bibjson.py
RefactoringTool: No changes to ./portality/models/cache.py
RefactoringTool: No changes to ./portality/models/editors.py
RefactoringTool: No changes to ./portality/models/history.py
RefactoringTool: Refactored ./portality/models/journal.py
RefactoringTool: No changes to ./portality/models/lcc.py
RefactoringTool: No changes to ./portality/models/lock.py
RefactoringTool: No changes to ./portality/models/oaipmh.py
RefactoringTool: Refactored ./portality/models/openurl.py
RefactoringTool: No changes to ./portality/models/provenance.py
RefactoringTool: No changes to ./portality/models/search.py
RefactoringTool: No changes to ./portality/models/shared_structs.py
RefactoringTool: No changes to ./portality/models/suggestion.py
RefactoringTool: No changes to ./portality/models/uploads.py
RefactoringTool: Refactored ./portality/scripts/accounts_with_marketing_consent.py
RefactoringTool: Refactored ./portality/scripts/accounts_with_missing_api_role.py
RefactoringTool: Refactored ./portality/scripts/accounts_with_missing_passwords.py
RefactoringTool: Refactored ./portality/scripts/anon_export.py
      "action" : {"coerce" : "unicode"},
         "queue_id" : {"coerce" : "unicode"}
--- ./portality/models/journal.py	(original)
+++ ./portality/models/journal.py	(refactored)
@@ -157,7 +157,7 @@
                 clusters[note["date"]] = [note]
             else:
                 clusters[note["date"]].append(note)
-        ordered_keys = sorted(clusters.keys(), reverse=True)
+        ordered_keys = sorted(list(clusters.keys()), reverse=True)
         ordered = []
         for key in ordered_keys:
             clusters[key].reverse()
@@ -350,7 +350,7 @@
             index["country"] = country
         if len(schema_codes) > 0:
             index["schema_code"] = schema_codes
-        if len(urls.keys()) > 0:
+        if len(list(urls.keys())) > 0:
             index.update(urls)
         if has_seal:
             index["has_seal"] = has_seal
@@ -733,7 +733,7 @@
     def _calculate_has_apc(self):
         # work out of the journal has an apc
         has_apc = "No Information"
-        apc_field_present = len(self.bibjson().apc.keys()) > 0
+        apc_field_present = len(list(self.bibjson().apc.keys())) > 0
         if apc_field_present:
             has_apc = "Yes"
         elif self.is_ticked():
--- ./portality/models/openurl.py	(original)
+++ ./portality/models/openurl.py	(refactored)
@@ -49,7 +49,7 @@
 
         # Save any attributes specified at creation time
         if kwargs:
-            for key, value in kwargs.iteritems():
+            for key, value in kwargs.items():
                 setattr(self, key, value)
 
     def __str__(self):
--- ./portality/scripts/accounts_with_marketing_consent.py	(original)
+++ ./portality/scripts/accounts_with_marketing_consent.py	(refactored)
@@ -46,7 +46,7 @@
     args = parser.parse_args()
 
     if not args.out:
-        print "Please specify an output file path with the -o option"
+        print("Please specify an output file path with the -o option")
         parser.print_help()
         exit()
 
--- ./portality/scripts/accounts_with_missing_api_role.py	(original)
+++ ./portality/scripts/accounts_with_missing_api_role.py	(refactored)
@@ -50,7 +50,7 @@
     args = parser.parse_args()
 
     if not args.out:
-        print "Please specify an output file path with the -o option"
+        print("Please specify an output file path with the -o option")
         parser.print_help()
         exit()
 
--- ./portality/scripts/accounts_with_missing_passwords.py	(original)
+++ ./portality/scripts/accounts_with_missing_passwords.py	(refactored)
@@ -46,7 +46,7 @@
     args = parser.parse_args()
 
     if not args.out:
-        print "Please specify an output file path with the -o option"
+        print("Please specify an output file path with the -o option")
         parser.print_help()
         exit()
 
--- ./portality/scripts/anon_export.py	(original)
+++ ./portality/scripts/anon_export.py	(refactored)
@@ -90,14 +90,14 @@
 def _copy_on_complete(path):
     name = os.path.basename(path)
     raw_size = os.path.getsize(path)
-    print("Compressing temporary file {x} (from {y} bytes)".format(x=path, y=raw_size))
+    print(("Compressing temporary file {x} (from {y} bytes)".format(x=path, y=raw_size)))
     zipped_name = name + ".gz"
     dir = os.path.dirname(path)
     zipped_path = os.path.join(dir, zipped_name)
     with open(path, "rb") as f_in, gzip.open(zipped_path, "wb") as f_out:
         shutil.copyfileobj(f_in, f_out)
     zipped_size = os.path.getsize(zipped_path)
-    print("Storing from temporary file {x} ({y} bytes)".format(x=zipped_name, y=zipped_size))
+    print(("Storing from temporary file {x} ({y} bytes)".format(x=zipped_name, y=zipped_size)))
     mainStore.store(container, name, source_path=zipped_path)
     tmpStore.delete_file(container, name)
     tmpStore.delete_file(container, zipped_name)
@@ -128,7 +128,7 @@
     for type_ in esprit.raw.list_types(connection=conn):
         filename = type_ + ".bulk"
         output_file = tmpStore.path(container, filename, create_container=True, must_exist=False)
-        print(dates.now() + " " + type_ + " => " + output_RefactoringTool: Refactored ./portality/scripts/anon_import.py
RefactoringTool: Refactored ./portality/scripts/applicationrm.py
file + ".*")
+        print((dates.now() + " " + type_ + " => " + output_file + ".*"))
         if type_ in anonymisation_procedures:
             transform = anonymisation_procedures[type_]
             filenames = esprit.tasks.dump(conn, type_, limit=limit, transform=transform,
@@ -139,7 +139,7 @@
                                           out_template=output_file, out_batch_sizes=args.batch, out_rollover_callback=_copy_on_complete,
                                           es_bulk_fields=["_id"])
 
-        print(dates.now() + " done\n")
+        print((dates.now() + " done\n"))
 
     tmpStore.delete_container(container)
 
--- ./portality/scripts/anon_import.py	(original)
+++ ./portality/scripts/anon_import.py	(refactored)
@@ -15,28 +15,28 @@
         app.config["ELASTIC_SEARCH_DB"] = index
 
     print("\n")
-    print("Using host {x} and index {y}\n".format(x=host, y=index))
+    print(("Using host {x} and index {y}\n".format(x=host, y=index)))
     conn = esprit.raw.make_connection(None, host, None, index)
 
     # filter for the types we are going to work with
     import_types = {}
-    for t, s in config.get("types", {}).iteritems():
+    for t, s in config.get("types", {}).items():
         if s.get("import", False) is True:
             import_types[t] = s
 
     print("==Carrying out the following import==")
-    for import_type, cfg in import_types.iteritems():
+    for import_type, cfg in import_types.items():
         count = "All" if cfg.get("limit", -1) == -1 else cfg.get("limit")
-        print("{x} from {y}".format(x=count, y=import_type))
+        print(("{x} from {y}".format(x=count, y=import_type)))
     print("\n")
 
     if config.get("confirm", True):
-        text = raw_input("Continue? [y/N] ")
+        text = input("Continue? [y/N] ")
         if text.lower() != "y":
             exit()
 
     # remove all the types that we are going to import
-    for import_type in import_types.keys():
+    for import_type in list(import_types.keys()):
         esprit.raw.delete(conn, import_type)
 
     # re-initialise the index (sorting out mappings, etc)
@@ -48,10 +48,10 @@
     container = app.config.get("STORE_ANON_DATA_CONTAINER")
 
     print("\n==Importing==")
-    for import_type, cfg in import_types.iteritems():
+    for import_type, cfg in import_types.items():
         count = "all" if cfg.get("limit", -1) == -1 else cfg.get("limit")
-        print("Importing {x} from {y}".format(x=count, y=import_type))
-        print("Obtaining {x} from storage".format(x=import_type))
+        print(("Importing {x} from {y}".format(x=count, y=import_type)))
+        print(("Obtaining {x} from storage".format(x=import_type)))
 
         limit = cfg.get("limit", -1)
         limit = None if limit == -1 else limit
@@ -63,17 +63,17 @@
             if handle is None:
                 break
             tempStore.store(container, filename + ".gz", source_stream=handle)
-            print("Retrieved {x} from storage".format(x=filename))
+            print(("Retrieved {x} from storage".format(x=filename)))
             handle.close()
 
-            print("Unzipping {x} in temporary store".format(x=filename))
+            print(("Unzipping {x} in temporary store".format(x=filename)))
             compressed_file = tempStore.path(container, filename + ".gz")
             uncompressed_file = tempStore.path(container, filename, must_exist=False)
             with gzip.open(compressed_file, "rb") as f_in, open(uncompressed_file, "wb") as f_out:
                 shutil.copyfileobj(f_in, f_out)
             tempStore.delete_file(container, filename + ".gz")
 
-            print("Importing from {x}".format(x=filename))
+            print(("Importing from {x}".format(x=filename)))
             imported_count = esprit.tasks.bulk_load(conn, import_type, uncompressed_file,
                                                     limit=limit, max_content_length=config.get("max_content_length", 100000000))
             tempStore.delete_file(container, filename)
--- ./portality/scripts/applicationrm.py	(original)
+++ ./portality/scriptRefactoringTool: Refactored ./portality/scripts/article_cleanup_sync.py
RefactoringTool: Refactored ./portality/scripts/article_duplicate_analysis.py
s/applicationrm.py	(refactored)
@@ -4,7 +4,7 @@
 if __name__ == "__main__":
 
     if app.config.get("SCRIPTS_READ_ONLY_MODE", False):
-        print "System is in READ-ONLY mode, script cannot run"
+        print("System is in READ-ONLY mode, script cannot run")
         exit()
 
     import argparse
@@ -16,7 +16,7 @@
     args = parser.parse_args()
 
     if not args.email:
-        print "Please specify an username with the -e option"
+        print("Please specify an username with the -e option")
         exit()
 
     statuses = []
--- ./portality/scripts/article_cleanup_sync.py	(original)
+++ ./portality/scripts/article_cleanup_sync.py	(refactored)
@@ -21,21 +21,21 @@
     args = parser.parse_args()
 
     if app.config.get("SCRIPTS_READ_ONLY_MODE", False):
-        print "System is in READ-ONLY mode, enforcing read-only for this script"
+        print("System is in READ-ONLY mode, enforcing read-only for this script")
         args.write = False
 
     if args.prepall and not args.write:
-        print "Prep all must be used with the -w flag set too (why prep but not save?). Exiting."
+        print("Prep all must be used with the -w flag set too (why prep but not save?). Exiting.")
         exit(1)
     elif args.prepall:
-        print "Prep all arg set. 'unchanged' articles will also have their indexes refreshed."
+        print("Prep all arg set. 'unchanged' articles will also have their indexes refreshed.")
 
     job = asc.ArticleCleanupSyncBackgroundTask.prepare("testuser", prepall=args.prepall, write=args.write)
     task = asc.ArticleCleanupSyncBackgroundTask(job)
     BackgroundApi.execute(task)
 
     end = datetime.now()
-    print start, "-", end
+    print(start, "-", end)
 
     for a in job.audit:
-        print a
+        print(a)
--- ./portality/scripts/article_duplicate_analysis.py	(original)
+++ ./portality/scripts/article_duplicate_analysis.py	(refactored)
@@ -169,9 +169,9 @@
     def report(self):
         return "\n".join(
             [k + " - " + "; ".join(
-                [a + " (" + "|".join(b) + ")" for a, b in v.iteritems()]
+                [a + " (" + "|".join(b) + ")" for a, b in v.items()]
             )
-            for k, v in self._actions.iteritems()]
+            for k, v in self._actions.items()]
         )
 
     def export_to(self, final_instructions):
@@ -185,7 +185,7 @@
 
     def resolve(self):
         resolved_actions = []
-        for k, v in self._actions.iteritems():
+        for k, v in self._actions.items():
             resolved = {"id" : k, "action" : None, "reason" : None}
             if "delete" in v:
                 resolved["action"] = "delete"
@@ -194,7 +194,7 @@
                 resolved["action"] = "delete"
                 resolved["reason"] = "; ".join(v["remove_doi"]) + "|" + "; ".join("remove_fulltext")
             else:
-                resolved["action"] = v.keys()[0]
+                resolved["action"] = list(v.keys())[0]
                 resolved["reason"] = "; ".join(v[resolved["action"]])
             resolved_actions.append(resolved)
         return resolved_actions
@@ -211,7 +211,7 @@
         reader = clcsv.UnicodeReader(f)
         noaction_writer = clcsv.UnicodeWriter(g)
         nocleanup_writer = clcsv.UnicodeWriter(h)
-        headers = reader.next()
+        headers = next(reader)
         noaction_writer.writerow(headers)
         noaction_writer.writerow([])
         nocleanup_writer.writerow(headers)
@@ -271,13 +271,13 @@
 
         with codecs.open(noids_report, "rb", "utf-8") as n:
             nreader = clcsv.UnicodeReader(n)
-            headers = nreader.next()
+            headers = next(nreader)
             for row in nreader:
                 final_instructions[row[0]] = {"action" : "delete", "reason" : "no doi or fulltext"}
 
         writer = clcsv.UnicodeWriter(o)
         writer.writerow(["id", "action", "reason"])
-        for k, v in final_instructions.iteritems():
+        for k, v in final_instructions.items():
             writer.writerow([k, v["action"], v["reason"]])
 
 
@@ -441,7 +441,7 @@
             next_rRefactoringTool: Refactored ./portality/scripts/article_duplicate_report.py
RefactoringTool: Refactored ./portality/scripts/articledump.py
RefactoringTool: Refactored ./portality/scripts/articleload.py
ow = None
         else:
             try:
-                row = reader.next()
+                row = next(reader)
             except StopIteration:
                 return match_set, None
 
@@ -503,7 +503,7 @@
     actions = ActionRegister()
     with codecs.open(source, "rb", "utf-8") as s:
         reader = clcsv.UnicodeReader(s)
-        headers = reader.next()
+        headers = next(reader)
 
         accounts = {}
         for row in reader:
@@ -548,13 +548,13 @@
     with codecs.open(final_actions, "wb", "utf-8") as fa:
         fawriter = clcsv.UnicodeWriter(fa)
         fawriter.writerow(["id", "action", "reason"])
-        for k, v in final_instructions.iteritems():
+        for k, v in final_instructions.items():
             fawriter.writerow([k, v["action"], v["reason"]])
 
     with codecs.open(report_out, "wb", "utf-8") as ro:
         writer = clcsv.UnicodeWriter(ro)
         writer.writerow(["account", "articles to delete", "article_details"])
-        for k, v in accounts.iteritems():
+        for k, v in accounts.items():
             fn = k + "_articles.csv"
             with codecs.open(os.path.join(articles_dir, fn), "wb", "utf-8") as a:
                 awriter = clcsv.UnicodeWriter(a)
@@ -585,21 +585,21 @@
 
     with codecs.open(original, "rb", "utf-8") as f1:
         r1 = clcsv.UnicodeReader(f1)
-        r1.next()
+        next(r1)
         id1 = [x[0] for x in r1]
 
     with codecs.open(compare, "rb", "utf-8") as f2:
         r2 = clcsv.UnicodeReader(f2)
-        r2.next()
+        next(r2)
         id2 = [x[0] for x in r2]
 
     missing = [x for x in id1 if x not in id2]
-    print("missing {x}".format(x=len(missing)))
+    print(("missing {x}".format(x=len(missing))))
     with codecs.open(missing_out, "wb", "utf-8") as f3:
         f3.write("\n".join(missing))
 
     extra = [x for x in id2 if x not in id1]
-    print("extra {x}".format(x=len(extra)))
+    print(("extra {x}".format(x=len(extra))))
     with codecs.open(extra_out, "wb", "utf-8") as f4:
         f4.write("\n".join(extra))
 
@@ -607,7 +607,7 @@
             codecs.open(reference, "wb", "utf-8") as f6:
         r5 = clcsv.UnicodeReader(f5)
         w6 = clcsv.UnicodeWriter(f6)
-        headers = r5.next()
+        headers = next(r5)
         w6.writerow(headers)
         w6.writerow([])
 
@@ -622,7 +622,7 @@
                         continue
                     seen_roots.append(root_id)
 
-                    print("Reference set for root id {x}".format(x=root_id))
+                    print(("Reference set for root id {x}".format(x=root_id)))
                     rows = match_set.to_rows()
                     for row in rows:
                         w6.writerow(row)
--- ./portality/scripts/article_duplicate_report.py	(original)
+++ ./portality/scripts/article_duplicate_report.py	(refactored)
@@ -53,5 +53,5 @@
     task = article_duplicate_report.ArticleDuplicateReportBackgroundTask(job)
     BackgroundApi.execute(task)
     for msg in job.audit:
-        print(msg["timestamp"] + " - " + msg["message"])
+        print((msg["timestamp"] + " - " + msg["message"]))
     app.logger.debug("Finished " + dates.now_with_microseconds())
--- ./portality/scripts/articledump.py	(original)
+++ ./portality/scripts/articledump.py	(refactored)
@@ -11,11 +11,11 @@
     args = parser.parse_args()
 
     if not args.username:
-        print "Please specify a username with the -u option"
+        print("Please specify a username with the -u option")
         exit()
 
     if not args.out:
-        print "Please specify and output file with the -o option"
+        print("Please specify and output file with the -o option")
         exit()
 
     issns = models.Journal.issns_by_owner(args.username)
--- ./portality/scripts/articleload.py	(original)
+++ ./portality/scripts/articleload.py	(refactored)
@@ -4,7 +4,7 @@
 
 if __name__ == "__main__":
     if app.config.get("SCRIPTS_READ_ONLY_MODE", False):
-        print "System is in READ-ONLY mode, script cannot run"
+        print("System is in READ-ONLY mode, script cannot run")
         exit()RefactoringTool: Refactored ./portality/scripts/articlerm.py
RefactoringTool: Refactored ./portality/scripts/async_workflow_notifications.py
RefactoringTool: Refactored ./portality/scripts/change_application_status.py

 
     import argparse
@@ -15,7 +15,7 @@
     args = parser.parse_args()
 
     if not args.source:
-        print "Please specify a source file with the -s option"
+        print("Please specify a source file with the -s option")
         exit()
 
     f = codecs.open(args.source)
--- ./portality/scripts/articlerm.py	(original)
+++ ./portality/scripts/articlerm.py	(refactored)
@@ -24,9 +24,9 @@
             article.bibjson().remove_identifiers(idtype=constants.IDENT_TYPE_DOI)
             article.save()
         else:
-            print("WARN: could not remove DOI from {0} as it has no fulltext URL".format(article_id))
+            print(("WARN: could not remove DOI from {0} as it has no fulltext URL".format(article_id)))
     except AttributeError as e:
-        print("ERROR: could not remove DOI from {0}: {1}".format(article_id, e.message))
+        print(("ERROR: could not remove DOI from {0}: {1}".format(article_id, e.message)))
     
 
 def remove_fulltext(article_id):
@@ -38,9 +38,9 @@
             article.bibjson().remove_urls(urltype=constants.LINK_TYPE_FULLTEXT)
             article.save()
         else:
-            print("WARN: could not remove Fulltext from {0} as it has no DOI".format(article_id))
+            print(("WARN: could not remove Fulltext from {0} as it has no DOI".format(article_id)))
     except AttributeError as e:
-        print("ERROR: could not remove fulltext from {0}: {1}".format(article_id, e.message))
+        print(("ERROR: could not remove fulltext from {0}: {1}".format(article_id, e.message)))
 
 
 if __name__ == "__main__":
@@ -76,7 +76,7 @@
 
     if args.username is not None:
         models.Article.delete_selected(owner=args.username, snapshot=snapshot)
-        print "Articles deleted"
+        print("Articles deleted")
     elif args.query is not None:
         f = open(args.query)
         query = json.loads(f.read())
@@ -99,7 +99,7 @@
         # hits['total'] will show you all results that match the query,
         # not just the articles that will actually be deleted (which
         # will be just the page of results specified by from: and size:).
-        go_on = raw_input("This will delete " + str(total) + " articles.  Are you sure? [Y/N]:")
+        go_on = input("This will delete " + str(total) + " articles.  Are you sure? [Y/N]:")
         if go_on.lower() == "y":
             models.Article.delete_selected(query=query, snapshot=snapshot)
             print("Articles deleted")
--- ./portality/scripts/async_workflow_notifications.py	(original)
+++ ./portality/scripts/async_workflow_notifications.py	(refactored)
@@ -7,7 +7,7 @@
 if __name__ == "__main__":
     user = app.config.get("SYSTEM_USERNAME")
     if user is None:
-        print "System user not specified. Set SYSTEM_USERNAME in config file to run this script."
+        print("System user not specified. Set SYSTEM_USERNAME in config file to run this script.")
         exit(1)
     else:
         job = async_workflow_notifications.AsyncWorkflowBackgroundTask.prepare(user)
--- ./portality/scripts/change_application_status.py	(original)
+++ ./portality/scripts/change_application_status.py	(refactored)
@@ -15,7 +15,7 @@
         a.save()
 
 if __name__ == "__main__":
-    print 'Starting {0}.'.format(datetime.now())
+    print('Starting {0}.'.format(datetime.now()))
 
     import argparse
     parser = argparse.ArgumentParser()
@@ -24,11 +24,11 @@
     args = parser.parse_args()
 
     if app.config.get("SCRIPTS_READ_ONLY_MODE", False):
-        print "System is in READ-ONLY mode, enforcing read-only for this script"
+        print("System is in READ-ONLY mode, enforcing read-only for this script")
         args.write = False
 
     if not args.csv or not args.status:
-        print "You must provide both the -c and -s options. Exiting."
+        print("You must provide both the -c and -s options. Exiting.")
         exit(1)
 
     with open(args.csv, "r") as f:
@@ -36,4 +36,4 @@
         ids = [row[0] for row in reader]
         change_status(ids, args.status)
 
-    print 'Finished {0}.'.format(datetime.now())
+   RefactoringTool: Refactored ./portality/scripts/createuser.py
RefactoringTool: Refactored ./portality/scripts/generate_iso639b_language_code_schema.py
RefactoringTool: No changes to ./portality/scripts/history_dirs_reports.py
RefactoringTool: Refactored ./portality/scripts/history_records_analyse.py
RefactoringTool: Refactored ./portality/scripts/history_records_assemble.py
 print('Finished {0}.'.format(datetime.now()))
--- ./portality/scripts/createuser.py	(original)
+++ ./portality/scripts/createuser.py	(refactored)
@@ -30,7 +30,7 @@
     acc.set_password(password)
     acc.save()
 
-    print(prefix + username)
+    print((prefix + username))
 
 
 def input_password(username):
@@ -44,14 +44,14 @@
     password = getpass.getpass()
     confirm = getpass.getpass("Confirm Password for " + username + ":")
     if password != confirm:
-        print "passwords do not match - try again!"
+        print("passwords do not match - try again!")
         return None
     return password
 
 
 if __name__ == "__main__":
     if app.config.get("SCRIPTS_READ_ONLY_MODE", False):
-        print "System is in READ-ONLY mode, script cannot run"
+        print("System is in READ-ONLY mode, script cannot run")
         exit()
 
     import argparse, getpass
@@ -66,11 +66,11 @@
     args = parser.parse_args()
     
     if not args.username and not args.csv:
-        print "Please specify a username with the -u option or a CSV of users with the -c option"
+        print("Please specify a username with the -u option or a CSV of users with the -c option")
         exit()
     
     if not args.role and not args.csv:
-        print "WARNING: no role specified, so this user won't be able to do anything"
+        print("WARNING: no role specified, so this user won't be able to do anything")
 
     if args.username:
         username = args.username
--- ./portality/scripts/generate_iso639b_language_code_schema.py	(original)
+++ ./portality/scripts/generate_iso639b_language_code_schema.py	(refactored)
@@ -97,8 +97,8 @@
                                            namespaces=new_tree.nsmap)]
 
     # List the simplified language entries
-    old_strlist = [u'{0}\t{1}'.format(t[0], t[1]) for t in zip(old_lc, old_ln)]
-    new_strlist = [u'{0}\t{1}'.format(t[0], t[1]) for t in zip(new_lc, new_ln)]
+    old_strlist = ['{0}\t{1}'.format(t[0], t[1]) for t in zip(old_lc, old_ln)]
+    new_strlist = ['{0}\t{1}'.format(t[0], t[1]) for t in zip(new_lc, new_ln)]
 
     old_file = schema_old.split('/').pop()
     new_file = schema_new.split('/').pop()
@@ -109,7 +109,7 @@
     with open(ofile, 'w') as o:
         o.writelines(l.encode('utf8') for l in diff)
 
-    print("Diff saved to " + ofile)
+    print(("Diff saved to " + ofile))
 
 
 if __name__ == '__main__':
@@ -126,8 +126,8 @@
     dest_path = paths.rel2abs(__file__, '..', 'static', 'doaj', args.filename)
 
     if os.path.exists(dest_path):
-        print('Schema already exists with name {n} - replace? [y/N]'.format(n=args.filename))
-        resp = raw_input('Your existing file will be retained as {fn}.old : '.format(fn=args.filename))
+        print(('Schema already exists with name {n} - replace? [y/N]'.format(n=args.filename)))
+        resp = input('Your existing file will be retained as {fn}.old : '.format(fn=args.filename))
         if resp.lower() == 'y':
             os.rename(dest_path, dest_path + '.old')
 
--- ./portality/scripts/history_records_analyse.py	(original)
+++ ./portality/scripts/history_records_analyse.py	(refactored)
@@ -22,7 +22,7 @@
     records = {}
     with codecs.open(source, "rb", "utf-8") as f:
         reader = clcsv.UnicodeReader(f)
-        reader.next()
+        next(reader)
         for row in reader:
             if date is None or row[0] in ids:
                 if row[0] not in records:
@@ -36,7 +36,7 @@
         writer.writerow(["count", "id", "reverted", "change history"])
         writer.writerow([])
 
-        for id, rows in records.iteritems():
+        for id, rows in records.items():
             rows = sorted(rows, key=lambda x: x[1])
             owners = []
             lastOwner = False
--- ./portality/scripts/history_records_assemble.py	(original)
+++ ./portality/scripts/history_records_assemble.py	(refactored)
@@ -90,7 +90,7 @@
         if not isinstance(d, dict):
             return d
         newData = {}
-        for k, v in d.iteritems():
+        for k, v in d.items():
             if isinstance(k, symbols.SymboRefactoringTool: Refactored ./portality/scripts/inconsistent_journal_prov_application.py
RefactoringTool: Refactored ./portality/scripts/journalcsv.py
RefactoringTool: Refactored ./portality/scripts/manage_background_jobs.py
l):
                 newData[repr(k)] = v
             else:
@@ -101,7 +101,7 @@
         if not isinstance(d, dict):
             return d
         d = fix_layer(d)
-        for k, v in d.items():
+        for k, v in list(d.items()):
             d[k] = recurse(d[k])
         return d
 
--- ./portality/scripts/inconsistent_journal_prov_application.py	(original)
+++ ./portality/scripts/inconsistent_journal_prov_application.py	(refactored)
@@ -58,7 +58,7 @@
         for result in esprit.tasks.scroll(conn, "suggestion", keepalive="45m"):
             counter += 1
             application = Suggestion(**result)
-            print counter, application.id
+            print(counter, application.id)
 
             # Part 1 - later provenance records exist
             latest_prov = Provenance.get_latest_by_resource_id(application.id)
@@ -98,7 +98,7 @@
                         created = latest_prov.created_date
                     out_missing.writerow([application.id, application.last_manual_update, created, " ".join(application.bibjson().issns()), application.bibjson().title])
 
-        print "processed", counter, "suggestions"
+        print("processed", counter, "suggestions")
 
 
 # looks for journals that were created after the last update on an application, implying the application was not updated
@@ -119,7 +119,7 @@
         for result in esprit.tasks.scroll(conn, "journal", keepalive="45m"):
             counter += 1
             journal = Journal(**result)
-            print counter, journal.id
+            print(counter, journal.id)
 
             # first figure out if there is a broken related application
             issns = journal.bibjson().issns()
@@ -136,7 +136,7 @@
 
             jcreated = journal.created_timestamp
             reapp = journal.last_update_request
-            print counter, journal.id, reapp
+            print(counter, journal.id, reapp)
             if reapp is not None:
                 jcreated = datetime.strptime(reapp, "%Y-%m-%dT%H:%M:%SZ")
             jcreated = adjust_timestamp(jcreated, JOURNAL_TIMEZONE_CUTOFF)
@@ -183,7 +183,7 @@
                 if acc is None:
                     out_accounts.writerow([journal.id, journal.created_date, journal.last_update_request, str(journal.is_in_doaj()), owner])
 
-        print "processed", counter, "journals"
+        print("processed", counter, "journals")
 
 
 PROV_QUERY = {
@@ -199,7 +199,7 @@
 }
 
 if __name__ == "__main__":
-    print 'Starting {0}.'.format(datetime.now())
+    print('Starting {0}.'.format(datetime.now()))
     applications_inconsistencies("apps_with_prov.csv", "apps_accepted_without_journals.csv", local)
     journals_applications_provenance("journals_applications_provenance.csv", "journals_no_accounts.csv", "journals_reapp_fails.csv", local)
-    print 'Finished {0}.'.format(datetime.now())
+    print('Finished {0}.'.format(datetime.now()))
--- ./portality/scripts/journalcsv.py	(original)
+++ ./portality/scripts/journalcsv.py	(refactored)
@@ -5,7 +5,7 @@
 
 if __name__ == "__main__":
     if app.config.get("SCRIPTS_READ_ONLY_MODE", False):
-        print "System is in READ-ONLY mode, script cannot run"
+        print("System is in READ-ONLY mode, script cannot run")
         exit()
 
     user = app.config.get("SYSTEM_USERNAME")
--- ./portality/scripts/manage_background_jobs.py	(original)
+++ ./portality/scripts/manage_background_jobs.py	(refactored)
@@ -60,17 +60,17 @@
 
     jobs = models.BackgroundJob.q2obj(q=q.query())
 
-    print('You are about to {verb} {count} job(s)'.format(verb=verb, count=len(jobs)))
-    doit = raw_input('Proceed? [y\\N] ')
+    print(('You are about to {verb} {count} job(s)'.format(verb=verb, count=len(jobs))))
+    doit = input('Proceed? [y\\N] ')
 
     if doit.lower() == 'y':
         print('Please wait...')
         for job in jobs:
             if job.action not in HANDLERS:
-                print('This script is not set up to {0} task type {1}. Skipping.'.format(verb, job.action))
+                print(('This script is not set up to {0} task type {1}. Skipping.'.format(verb, job.RefactoringTool: No changes to ./portality/scripts/missing_q14q18.py
RefactoringTool: No changes to ./portality/scripts/missing_quick_reject_emails.py
RefactoringTool: Refactored ./portality/scripts/news.py
RefactoringTool: Refactored ./portality/scripts/not_in_doaj_with_articles.py
RefactoringTool: Refactored ./portality/scripts/orphaned_datasets.py
RefactoringTool: Refactored ./portality/scripts/prepare_delete_request.py
RefactoringTool: Refactored ./portality/scripts/prune_es_backups.py
RefactoringTool: Refactored ./portality/scripts/prune_marvel.py
action)))
                 continue
 
-            job.add_audit_message(u"Job {pp} from job management script.".format(
+            job.add_audit_message("Job {pp} from job management script.".format(
                 pp={'requeue': 'requeued', 'cancel': 'cancelled'}[verb]))
 
             if verb == 'requeue':                                                     # Re-queue and execute immediately
--- ./portality/scripts/news.py	(original)
+++ ./portality/scripts/news.py	(refactored)
@@ -4,7 +4,7 @@
 
 if __name__ == "__main__":
     if app.config.get("SCRIPTS_READ_ONLY_MODE", False):
-        print "System is in READ-ONLY mode, script cannot run"
+        print("System is in READ-ONLY mode, script cannot run")
         exit()
 
     user = app.config.get("SYSTEM_USERNAME")
--- ./portality/scripts/not_in_doaj_with_articles.py	(original)
+++ ./portality/scripts/not_in_doaj_with_articles.py	(refactored)
@@ -23,7 +23,7 @@
     args = parser.parse_args()
 
     if not args.out:
-        print "Please specify an output file path with the -o option"
+        print("Please specify an output file path with the -o option")
         parser.print_help()
         exit()
 
--- ./portality/scripts/orphaned_datasets.py	(original)
+++ ./portality/scripts/orphaned_datasets.py	(refactored)
@@ -54,4 +54,4 @@
         'applications': traverse_applications(),
         'journals': traverse_journals()
     }
-    print(json.dumps(full_report, indent=2))
+    print((json.dumps(full_report, indent=2)))
--- ./portality/scripts/prepare_delete_request.py	(original)
+++ ./portality/scripts/prepare_delete_request.py	(refactored)
@@ -1,4 +1,4 @@
-import sys, os, json, urllib, argparse
+import sys, os, json, urllib.request, urllib.parse, urllib.error, argparse
 
 TEST_SOURCE = "{%22query%22:{%22filtered%22:{%22query%22:{%22query_string%22:{%22query%22:%221893-3211%22,%22default_field%22:%22index.issn.exact%22,%22default_operator%22:%22AND%22}},%22filter%22:{%22bool%22:{%22must%22:%5B{%22term%22:{%22_type%22:%22article%22}}%5D}}}}}"
 
@@ -48,7 +48,7 @@
 
 
 def source2json(s):
-    parsed = urllib.unquote(s)
+    parsed = urllib.parse.unquote(s)
     return json.loads(parsed)
 
 
@@ -63,14 +63,14 @@
     args = parser.parse_args()
 
     if args.querystring:
-        print json.dumps(source2json(args.querystring), indent=3)
+        print(json.dumps(source2json(args.querystring), indent=3))
         sys.exit(0)
 
     if args.test:
-        print 'Test encoded source (e.g. from a url with a source= argument in it)'
-        print TEST_SOURCE
-        print '  Result:'
-        print json.dumps(source2json(TEST_SOURCE), indent=3)
+        print('Test encoded source (e.g. from a url with a source= argument in it)')
+        print(TEST_SOURCE)
+        print('  Result:')
+        print(json.dumps(source2json(TEST_SOURCE), indent=3))
         sys.exit(0)
 
     if not args.input and not args.outputdir:
@@ -79,7 +79,7 @@
         sys.exit(1)
     
     if (args.input and not args.outputdir) or (args.outputdir and not args.input):
-        print 'If you specify an input file, you must also specify the output dir. And vice versa.'
+        print('If you specify an input file, you must also specify the output dir. And vice versa.')
         sys.exit(1)
 
     convert_from_file_to_dir(args.input, args.outputdir)
--- ./portality/scripts/prune_es_backups.py	(original)
+++ ./portality/scripts/prune_es_backups.py	(refactored)
@@ -4,7 +4,7 @@
 
 if __name__ == "__main__":
     if app.config.get("SCRIPTS_READ_ONLY_MODE", False):
-        print "System is in READ-ONLY mode, script cannot run"
+        print("System is in READ-ONLY mode, script cannot run")
         exit()
 
     user = app.config.get("SYSTEM_USERNAME")
--- ./portality/scripts/prune_marvel.py	(original)
+++ ./portality/scripts/prune_marvel.py	(refactored)
@@ -13,11 +13,11 @@
 
 def main():
     delete_pattern = generate_delete_pattern()
-    print "To delete: {0}".format(delete_pattern)
+    print("To delete: {0}".format(delete_pattern))
 
     r = requests.delete("http://localhost:9200/{0}".format(delRefactoringTool: No changes to ./portality/scripts/public_data_dump.py
RefactoringTool: No changes to ./portality/scripts/publisher_email_in_doaj.py
RefactoringTool: Refactored ./portality/scripts/rejected_applications.py
RefactoringTool: No changes to ./portality/scripts/reports.py
RefactoringTool: Refactored ./portality/scripts/request_es_backup.py
RefactoringTool: No changes to ./portality/scripts/sitemap.py
RefactoringTool: Refactored ./portality/scripts/sync_doaj_records.py
RefactoringTool: Refactored ./portality/scripts/update_mapping_reindex.py
RefactoringTool: Refactored ./portality/tasks/article_bulk_delete.py
ete_pattern))
-    print 'Delete response status code: {0}'.format(r.status_code)
-    print "Delete response text:\n\n{0}".format(r.text)
+    print('Delete response status code: {0}'.format(r.status_code))
+    print("Delete response text:\n\n{0}".format(r.text))
 
 if __name__ == '__main__':
     main()
--- ./portality/scripts/rejected_applications.py	(original)
+++ ./portality/scripts/rejected_applications.py	(refactored)
@@ -51,7 +51,7 @@
     args = parser.parse_args()
 
     if not args.out:
-        print "Please specify an output file path with -o"
+        print("Please specify an output file path with -o")
         exit()
 
     do_report(args.out)
--- ./portality/scripts/request_es_backup.py	(original)
+++ ./portality/scripts/request_es_backup.py	(refactored)
@@ -4,8 +4,8 @@
 
 if __name__ == "__main__":
     if app.config.get("SCRIPTS_READ_ONLY_MODE", False):
-        print "System is in READ-ONLY mode, confirm to continue"
-        if raw_input("Continue? y/N: ").lower() != 'y':
+        print("System is in READ-ONLY mode, confirm to continue")
+        if input("Continue? y/N: ").lower() != 'y':
             exit()
 
     user = app.config.get("SYSTEM_USERNAME")
--- ./portality/scripts/sync_doaj_records.py	(original)
+++ ./portality/scripts/sync_doaj_records.py	(refactored)
@@ -40,7 +40,7 @@
         s_total = s['hits']['total']
         s = s['hits']['hits']
     except KeyError:
-        print 'Skipping', es_type, 'does not have created_date in its mapping. Probably 0 documents, so no mapping beyond the dynamic template has been created, so no point in syncing this type anyway.'
+        print('Skipping', es_type, 'does not have created_date in its mapping. Probably 0 documents, so no mapping beyond the dynamic template has been created, so no point in syncing this type anyway.')
         return
 
     d_total = d['hits']['total']
@@ -56,21 +56,21 @@
                 removed += 1
                 s.remove(s_hit)
 
-    print 'Putting', len(s), 'newest records into ES.'
+    print('Putting', len(s), 'newest records into ES.')
     # print 'You have 5 seconds to terminate this script with Ctrl+C if the number seems off.'
     # time.sleep(5)
 
     for diff in s:
         if testing:
-            print 'TESTING - would PUT', diff['_id']
+            print('TESTING - would PUT', diff['_id'])
         else:
             r = requests.put(
                 '{es_host}/{es_index}/{es_type}/{rec_id}'.format(es_host=config['ELASTIC_SEARCH_HOST'], es_index=config['ELASTIC_SEARCH_DB'], es_type=es_type, rec_id=diff['_id']),
                 data=json.dumps(diff['_source'])
             )
-            print diff['_id'], r.status_code
+            print(diff['_id'], r.status_code)
             if r.status_code not in [200, 201]:
-                print 'ES error for record', diff['_id'], 'HTTP status code:', r.status_code
+                print('ES error for record', diff['_id'], 'HTTP status code:', r.status_code)
 
     #print 'PUT', len(s), 'newest records into ES.'
 
--- ./portality/scripts/update_mapping_reindex.py	(original)
+++ ./portality/scripts/update_mapping_reindex.py	(refactored)
@@ -23,7 +23,7 @@
     old = esprit.raw.make_connection(None, app.config["ELASTIC_SEARCH_HOST"], None, args.old)
     new = esprit.raw.make_connection(None, app.config["ELASTIC_SEARCH_HOST"], None, args.new)
 
-    esprit.tasks.reindex(old, new, args.alias, new_mappings.keys(), new_mappings, new_version="1.7.5")
+    esprit.tasks.reindex(old, new, args.alias, list(new_mappings.keys()), new_mappings, new_version="1.7.5")
 
-    eq = esprit.tasks.compare_index_counts([old, new], new_mappings.keys())
-    print "all equal: ", eq
+    eq = esprit.tasks.compare_index_counts([old, new], list(new_mappings.keys()))
+    print("all equal: ", eq)
--- ./portality/tasks/article_bulk_delete.py	(original)
+++ ./portality/tasks/article_bulk_delete.py	(refactored)
@@ -56,17 +56,17 @@
         ids = self.get_param(params, 'ids')
 
         if not self._job_parameter_check(params):
-            raise BackgroundException(u"{}.run run without sufficieRefactoringTool: Refactored ./portality/tasks/article_cleanup_sync.py
nt parameters".format(self.__class__.__name__))
+            raise BackgroundException("{}.run run without sufficient parameters".format(self.__class__.__name__))
 
         batches_count = len(ids) / self.BATCH_SIZE + (0 if len(ids) % self.BATCH_SIZE == 0 else 1)
-        job.add_audit_message(u"About to delete {} articles in {} batches".format(len(ids), batches_count))
+        job.add_audit_message("About to delete {} articles in {} batches".format(len(ids), batches_count))
 
         for batch_num, batch in enumerate(batch_up(ids, self.BATCH_SIZE), start=1):
             article_delete_q_by_ids = models.Article.make_query(should_terms={'_id': batch}, consistent_order=False)
             models.Article.delete_selected(query=article_delete_q_by_ids, snapshot=True)
-            job.add_audit_message(u"Deleted {} articles in batch {} of {}".format(len(batch), batch_num, batches_count))
+            job.add_audit_message("Deleted {} articles in batch {} of {}".format(len(batch), batch_num, batches_count))
 
-        job.add_audit_message(u"Deleted {} articles".format(len(ids)))
+        job.add_audit_message("Deleted {} articles".format(len(ids)))
 
     def cleanup(self):
         """
@@ -112,7 +112,7 @@
         cls.set_param(params, 'ids', kwargs['ids'])
 
         if not cls._job_parameter_check(params):
-            raise BackgroundException(u"{}.prepare run without sufficient parameters".format(cls.__name__))
+            raise BackgroundException("{}.prepare run without sufficient parameters".format(cls.__name__))
 
         job.params = params
 
--- ./portality/tasks/article_cleanup_sync.py	(original)
+++ ./portality/tasks/article_cleanup_sync.py	(refactored)
@@ -52,7 +52,7 @@
 
                 # for debugging, just print out the progress
                 i += 1
-                print i, article_model.id, len(journal_cache.keys()), len(write_batch), len(delete_batch)
+                print(i, article_model.id, len(list(journal_cache.keys())), len(write_batch), len(delete_batch))
 
                 # Try to find journal in our cache
                 bibjson = article_model.bibjson()
@@ -67,8 +67,8 @@
                     else:
                         cache_miss = True
                 assoc_journal = None
-                if len(possibles.keys()) > 0:
-                    assoc_journal = self._get_best_journal(possibles.values())
+                if len(list(possibles.keys())) > 0:
+                    assoc_journal = self._get_best_journal(list(possibles.values()))
 
                 # Cache miss; ask the article model to try to find its journal
                 if assoc_journal is None or cache_miss:
@@ -112,31 +112,31 @@
 
             # When we have reached the batch limit, do some writing or deleting
             if len(write_batch) >= batch_size:
-                job.add_audit_message(u"Writing {x} articles".format(x=len(write_batch)))
+                job.add_audit_message("Writing {x} articles".format(x=len(write_batch)))
                 models.Article.bulk(write_batch)
                 write_batch = []
 
             if len(delete_batch) >= batch_size:
-                job.add_audit_message(u"Deleting {x} articles".format(x=len(delete_batch)))
+                job.add_audit_message("Deleting {x} articles".format(x=len(delete_batch)))
                 esprit.raw.bulk_delete(conn, 'article', delete_batch)
                 delete_batch.clear()
 
         # Finish the last part-batches of writes or deletes
         if len(write_batch) > 0:
-            job.add_audit_message(u"Writing {x} articles".format(x=len(write_batch)))
+            job.add_audit_message("Writing {x} articles".format(x=len(write_batch)))
             models.Article.bulk(write_batch)
         if len(delete_batch) > 0:
-            job.add_audit_message(u"Deleting {x} articles".format(x=len(delete_batch)))
+            job.add_audit_message("Deleting {x} articles".format(x=len(delete_batch)))
             esprit.raw.bulk_delete(conn, 'article', delete_batch)
             delete_batch.clear()
 
         if write_changes:
-            job.RefactoringTool: Refactored ./portality/tasks/article_duplicate_report.py
RefactoringTool: Refactored ./portality/tasks/async_workflow_notifications.py
RefactoringTool: No changes to ./portality/tasks/check_latest_es_backup.py
RefactoringTool: No changes to ./portality/tasks/consumer_long_running.py
RefactoringTool: No changes to ./portality/tasks/consumer_main_queue.py
RefactoringTool: Refactored ./portality/tasks/ingestarticles.py
add_audit_message(u"Done. {0} articles updated, {1} remain unchanged, and {2} deleted.".format(updated_count, same_count, deleted_count))
+            job.add_audit_message("Done. {0} articles updated, {1} remain unchanged, and {2} deleted.".format(updated_count, same_count, deleted_count))
         else:
-            job.add_audit_message(u"Done. Changes not written to index. {0} articles to be updated, {1} to remain unchanged, and {2} to be deleted. Set 'write' to write changes.".format(updated_count, same_count, deleted_count))
+            job.add_audit_message("Done. Changes not written to index. {0} articles to be updated, {1} to remain unchanged, and {2} to be deleted. Set 'write' to write changes.".format(updated_count, same_count, deleted_count))
 
         if len(failed_articles) > 0:
-            job.add_audit_message(u"Failed to create models for {x} articles in the index. Something is quite wrong.".format(x=len(failed_articles)))
+            job.add_audit_message("Failed to create models for {x} articles in the index. Something is quite wrong.".format(x=len(failed_articles)))
             job.add_audit_message("Failed article ids: {x}".format(x=", ".join(failed_articles)))
             job.fail()
 
@@ -166,16 +166,16 @@
             context[lmu][lu] = j
 
         context = None
-        if len(result["in_doaj"].keys()) > 0:
+        if len(list(result["in_doaj"].keys())) > 0:
             context = result["in_doaj"]
         else:
             context = result["not_in_doaj"]
 
-        lmus = context.keys()
+        lmus = list(context.keys())
         lmus.sort()
         context = context[lmus.pop()]
 
-        lus = context.keys()
+        lus = list(context.keys())
         lus.sort()
         best = context[lus.pop()]
         return best
--- ./portality/tasks/article_duplicate_report.py	(original)
+++ ./portality/tasks/article_duplicate_report.py	(refactored)
@@ -224,7 +224,7 @@
 
         # write rows to report
         a_summary = self._summarise_article(article, owner)
-        for k, v in dups.iteritems():
+        for k, v in dups.items():
             row = [article.id,
                    a_summary['created'],
                    a_summary['doi'],
--- ./portality/tasks/async_workflow_notifications.py	(original)
+++ ./portality/tasks/async_workflow_notifications.py	(refactored)
@@ -328,7 +328,7 @@
 
 def send_emails(emails_dict):
 
-    for (email, (to_name, paragraphs)) in emails_dict.iteritems():
+    for (email, (to_name, paragraphs)) in emails_dict.items():
         pre = 'Dear ' + to_name + ',\n\n'
         post = '\n\nThe DOAJ Team\n\n***\nThis is an automated message. Please do not reply to this email.'
         full_body = pre + '\n\n'.join(paragraphs) + post
--- ./portality/tasks/ingestarticles.py	(original)
+++ ./portality/tasks/ingestarticles.py	(refactored)
@@ -11,7 +11,7 @@
 from portality.lib import plugin
 
 import ftplib, os, requests, traceback, shutil
-from urlparse import urlparse
+from urllib.parse import urlparse
 
 DEFAULT_MAX_REMOTE_SIZE=262144000
 CHUNK_SIZE=1048576
@@ -90,7 +90,7 @@
             file_upload.downloaded()
             return True
 
-        msg = u'Bad code returned by FTP server for the file transfer: "{0}"'.format(c)
+        msg = 'Bad code returned by FTP server for the file transfer: "{0}"'.format(c)
         job.add_audit_message(msg)
         file_upload.failed(msg)
         return False
@@ -164,25 +164,25 @@
         params = job.params
 
         if params is None:
-            raise BackgroundException(u"IngestArticleBackgroundTask.run run without sufficient parameters")
+            raise BackgroundException("IngestArticleBackgroundTask.run run without sufficient parameters")
 
         file_upload_id = self.get_param(params, "file_upload_id")
         if file_upload_id is None:
-            raise BackgroundException(u"IngestArticleBackgroundTask.run run without sufficient parameters")
+            raise BackgroundException("IngestArticleBackgroundTask.run run without sufficient parameters")
 
         file_upload = models.FileUpload.pull(file_upload_id)
         if file_upload is None:
-            raise BackgroundException(u"IngestArticleBackgroundTask.run unable to find file upload with id {x}".format(x=file_upload_id))
+            raise BackgroundException("IngestArticleBackgroundTask.run unable to find file upload with id {x}".format(x=file_upload_id))
 
         try:
             # if the file "exists", this means its a remote file that needs to be downloaded, so do that
             if file_upload.status == "exists":
-                job.add_audit_message(u"Downloading file for file upload {x}, job {y}".format(x=file_upload_id, y=job.id))
+                job.add_audit_message("Downloading file for file upload {x}, job {y}".format(x=file_upload_id, y=job.id))
                 self._download(file_upload)
 
             # if the file is validated, which will happen if it has been uploaded, or downloaded successfully, process it.
             if file_upload.status == "validated":
-                job.add_audit_message(u"Importing file for file upload {x}, job {y}".format(x=file_upload_id, y=job.id))
+                job.add_audit_message("Importing file for file upload {x}, job {y}".format(x=file_upload_id, y=job.id))
                 self._process(file_upload)
         finally:
             file_upload.save()
@@ -201,12 +201,12 @@
             if not http_upload(job, path, file_upload):
                 return False
         else:
-            msg = u"We only support HTTP(s) and FTP uploads by URL. This is a: {x}".format(x=parsed_url.scheme)
+            msg = "We only support HTTP(s) and FTP uploads by URL. This is a: {x}".format(x=parsed_url.scheme)
             job.add_audit_message(msg)
             file_upload.failed(msg)
             return False
 
-        job.add_audit_message(u"Downloaded {x} as {y}".format(x=file_upload.filename, y=file_upload.local_filename))
+        job.add_audit_message("Downloaded {x} as {y}".format(x=file_upload.filename, y=file_upload.local_filename))
 
         xwalk_name = app.config.get("ARTICLE_CROSSWALKS", {}).get(file_upload.schema)
         xwalk = plugin.load_class(xwalk_name)()
@@ -217,25 +217,25 @@
             with open(path) as handle:
                 xwalk.validate_file(handle)
         except IngestException as e:
-            job.add_audit_message(u"IngestException: {x}".format(x=e.trace()))
+            job.add_audit_message("IngestException: {x}".format(x=e.trace()))
             file_upload.failed(e.message, e.inner_message)
             try:
                 file_failed(path)
             except:
-                job.add_audit_message(u"Error cleaning up file which caused IngestException: {x}".format(x=traceback.format_exc()))
+                job.add_audit_message("Error cleaning up file which caused IngestException: {x}".format(x=traceback.format_exc()))
             return False
         except Exception as e:
-            job.add_audit_message(u"File system error while downloading file: {x}".format(x=traceback.format_exc()))
+            job.add_audit_message("File system error while downloading file: {x}".format(x=traceback.format_exc()))
             file_upload.failed("File system error when downloading file")
             try:
                 file_failed(path)
             except:
-                job.add_audit_message(u"Error cleaning up file which caused Exception: {x}".format(x=traceback.format_exc()))
+                job.add_audit_message("Error cleaning up file which caused Exception: {x}".format(x=traceback.format_exc()))
             return False
 
         # if we get to here then we have a successfully downloaded and validated
         # document, so we can write it to the index
-        job.add_audit_message(u"Validated file as schema {x}".format(x=file_upload.schema))
+        job.add_audit_message("Validated file as schema {x}".format(x=file_upload.schema))
         file_upload.validated(file_upload.schema)
         return True
 
@@ -245,18 +245,18 @@
         path = os.path.join(upload_dir, file_upload.local_filename)
 
         if not os.path.exists(path):
-            job.add_RefactoringTool: Refactored ./portality/tasks/journal_bulk_delete.py
audit_message(u"File not found at path {} . Retrying job later.".format(path))
+            job.add_audit_message("File not found at path {} . Retrying job later.".format(path))
             count = self.get_param(job.params, "attempts")
             retry_limit = app.config.get("HUEY_TASKS", {}).get("ingest_articles", {}).get("retries", 0)
             self.set_param(job.params, "attempts", count + 1)
 
             if retry_limit <= count:
-                job.add_audit_message(u"File still not found at path {} . Giving up.".format(path))
+                job.add_audit_message("File still not found at path {} . Giving up.".format(path))
                 job.fail()
 
             raise RetryException()
 
-        job.add_audit_message(u"Importing from {x}".format(x=path))
+        job.add_audit_message("Importing from {x}".format(x=path))
 
         articleService = DOAJ.articleService()
         account = models.Account.pull(file_upload.owner)
@@ -273,29 +273,29 @@
                     article.set_upload_id(file_upload.id)
                 result = articleService.batch_create_articles(articles, account, add_journal_info=True)
         except IngestException as e:
-            job.add_audit_message(u"IngestException: {msg}. Inner message: {inner}.  Stack: {x}".format(msg=e.message, inner=e.inner_message, x=e.trace()))
+            job.add_audit_message("IngestException: {msg}. Inner message: {inner}.  Stack: {x}".format(msg=e.message, inner=e.inner_message, x=e.trace()))
             file_upload.failed(e.message, e.inner_message)
             result = e.result
             try:
                 file_failed(path)
                 ingest_exception = True
             except:
-                job.add_audit_message(u"Error cleaning up file which caused IngestException: {x}".format(x=traceback.format_exc()))
+                job.add_audit_message("Error cleaning up file which caused IngestException: {x}".format(x=traceback.format_exc()))
         except (DuplicateArticleException, ArticleNotAcceptable) as e:
-            job.add_audit_message(u"One or more articles did not contain either a DOI or a Fulltext URL")
-            file_upload.failed(u"One or more articles did not contain either a DOI or a Fulltext URL")
+            job.add_audit_message("One or more articles did not contain either a DOI or a Fulltext URL")
+            file_upload.failed("One or more articles did not contain either a DOI or a Fulltext URL")
             try:
                 file_failed(path)
             except:
-                job.add_audit_message(u"Error cleaning up file which caused Exception: {x}".format(x=traceback.format_exc()))
+                job.add_audit_message("Error cleaning up file which caused Exception: {x}".format(x=traceback.format_exc()))
                 return
         except Exception as e:
-            job.add_audit_message(u"Unanticipated error: {x}".format(x=traceback.format_exc()))
+            job.add_audit_message("Unanticipated error: {x}".format(x=traceback.format_exc()))
             file_upload.failed("Unanticipated error when importing articles")
             try:
                 file_failed(path)
             except:
-                job.add_audit_message(u"Error cleaning up file which caused Exception: {x}".format(x=traceback.format_exc()))
+                job.add_audit_message("Error cleaning up file which caused Exception: {x}".format(x=traceback.format_exc()))
                 return
 
         success = result.get("success", 0)
@@ -324,7 +324,7 @@
             try:
                 os.remove(path) # just remove the file, no need to keep it
             except Exception as e:
-                job.add_audit_message(u"Error while deleting file {x}: {y}".format(x=path, y=e.message))
+                job.add_audit_message("Error while deleting file {x}: {y}".format(x=path, y=e.message))
 
     def cleanup(self):
         """
--- ./portality/tasks/journal_bulk_delete.py	(original)
+++ ./portality/tasks/journal_bulk_delete.py	(refactored)
@@ -54,17 +54,17 @@
         ids = self.get_param(params, 'ids')
 
   RefactoringTool: Refactored ./portality/tasks/journal_bulk_edit.py
      if not self._job_parameter_check(params):
-            raise BackgroundException(u"{}.run run without sufficient parameters".format(self.__class__.__name__))
+            raise BackgroundException("{}.run run without sufficient parameters".format(self.__class__.__name__))
 
         # repeat the estimations and log what they were at the time the job ran, in addition to what the user saw
         # when requesting the job in journal_bulk_delete_manage
         estimates = self.estimate_delete_counts(json.loads(job.reference['selection_query']))
-        job.add_audit_message(u"About to delete an estimated {} journals with {} articles associated with their ISSNs."
+        job.add_audit_message("About to delete an estimated {} journals with {} articles associated with their ISSNs."
                               .format(estimates['journals-to-be-deleted'], estimates['articles-to-be-deleted']))
 
         journal_delete_q_by_ids = models.Journal.make_query(should_terms={'_id': ids}, consistent_order=False)
         models.Journal.delete_selected(query=journal_delete_q_by_ids, articles=True, snapshot_journals=True, snapshot_articles=True)
-        job.add_audit_message(u"Deleted {} journals and all articles associated with their ISSNs.".format(len(ids)))
+        job.add_audit_message("Deleted {} journals and all articles associated with their ISSNs.".format(len(ids)))
 
     def cleanup(self):
         """
@@ -114,7 +114,7 @@
         cls.set_param(params, 'ids', kwargs['ids'])
 
         if not cls._job_parameter_check(params):
-            raise BackgroundException(u"{}.prepare run without sufficient parameters".format(cls.__name__))
+            raise BackgroundException("{}.prepare run without sufficient parameters".format(cls.__name__))
 
         job.params = params
 
--- ./portality/tasks/journal_bulk_edit.py	(original)
+++ ./portality/tasks/journal_bulk_edit.py	(refactored)
@@ -59,7 +59,7 @@
 
         metadata = cls.get_param(params, "replacement_metadata", "{}")
         metadata = json.loads(metadata)
-        metadata_valid = len(metadata.keys()) > 0
+        metadata_valid = len(list(metadata.keys())) > 0
 
         return ids_valid and (note_valid or metadata_valid)
 
@@ -72,7 +72,7 @@
         params = job.params
 
         if not self._job_parameter_check(params):
-            raise BackgroundException(u"{}.run run without sufficient parameters".format(self.__class__.__name__))
+            raise BackgroundException("{}.run run without sufficient parameters".format(self.__class__.__name__))
 
         # get the parameters for the job
         ids = self.get_param(params, 'ids')
@@ -80,7 +80,7 @@
         metadata = json.loads(self.get_param(params, 'replacement_metadata', "{}"))
 
         # if there is metadata, validate it
-        if (len(metadata.keys()) > 0):
+        if (len(list(metadata.keys())) > 0):
             formdata = MultiDict(metadata)
             fc = formcontext.JournalFormFactory.get_form_context(
                 role="bulk_edit",
@@ -95,7 +95,7 @@
             j = models.Journal.pull(journal_id)
 
             if j is None:
-                job.add_audit_message(u"Journal with id {} does not exist, skipping".format(journal_id))
+                job.add_audit_message("Journal with id {} does not exist, skipping".format(journal_id))
                 continue
 
             fc = formcontext.JournalFormFactory.get_form_context(role="admin", source=j)
@@ -122,13 +122,13 @@
             if "contact_email" in metadata:
                 fc.form.confirm_contact_email.data = metadata["contact_email"]
 
-            for k, v in metadata.iteritems():
-                job.add_audit_message(u"Setting {f} to {x} for journal {y}".format(f=k, x=v, y=journal_id))
+            for k, v in metadata.items():
+                job.add_audit_message("Setting {f} to {x} for journal {y}".format(f=k, x=v, y=journal_id))
                 fc.form[k].data = v
                 updated = True
 
             if note:
-                job.add_audit_message(u"Adding note to for journal {y}".format(y=journal_id))
+RefactoringTool: Refactored ./portality/tasks/journal_csv.py
RefactoringTool: Refactored ./portality/tasks/journal_in_out_doaj.py
                job.add_audit_message("Adding note to for journal {y}".format(y=journal_id))
                 fc.form.notes.append_entry(
                     {'date': datetime.now().strftime(app.config['DEFAULT_DATE_FORMAT']), 'note': note}
                 )
@@ -139,21 +139,21 @@
                     try:
                         fc.finalise()
                     except formcontext.FormContextException as e:
-                        job.add_audit_message(u"Form context exception while bulk editing journal {} :\n{}".format(journal_id, e.message))
+                        job.add_audit_message("Form context exception while bulk editing journal {} :\n{}".format(journal_id, e.message))
                 else:
                     data_submitted = {}
-                    for affected_field_name in fc.form.errors.keys():
+                    for affected_field_name in list(fc.form.errors.keys()):
                         affected_field = getattr(fc.form, affected_field_name,
                                                  ' Field {} does not exist on form. '.format(affected_field_name))
-                        if isinstance(affected_field, basestring):  # ideally this should never happen, an error should not be reported on a field that is not present on the form
+                        if isinstance(affected_field, str):  # ideally this should never happen, an error should not be reported on a field that is not present on the form
                             data_submitted[affected_field_name] = affected_field
                             continue
 
                         data_submitted[affected_field_name] = affected_field.data
                     job.add_audit_message(
-                        u"Data validation failed while bulk editing journal {} :\n"
-                        u"{}\n\n"
-                        u"The data from the fields with the errors is:\n{}".format(
+                        "Data validation failed while bulk editing journal {} :\n"
+                        "{}\n\n"
+                        "The data from the fields with the errors is:\n{}".format(
                             journal_id, json.dumps(fc.form.errors), json.dumps(data_submitted)
                         )
                     )
@@ -208,14 +208,14 @@
         # get the metadata overwrites
         if "replacement_metadata" in kwargs:
             metadata = {}
-            for k, v in kwargs["replacement_metadata"].iteritems():
+            for k, v in kwargs["replacement_metadata"].items():
                 if v is not None and v != "":
                     metadata[k] = v
-            if len(metadata.keys()) > 0:
+            if len(list(metadata.keys())) > 0:
                 cls.set_param(params, 'replacement_metadata', json.dumps(metadata))
 
         if not cls._job_parameter_check(params):
-            raise BackgroundException(u"{}.prepare run without sufficient parameters".format(cls.__name__))
+            raise BackgroundException("{}.prepare run without sufficient parameters".format(cls.__name__))
 
         job.params = params
 
--- ./portality/tasks/journal_csv.py	(original)
+++ ./portality/tasks/journal_csv.py	(refactored)
@@ -22,7 +22,7 @@
         url, action_register = journalService.csv()
         for ar in action_register:
             job.add_audit_message(ar)
-        job.add_audit_message(u"CSV generated; will be served from {y}".format(y=url))
+        job.add_audit_message("CSV generated; will be served from {y}".format(y=url))
 
     def cleanup(self):
         """
--- ./portality/tasks/journal_in_out_doaj.py	(original)
+++ ./portality/tasks/journal_in_out_doaj.py	(refactored)
@@ -49,19 +49,19 @@
         in_doaj = self.get_param(params, "in_doaj")
 
         if journal_ids is None or len(journal_ids) == 0 or in_doaj is None:
-            raise RuntimeError(u"SetInDOAJBackgroundTask.run run without sufficient parameters")
+            raise RuntimeError("SetInDOAJBackgroundTask.run run without sufficient parameters")
 
         for journal_id in journal_ids:
-            job.add_audit_message(u"Setting in_dRefactoringTool: No changes to ./portality/tasks/prune_es_backups.py
RefactoringTool: Refactored ./portality/tasks/public_data_dump.py
oaj to {x} for journal {y}".format(x=str(in_doaj), y=journal_id))
+            job.add_audit_message("Setting in_doaj to {x} for journal {y}".format(x=str(in_doaj), y=journal_id))
 
             j = models.Journal.pull(journal_id)
             if j is None:
-                raise RuntimeError(u"Journal with id {} does not exist".format(journal_id))
+                raise RuntimeError("Journal with id {} does not exist".format(journal_id))
             j.bibjson().active = in_doaj
             j.set_in_doaj(in_doaj)
             j.save()
             j.propagate_in_doaj_status_to_articles()  # will save each article, could take a while
-            job.add_audit_message(u"Journal {x} set in_doaj to {y}, and all associated articles".format(x=journal_id, y=str(in_doaj)))
+            job.add_audit_message("Journal {x} set in_doaj to {y}, and all associated articles".format(x=journal_id, y=str(in_doaj)))
 
     def cleanup(self):
         """
@@ -98,7 +98,7 @@
         cls.set_param(params, "in_doaj", kwargs.get("in_doaj"))
 
         if journal_ids is None or len(journal_ids) == 0 or kwargs.get("in_doaj") is None:
-            raise RuntimeError(u"SetInDOAJBackgroundTask.prepare run without sufficient parameters")
+            raise RuntimeError("SetInDOAJBackgroundTask.prepare run without sufficient parameters")
 
         job.params = params
 
--- ./portality/tasks/public_data_dump.py	(original)
+++ ./portality/tasks/public_data_dump.py	(refactored)
@@ -73,7 +73,7 @@
 
         # Scroll for article and/or journal
         for typ in types:
-            job.add_audit_message(dates.now() + u": Starting export of " + typ)
+            job.add_audit_message(dates.now() + ": Starting export of " + typ)
             job.save()
 
             out_dir = tmpStore.path(container, "doaj_" + typ + "_data_" + day_at_start, create_container=True, must_exist=False)
@@ -126,19 +126,19 @@
             self._prune_container(mainStore, container, day_at_start, types)
             job.save()
 
-        self.background_job.add_audit_message(u"Removing temp store container {x}".format(x=container))
+        self.background_job.add_audit_message("Removing temp store container {x}".format(x=container))
         tmpStore.delete_container(container)
 
         # finally update the cache
         cache.Cache.cache_public_data_dump(urls["article"], sizes["article"], urls["journal"], sizes["journal"])
 
-        job.add_audit_message(dates.now() + u": done")
+        job.add_audit_message(dates.now() + ": done")
 
     def _finish_file(self, storage, container, filename, path, out_file, tarball):
         out_file.write("]")
         out_file.close()
 
-        self.background_job.add_audit_message(u"Adding file {filename} to compressed tar".format(filename=filename))
+        self.background_job.add_audit_message("Adding file {filename} to compressed tar".format(filename=filename))
         tarball.add(path, arcname=filename)
         storage.delete_file(container, filename)
 
@@ -148,7 +148,7 @@
         dn = os.path.dirname(output_file)
         if not os.path.exists(dn):
             os.makedirs(dn)
-        self.background_job.add_audit_message(u"Saving to file {filename}".format(filename=filename))
+        self.background_job.add_audit_message("Saving to file {filename}".format(filename=filename))
 
         out_file = codecs.open(output_file, "wb", "utf-8")
         out_file.write("[")
@@ -163,7 +163,7 @@
     def _copy_on_complete(self, mainStore, tmpStore, container, zipped_path):
         zipped_size = os.path.getsize(zipped_path)
         zipped_name = os.path.basename(zipped_path)
-        self.background_job.add_audit_message(u"Storing from temporary file {0} ({1} bytes) to container {2}".format(zipped_name, zipped_size, container))
+        self.background_job.add_audit_message("Storing from temporary file {0} ({1} bytes) to container {2}".format(zipped_name, zipped_size, container))
         mainStore.store(container, zipped_name, source_path=zipped_path)
         tmpStore.delete_file(container, zipped_name)
         return zipped_sizRefactoringTool: No changes to ./portality/tasks/read_news.py
RefactoringTool: Refactored ./portality/tasks/redis_huey.py
RefactoringTool: Refactored ./portality/tasks/reporting.py
e
@@ -185,13 +185,13 @@
 
         # only proceed if the files for today are present
         if found != len(files_for_today):
-            self.background_job.add_audit_message(u"Files not pruned. One of {0} is missing".format(",".join(files_for_today)))
+            self.background_job.add_audit_message("Files not pruned. One of {0} is missing".format(",".join(files_for_today)))
             return
 
         # go through the container files and remove any that are not today's files
         for container_file in container_files:
             if container_file not in files_for_today:
-                self.background_job.add_audit_message(u"Pruning old file {x} from storage container {y}".format(x=container_file, y=container))
+                self.background_job.add_audit_message("Pruning old file {x} from storage container {y}".format(x=container_file, y=container))
                 mainStore.delete_file(container, container_file)
 
     def cleanup(self):
--- ./portality/tasks/redis_huey.py	(original)
+++ ./portality/tasks/redis_huey.py	(refactored)
@@ -10,7 +10,7 @@
     cfg = app.config.get("HUEY_SCHEDULE", {})
     action_cfg = cfg.get(action)
     if action_cfg is None:
-        raise RuntimeError(u"No configuration for scheduled action '{x}'.  Define this in HUEY_SCHEDULE first then try again.".format(x=action))
+        raise RuntimeError("No configuration for scheduled action '{x}'.  Define this in HUEY_SCHEDULE first then try again.".format(x=action))
 
     return crontab(**action_cfg)
 
@@ -19,6 +19,6 @@
     cfg = app.config.get("HUEY_TASKS", {})
     action_cfg = cfg.get(action)
     if action_cfg is None:
-        raise RuntimeError(u"No task configuration for action '{x}'.  Define this in HUEY_TASKS first then try again.".format(x=action))
+        raise RuntimeError("No task configuration for action '{x}'.  Define this in HUEY_TASKS first then try again.".format(x=action))
     return action_cfg
 
--- ./portality/tasks/reporting.py	(original)
+++ ./portality/tasks/reporting.py	(refactored)
@@ -74,12 +74,12 @@
 
 
 def _tabulate_time_entity_group(group, entityKey):
-    date_keys = group.keys()
+    date_keys = list(group.keys())
     date_keys.sort()
     table = []
     padding = []
     for db in date_keys:
-        users_active_this_period = group[db].keys()
+        users_active_this_period = list(group[db].keys())
         for u in users_active_this_period:
             c = group[db][u]["count"]
             existing = False
@@ -94,7 +94,7 @@
         # have any actions in the current time period we're looping over. E.g.
         # if we're counting edits by month, this would be "users who were active
         # in a previous month but haven't made any edits this month".
-        users_in_table = set(map(lambda each_row: each_row[0], table))
+        users_in_table = set([each_row[0] for each_row in table])
         previously_active_users = users_in_table - set(users_active_this_period)
         for row in table:
             if row[0] in previously_active_users:
@@ -174,7 +174,7 @@
     def _count_down(self, p):
         if p is None:
             return
-        for k in self.report[p].keys():
+        for k in list(self.report[p].keys()):
             self.report[p][k]["count"] = len(self.report[p][k]["ids"])
             del self.report[p][k]["ids"]
 
@@ -252,7 +252,7 @@
     def _count_down(self, p):
         if p is None:
             return
-        for k in self.report[p].keys():
+        for k in list(self.report[p].keys()):
             self.report[p][k]["count"] = len(self.report[p][k]["ids"])
             del self.report[p][k]["ids"]
 
@@ -339,7 +339,7 @@
         self.set_reference(refs, "content_outfiles", cont_outfiles)
         job.reference = refs
 
-        msg = u"Generated reports for period {x} to {y}".format(x=fr, y=to)
+        msg = "Generated reports for period {x} to {y}".format(x=fr, y=to)
         job.add_audit_message(msg)
 
         send_email = self.get_param(params, "email", False)
@@ -367,7 +367,7 @@
         if outdir is not None and os.path.exists(outdir):
    RefactoringTool: No changes to ./portality/tasks/request_es_backup.py
RefactoringTool: Refactored ./portality/tasks/sitemap.py
RefactoringTool: Refactored ./portality/tasks/suggestion_bulk_edit.py
         shutil.rmtree(outdir)
 
-        self.background_job.add_audit_message(u"Deleted directory {x} due to job failure".format(x=outdir))
+        self.background_job.add_audit_message("Deleted directory {x} due to job failure".format(x=outdir))
 
     @classmethod
     def prepare(cls, username, **kwargs):
--- ./portality/tasks/sitemap.py	(original)
+++ ./portality/tasks/sitemap.py	(refactored)
@@ -69,7 +69,7 @@
             counter += 1
 
         # log to the screen
-        job.add_audit_message(u"{x} urls written to sitemap".format(x=counter))
+        job.add_audit_message("{x} urls written to sitemap".format(x=counter))
 
         # save it into the cache directory
         attachment_name = 'doaj_' + datetime.strftime(datetime.now(), '%Y%m%d_%H%M') + '.xml'
--- ./portality/tasks/suggestion_bulk_edit.py	(original)
+++ ./portality/tasks/suggestion_bulk_edit.py	(refactored)
@@ -66,7 +66,7 @@
         application_status = self.get_param(params, 'application_status')
 
         if not self._job_parameter_check(params):
-            raise BackgroundException(u"{}.run run without sufficient parameters".format(self.__class__.__name__))
+            raise BackgroundException("{}.run run without sufficient parameters".format(self.__class__.__name__))
 
         for suggestion_id in ids:
             updated = False
@@ -74,14 +74,14 @@
             s = models.Suggestion.pull(suggestion_id)
 
             if s is None:
-                job.add_audit_message(u"Suggestion with id {} does not exist, skipping".format(suggestion_id))
+                job.add_audit_message("Suggestion with id {} does not exist, skipping".format(suggestion_id))
                 continue
 
             fc = formcontext.ApplicationFormFactory.get_form_context(role="admin", source=s)
 
             if editor_group:
                 job.add_audit_message(
-                    u"Setting editor_group to {x} for suggestion {y}".format(x=str(editor_group), y=suggestion_id))
+                    "Setting editor_group to {x} for suggestion {y}".format(x=str(editor_group), y=suggestion_id))
 
                 # set the editor group
                 f = fc.form.editor_group
@@ -94,7 +94,7 @@
                 updated = True
 
             if note:
-                job.add_audit_message(u"Adding note to for suggestion {y}".format(y=suggestion_id))
+                job.add_audit_message("Adding note to for suggestion {y}".format(y=suggestion_id))
                 fc.form.notes.append_entry(
                     {'date': datetime.now().strftime(app.config['DEFAULT_DATE_FORMAT']), 'note': note}
                 )
@@ -102,7 +102,7 @@
 
             if application_status:
                 job.add_audit_message(
-                    u"Setting application_status to {x} for suggestion {y}".format(x=str(editor_group), y=suggestion_id))
+                    "Setting application_status to {x} for suggestion {y}".format(x=str(editor_group), y=suggestion_id))
                 f = fc.form.application_status
                 f.data = application_status
                 updated = True
@@ -113,21 +113,21 @@
                         fc.finalise()
                     except formcontext.FormContextException as e:
                         job.add_audit_message(
-                            u"Form context exception while bulk editing suggestion {} :\n{}".format(suggestion_id, e.message))
+                            "Form context exception while bulk editing suggestion {} :\n{}".format(suggestion_id, e.message))
                 else:
                     data_submitted = {}
-                    for affected_field_name in fc.form.errors.keys():
+                    for affected_field_name in list(fc.form.errors.keys()):
                         affected_field = getattr(fc.form, affected_field_name,
                                                  ' Field {} does not exist on form. '.format(affected_field_name))
-                        if isinstance(affected_field, basestring):  # ideally this should never happen, an error should not be reported on a field that is not present oRefactoringTool: Refactored ./portality/ui/messages.py
n the form
+                        if isinstance(affected_field, str):  # ideally this should never happen, an error should not be reported on a field that is not present on the form
                             data_submitted[affected_field_name] = affected_field
                             continue
 
                         data_submitted[affected_field_name] = affected_field.data
                     job.add_audit_message(
-                        u"Data validation failed while bulk editing suggestion {} :\n"
-                        u"{}\n\n"
-                        u"The data from the fields with the errors is:\n{}".format(
+                        "Data validation failed while bulk editing suggestion {} :\n"
+                        "{}\n\n"
+                        "The data from the fields with the errors is:\n{}".format(
                             suggestion_id, json.dumps(fc.form.errors), json.dumps(data_submitted)
                         )
                     )
@@ -176,7 +176,7 @@
         cls.set_param(params, 'application_status', kwargs.get('application_status', ''))
 
         if not cls._job_parameter_check(params):
-            raise BackgroundException(u"{}.prepare run without sufficient parameters".format(cls.__name__))
+            raise BackgroundException("{}.prepare run without sufficient parameters".format(cls.__name__))
 
         job.params = params
 
--- ./portality/ui/messages.py	(original)
+++ ./portality/ui/messages.py	(refactored)
@@ -7,52 +7,52 @@
         for review. Click 'Edit this update request' to make further changes.
         """, 'success')
 
-    ARTICLE_METADATA_SUBMITTED_FLASH = (u"Article created/updated", u"success")
-    ARTICLE_METADATA_MERGE_CONFLICT = (u"""Article could not be submitted, as it matches more than one existing article.
-    Please check your metadata, and contact us if you cannot resolve the issue yourself.""", u"error")
+    ARTICLE_METADATA_SUBMITTED_FLASH = ("Article created/updated", "success")
+    ARTICLE_METADATA_MERGE_CONFLICT = ("""Article could not be submitted, as it matches more than one existing article.
+    Please check your metadata, and contact us if you cannot resolve the issue yourself.""", "error")
 
-    SENT_ACCEPTED_APPLICATION_EMAIL = u"""Sent email to '{email}' to tell them that their journal was accepted."""
-    SENT_REJECTED_APPLICATION_EMAIL_TO_OWNER = u"""Sent email to user '{user}' ({name}, {email}) to tell them that their journal application was rejected."""
-    SENT_REJECTED_APPLICATION_EMAIL_TO_SUGGESTER = u"""Sent email to suggester {name} ({email}) to tell them that their journal application was rejected."""
-    SENT_ACCEPTED_UPDATE_REQUEST_EMAIL = u"""Sent email to '{email}' to tell them that their journal update was accepted."""
-    SENT_REJECTED_UPDATE_REQUEST_EMAIL = u"""Sent email to user '{user}' ({name}, {email}) to tell them that their journal update was rejected."""
-    SENT_REJECTED_UPDATE_REQUEST_REVISIONS_REQUIRED_EMAIL = u"""Sent email to user '{user}' to tell them that their journal update requires revisions.  You will need to contact them separately with details."""
-    SENT_JOURNAL_CONTACT_ACCEPTED_APPLICATION_EMAIL = u"""Sent email to journal contact '{email}' to tell them their journal was accepted."""
-    SENT_JOURNAL_CONTACT_ACCEPTED_UPDATE_REQUEST_EMAIL = u"""Sent email to journal contact '{email}' to tell that an update to their journal was accepted."""
-    SENT_JOURNAL_CONTACT_IN_PROGRESS_EMAIL = u"""An email has been sent to the Journal Contact alerting them that you are working on their application."""
-    SENT_JOURNAL_CONTACT_ASSIGNED_EMAIL = u"""An email has been sent to the Journal Contact alerting them that an editor has been assigned to their application."""
-    SENT_PUBLISHER_IN_PROGRESS_EMAIL = u"""An email has been sent to the Owner alerting them that you are working on their application."""
-    SENT_PUBLISHER_ASSIGNED_EMAIL = u"""An email has been sent to the Owner alerting them that an editor has been assigned to their application."""
+    SENT_ACCEPTED_APPLICATION_EMAIL = """Sent email to '{email}' to tell them that their journal was accepted."""
+    SENT_REJECTED_APPLICATION_EMAIL_TO_OWNER = """Sent email to user '{user}' ({name}, {email}) to tell them that their journal application was rejected."""
+    SENT_REJECTED_APPLICATION_EMAIL_TO_SUGGESTER = """Sent email to suggester {name} ({email}) to tell them that their journal application was rejected."""
+    SENT_ACCEPTED_UPDATE_REQUEST_EMAIL = """Sent email to '{email}' to tell them that their journal update was accepted."""
+    SENT_REJECTED_UPDATE_REQUEST_EMAIL = """Sent email to user '{user}' ({name}, {email}) to tell them that their journal update was rejected."""
+    SENT_REJECTED_UPDATE_REQUEST_REVISIONS_REQUIRED_EMAIL = """Sent email to user '{user}' to tell them that their journal update requires revisions.  You will need to contact them separately with details."""
+    SENT_JOURNAL_CONTACT_ACCEPTED_APPLICATION_EMAIL = """Sent email to journal contact '{email}' to tell them their journal was accepted."""
+    SENT_JOURNAL_CONTACT_ACCEPTED_UPDATE_REQUEST_EMAIL = """Sent email to journal contact '{email}' to tell that an update to their journal was accepted."""
+    SENT_JOURNAL_CONTACT_IN_PROGRESS_EMAIL = """An email has been sent to the Journal Contact alerting them that you are working on their application."""
+    SENT_JOURNAL_CONTACT_ASSIGNED_EMAIL = """An email has been sent to the Journal Contact alerting them that an editor has been assigned to their application."""
+    SENT_PUBLISHER_IN_PROGRESS_EMAIL = """An email has been sent to the Owner alerting them that you are working on their application."""
+    SENT_PUBLISHER_ASSIGNED_EMAIL = """An email has been sent to the Owner alerting them that an editor has been assigned to their application."""
 
-    NOT_SENT_ACCEPTED_APPLICATION_EMAIL = u"""Did not send email to '{email}' to tell them that their journal was accepted.  Email may be disabled, or there is a problem with the email address."""
-    NOT_SENT_REJECTED_APPLICATION_EMAILS = u"""Did not send email to user '{user}' or application suggester to tell them that their journal was rejected  Email may be disabled, or there is a problem with the email address."""
-    NOT_SENT_ACCEPTED_UPDATE_REQUEST_EMAIL = u"""Did not send email to '{email}' to tell them that their update was accepted  Email may be disabled, or there is a problem with the email address."""
-    NOT_SENT_REJECTED_UPDATE_REQUEST_EMAIL = u"""Did not send email to user '{user}' to tell them that their update was rejected. Email may be disabled, or there is a problem with the email address"""
-    NOT_SENT_REJECTED_UPDATE_REQUEST_REVISIONS_REQUIRED_EMAIL = u"""Did not send email to user '{user}' to tell them that their update required revisions. Email may be disabled, or there is a problem with the email address"""
-    NOT_SENT_JOURNAL_CONTACT_ACCEPTED_APPLICATION_EMAIL = u"""Did not send email to '{email}' to tell them that their application/update request was accepted. Email may be disabled, or there is a problem with the email address"""
-    NOT_SENT_JOURNAL_CONTACT_IN_PROGRESS_EMAIL = u"""An email could not be sent to the Journal Contact alerting them that you are working on their application. Email may be disabled, or there is a problem with the email address"""
-    NOT_SENT_JOURNAL_CONTACT_ASSIGNED_EMAIL = u"""An email could not be sent to the Journal Contact alerting them that an editor has been assigned to their application. Email may be disabled, or there is a problem with the email address"""
-    NOT_SENT_PUBLISHER_IN_PROGRESS_EMAIL = u"""An email could not be sent to the Owner alerting them that you are working on their application. Email may be disabled, or there is a problem with the email address. """
-    NOT_SENT_PUBLISHER_ASSIGNED_EMAIL = u"""An email could not be sent to the Owner alerting them that an editor has been assigned to their application. Email may be disabled, or there is a problem with the email address"""
+    NOT_SENT_ACCEPTED_APPLICATION_EMAIL = """Did not send email to '{email}' to tell them that their journal was accepted.  Email may be disabled, or there is a problem with the email address."""
+    NOT_SENT_REJECTED_APPLICATION_EMAILS = """Did not send email to user '{user}' or application suggester to tell them that their journal was rejected  Email may be disabled, or there is a problem with the email address."""
+    NOT_SENT_ACCEPTED_UPDATE_REQUEST_EMAIL = """Did not send email to '{email}' to tell them that their update was accepted  Email may be disabled, or there is a problem with the email address."""
+    NOT_SENT_REJECTED_UPDATE_REQUEST_EMAIL = """Did not send email to user '{user}' to tell them that their update was rejected. Email may be disabled, or there is a problem with the email address"""
+    NOT_SENT_REJECTED_UPDATE_REQUEST_REVISIONS_REQUIRED_EMAIL = """Did not send email to user '{user}' to tell them that their update required revisions. Email may be disabled, or there is a problem with the email address"""
+    NOT_SENT_JOURNAL_CONTACT_ACCEPTED_APPLICATION_EMAIL = """Did not send email to '{email}' to tell them that their application/update request was accepted. Email may be disabled, or there is a problem with the email address"""
+    NOT_SENT_JOURNAL_CONTACT_IN_PROGRESS_EMAIL = """An email could not be sent to the Journal Contact alerting them that you are working on their application. Email may be disabled, or there is a problem with the email address"""
+    NOT_SENT_JOURNAL_CONTACT_ASSIGNED_EMAIL = """An email could not be sent to the Journal Contact alerting them that an editor has been assigned to their application. Email may be disabled, or there is a problem with the email address"""
+    NOT_SENT_PUBLISHER_IN_PROGRESS_EMAIL = """An email could not be sent to the Owner alerting them that you are working on their application. Email may be disabled, or there is a problem with the email address. """
+    NOT_SENT_PUBLISHER_ASSIGNED_EMAIL = """An email could not be sent to the Owner alerting them that an editor has been assigned to their application. Email may be disabled, or there is a problem with the email address"""
 
-    IN_PROGRESS_NOT_SENT_EMAIL_DISABLED = u"""Did not send email to Owner or Journal Contact about the status change, as publisher emails are disabled."""
+    IN_PROGRESS_NOT_SENT_EMAIL_DISABLED = """Did not send email to Owner or Journal Contact about the status change, as publisher emails are disabled."""
 
     DIFF_TABLE_NOT_PRESENT = """-- Not held in journal metadata --"""
 
-    REJECT_NOTE_WRAPPER = u"""{editor}: This application was rejected with the reason '{note}'"""
+    REJECT_NOTE_WRAPPER = """{editor}: This application was rejected with the reason '{note}'"""
 
-    EXCEPTION_ARTICLE_BATCH_DUPLICATE = u"One or more articles in this batch have duplicate identifiers"
-    EXCEPTION_ARTICLE_BATCH_FAIL = u"One or more articles failed to ingest; entire batch ingest halted"
-    EXCEPTION_ARTICLE_BATCH_CONFLICT = u"One or more articles in this batch matched multiple articles as duplicates; entire batch ingest halted"
-    EXCEPTION_DETECT_DUPLICATE_NO_ID = u"The article you provided has neither doi nor fulltext url, and as a result cannot be deduplicated"
-    EXCEPTION_ARTICLE_MERGE_CONFLICT = u"The article matched multiple existing articles as duplicates, and we cannot tell which one to update"
-    EXCEPTION_NO_DOI_NO_FULLTEXT = u"The article must have a DOI and/or a Full-Text URL"
+    EXCEPTION_ARTICLE_BATCH_DUPLICATE = "One or more articles in this batch have duplicate identifiers"
+    EXCEPTION_ARTICLE_BATCH_FAIL = "One or more articles failed to ingest; entire batch ingest halted"
+    EXCEPTION_ARTICLE_BATCH_CONFLICT = "One or more articles in this batch matched multiple articles as duplicates; entire batch ingest halted"
+    EXCEPTION_DETECT_DUPLICATE_NO_ID = "The article you provided has neither doi nor fulltext url, and as a result cannot be deduplicated"
+    EXCEPTION_ARTICLE_MERGE_CONFLICT = "The article matched multiple existing articles as duplicates, and we cannot tell which one to update"
+    EXCEPTION_NO_DOI_NO_FULLTEXT = "The article must have a DRefactoringTool: Refactored ./portality/view/account.py
RefactoringTool: Refactored ./portality/view/admin.py
RefactoringTool: No changes to ./portality/view/api_v1.py
RefactoringTool: Refactored ./portality/view/atom.py
RefactoringTool: Refactored ./portality/view/doaj.py
RefactoringTool: Refactored ./portality/view/doajservices.py
OI and/or a Full-Text URL"
 
-    PREVENT_DEEP_PAGING_IN_API = u"""You cannot access results beyond {max_records} records via this API.
+    PREVENT_DEEP_PAGING_IN_API = """You cannot access results beyond {max_records} records via this API.
     If you would like to see more results, you can download all of our data from
     {data_dump_url}. You can also harvest from our OAI-PMH endpoints; articles: {oai_article_url}, journals: {oai_journal_url}"""
 
-    CONSENT_COOKIE_VALUE = u"""By using the DOAJ website you have agreed to our cookie policy."""
+    CONSENT_COOKIE_VALUE = """By using the DOAJ website you have agreed to our cookie policy."""
 
 
     @classmethod
--- ./portality/view/account.py	(original)
+++ ./portality/view/account.py	(refactored)
@@ -54,7 +54,7 @@
                 acc = models.Account.pull(newdata['id'])
         if request.values.get('submit', False) == 'Generate':
             acc.generate_api_key()
-        for k, v in newdata.items():
+        for k, v in list(newdata.items()):
             if k not in ['marketing_consent', 'submit','password', 'role', 'confirm', 'reset_token', 'reset_expires', 'last_updated', 'created_date', 'id']:
                 acc.data[k] = v
         if 'password' in newdata and not newdata['password'].startswith('sha1'):
--- ./portality/view/admin.py	(original)
+++ ./portality/view/admin.py	(refactored)
@@ -100,7 +100,7 @@
         try:
             query = json.loads(request.values.get("q"))
         except:
-            print request.values.get("q")
+            print(request.values.get("q"))
             abort(400)
         total = models.Article.hit_count(query, consistent_order=False)
         resp = make_response(json.dumps({"total" : total}))
--- ./portality/view/atom.py	(original)
+++ ./portality/view/atom.py	(refactored)
@@ -186,7 +186,7 @@
         dr = datetime.strftime(self.last_updated, "%Y-%m-%dT%H:%M:%SZ")
         updated.text = dr
         
-        entry_dates = self.entries.keys()
+        entry_dates = list(self.entries.keys())
         entry_dates.sort(reverse=True)
         for ed in entry_dates:
             es = self.entries.get(ed)
--- ./portality/view/doaj.py	(original)
+++ ./portality/view/doaj.py	(refactored)
@@ -1,7 +1,7 @@
 from flask import Blueprint, request, flash, make_response
 from flask import render_template, abort, redirect, url_for, send_file, jsonify
 from flask_login import current_user, login_required
-import urllib
+import urllib.request, urllib.parse, urllib.error
 from jinja2.exceptions import TemplateNotFound
 
 from portality import dao
@@ -119,7 +119,7 @@
     if ref is None:
         abort(400)                                                                                # Referrer is required
     else:
-        return redirect(url_for('.search') + '?source=' + urllib.quote(json.dumps(query)) + "&ref=" + urllib.quote(ref))
+        return redirect(url_for('.search') + '?source=' + urllib.parse.quote(json.dumps(query)) + "&ref=" + urllib.parse.quote(ref))
 
 
 @blueprint.route("/subjects")
--- ./portality/view/doajservices.py	(original)
+++ ./portality/view/doajservices.py	(refactored)
@@ -1,4 +1,4 @@
-import json, urllib, requests
+import json, urllib.request, urllib.parse, urllib.error, requests
 
 from flask import Blueprint, make_response, request, abort
 from flask_login import current_user, login_required
@@ -49,7 +49,7 @@
         q = d['query']
 
         # re-serialise the query, and url encode it
-        source = urllib.quote(json.dumps(q))
+        source = urllib.parse.quote(json.dumps(q))
 
         # assemble the DOAJ url
         doajurl = p + "?source=" + source
@@ -58,7 +58,7 @@
         # query arguments, so by this point it is double-encoded
         bitly = app.config.get("BITLY_SHORTENING_API_URL")
         bitly_oauth = app.config.get("BITLY_OAUTH_TOKEN")
-        bitlyurl = bitly + "?access_token=" + bitly_oauth + "&longUrl=" + urllib.quote(doajurl)
+        bitlyurl = bitly + "?access_token=" + bitly_oauth + "&longUrl=" + urllib.parse.quote(doajurl)
 
         # make the request
         resp =RefactoringTool: No changes to ./portality/view/editor.py
RefactoringTool: No changes to ./portality/view/forms.py
RefactoringTool: Refactored ./portality/view/oaipmh.py
RefactoringTool: Refactored ./portality/view/openurl.py
RefactoringTool: Refactored ./portality/view/publisher.py
RefactoringTool: Refactored ./portality/view/query.py
RefactoringTool: Refactored ./portality/view/status.py
 requests.get(bitlyurl)
--- ./portality/view/oaipmh.py	(original)
+++ ./portality/view/oaipmh.py	(refactored)
@@ -300,9 +300,9 @@
                              (0xBFFFE, 0xBFFFF), (0xCFFFE, 0xCFFFF),
                              (0xDFFFE, 0xDFFFF), (0xEFFFE, 0xEFFFF),
                              (0xFFFFE, 0xFFFFF), (0x10FFFE, 0x10FFFF)])
-_illegal_ranges = ["%s-%s" % (unichr(low), unichr(high))
+_illegal_ranges = ["%s-%s" % (chr(low), chr(high))
                    for (low, high) in _illegal_unichrs]
-_illegal_xml_chars_RE = re.compile(u'[%s]' % u''.join(_illegal_ranges))
+_illegal_xml_chars_RE = re.compile('[%s]' % ''.join(_illegal_ranges))
 
 
 def valid_XML_char_ordinal(i):
--- ./portality/view/openurl.py	(original)
+++ ./portality/view/openurl.py	(refactored)
@@ -3,7 +3,7 @@
 from portality.models import OpenURLRequest
 from portality.lib import analytics
 from portality.core import app
-from urllib import unquote
+from urllib.parse import unquote
 
 blueprint = Blueprint('openurl', __name__)
 
@@ -57,7 +57,7 @@
     rem_ns = lambda x: re.sub('rft.', '', x)
 
     # Pack the list of parameters into a dictionary, while un-escaping the string.
-    dict_params = {rem_ns(key): value for (key, value) in req.values.iteritems()}
+    dict_params = {rem_ns(key): value for (key, value) in req.values.items()}
 
     # Create an object to represent this OpenURL request.
     try:
@@ -83,7 +83,7 @@
     sub_title = lambda x: re.sub('^title', 'jtitle', x)
 
     # Add referent tags to each parameter, and change title tag using above function
-    rewritten_params = {"rft." + sub_title(key): value for (key, value) in req.values.iteritems()}
+    rewritten_params = {"rft." + sub_title(key): value for (key, value) in req.values.items()}
 
     # Add the rewritten parameters to the meta params
     params.update(rewritten_params)
--- ./portality/view/publisher.py	(original)
+++ ./portality/view/publisher.py	(refactored)
@@ -193,7 +193,7 @@
         # the user might request by pressing the add/remove authors buttons
         more_authors = request.values.get("more_authors")
         remove_author = None
-        for v in request.values.keys():
+        for v in list(request.values.keys()):
             if v.startswith("remove_authors"):
                 remove_author = v.split("-")[1]
         
--- ./portality/view/query.py	(original)
+++ ./portality/view/query.py	(refactored)
@@ -1,4 +1,4 @@
-import json, urllib2
+import json, urllib.request, urllib.error, urllib.parse
 
 from flask import Blueprint, request, abort, make_response
 from flask_login import current_user
@@ -31,7 +31,7 @@
         q = request.json
     # if there is a source param, load the json from it
     elif 'source' in request.values:
-        q = json.loads(urllib2.unquote(request.values['source']))
+        q = json.loads(urllib.parse.unquote(request.values['source']))
 
     try:
         account = None
--- ./portality/view/status.py	(original)
+++ ./portality/view/status.py	(refactored)
@@ -104,7 +104,7 @@
     #res['notes'].append(memory_note)
     
     # check that all necessary ES nodes can actually be pinged from this machine
-    for eddr in [app.config['ELASTIC_SEARCH_HOST']] if isinstance(app.config['ELASTIC_SEARCH_HOST'], basestring) else app.config['ELASTIC_SEARCH_HOST']:
+    for eddr in [app.config['ELASTIC_SEARCH_HOST']] if isinstance(app.config['ELASTIC_SEARCH_HOST'], str) else app.config['ELASTIC_SEARCH_HOST']:
         if not eddr.startswith('http'): eddr = 'http://' + eddr
         if not eddr.endswith(':9200'): eddr += ':9200'
         r = requests.get(eddr)
@@ -117,13 +117,13 @@
     res['notes'].append(es_note)
         
     # query ES for cluster health and nodes up
-    es_addr = str(app.config['ELASTIC_SEARCH_HOST'][0] if not isinstance(app.config['ELASTIC_SEARCH_HOST'], basestring) else app.config['ELASTIC_SEARCH_HOST']).rstrip('/')
+    es_addr = str(app.config['ELASTIC_SEARCH_HOST'][0] if not isinstance(app.config['ELASTIC_SEARCH_HOST'], str) else app.config['ELASTIC_SEARCH_HOST']).rstrip('/')
     if not es_addr.startswith('httpRefactoringTool: Refactored ./scratchpad/duplicate_articles/duplicate_articles.py
RefactoringTool: No changes to ./scratchpad/issue171/delete_articles.py
RefactoringTool: Refactored ./scratchpad/issue239/date_histogram.py
RefactoringTool: No changes to ./scratchpad/issue277/dump_articles.py
RefactoringTool: Refactored ./scratchpad/issue290/orphan_issns.py
'): es_addr = 'http://' + es_addr
     if not es_addr.endswith(':9200'): es_addr += ':9200'
     try:
         es = requests.get(es_addr + '/_status').json()
         res['index'] = { 'cluster': {}, 'shards': { 'total': es['_shards']['total'], 'successful': es['_shards']['successful'] }, 'indices': {} }
-        for k, v in es['indices'].iteritems():
+        for k, v in es['indices'].items():
             res['index']['indices'][k] = { 'docs': v['docs']['num_docs'], 'size': int(math.ceil(v['index']['primary_size_in_bytes']) / 1024 / 1024) }
         try:
             ces = requests.get(es_addr + '/_cluster/health')
--- ./scratchpad/duplicate_articles/duplicate_articles.py	(original)
+++ ./scratchpad/duplicate_articles/duplicate_articles.py	(refactored)
@@ -30,7 +30,7 @@
         with codecs.open(IN, "rb", "utf-8") as f:
             reader = clcsv.UnicodeReader(f)
 
-            headers = reader.next()
+            headers = next(reader)
             awriter.writerow(headers)
             bwriter.writerow(headers)
 
@@ -76,14 +76,14 @@
 
 
 
-print("Total articles engaged in duplication: " + str(len(unique_ids)))
-print("Total articles that would remain after de-duplication: " + str(len(unique_deduplicated)))
+print(("Total articles engaged in duplication: " + str(len(unique_ids))))
+print(("Total articles that would remain after de-duplication: " + str(len(unique_deduplicated))))
 
-print("Total estimated genuine duplication pairs: " + str(genuine_count))
-print("Total estimated articles engaged in genuine duplication: " + str(len(genuine_unique_ids)))
-print("Total estimated articles that would remain from genuine duplication after de-duplication: " + str(len(genuine_unique_deduplicated)))
+print(("Total estimated genuine duplication pairs: " + str(genuine_count)))
+print(("Total estimated articles engaged in genuine duplication: " + str(len(genuine_unique_ids))))
+print(("Total estimated articles that would remain from genuine duplication after de-duplication: " + str(len(genuine_unique_deduplicated))))
 
-print("Total estimated bad data duplication pairs: " + str(bad_data_count))
-print("Total estimated articles engaged in 'bad data' duplication: " + str(len(bad_data_unique_ids)))
-print("Total estimated articles that would remain from 'bad data' duplication after de-duplication: " + str(len(bad_data_unique_deduplicated)))
+print(("Total estimated bad data duplication pairs: " + str(bad_data_count)))
+print(("Total estimated articles engaged in 'bad data' duplication: " + str(len(bad_data_unique_ids))))
+print(("Total estimated articles that would remain from 'bad data' duplication after de-duplication: " + str(len(bad_data_unique_deduplicated))))
 
--- ./scratchpad/issue239/date_histogram.py	(original)
+++ ./scratchpad/issue239/date_histogram.py	(refactored)
@@ -1,6 +1,6 @@
 from portality import models
 from datetime import datetime
-import csv, requests, urllib, json
+import csv, requests, urllib.request, urllib.parse, urllib.error, json
 
 query = {
     "query" : { "bool" : {"must" : [{"term" : {"_type" : "article"}}]}},
@@ -22,7 +22,7 @@
 }
 
 base_url = "http://doaj.org/query/journal,article/_search"
-query_url = base_url + "?source=" + urllib.quote_plus(json.dumps(query))
+query_url = base_url + "?source=" + urllib.parse.quote_plus(json.dumps(query))
 resp = requests.get(query_url)
 j = resp.json()
 
--- ./scratchpad/issue290/orphan_issns.py	(original)
+++ ./scratchpad/issue290/orphan_issns.py	(refactored)
@@ -19,7 +19,7 @@
 j = resp.json()
 jissns = [t.get("term") for t in j.get("facets", {}).get("issns", {}).get("terms", [])]
 
-print "Journal ISSNs", len(jissns)
+print("Journal ISSNs", len(jissns))
 
 article_issn_query = {
     "query" : {"term" : {"_type" : "article"}},
@@ -37,12 +37,12 @@
 j = resp.json()
 aissns = [t.get("term") for t in j.get("facets", {}).get("issns", {}).get("terms", [])]
 
-print "Article ISSNs", len(aissns)
+print("Article ISSNs", len(aissns))
 
 missing = [issn for issn in aissns if issn not in jissns]
 
-print "Orphaned", len(missing)
-print missing
+print("Orphaned", len(missiRefactoringTool: Can't parse ./scratchpad/rate_limit/rate_test.py: ParseError: bad input: type=3, value=u"'https://doaj.org/api/v1/search/journals/issn:{issn}'", context=('', (14, 25))
RefactoringTool: Can't parse ./scratchpad/rate_limit/rate_test2.py: ParseError: bad input: type=3, value=u"'codes_555.csv'", context=('', (10, 11))
RefactoringTool: No changes to ./scratchpad/sync/sync_from_remote.py
RefactoringTool: Files that were modified:
RefactoringTool: ./setup.py
RefactoringTool: ./deploy/doaj_gunicorn_config.py
RefactoringTool: ./deploy/doaj_test_gunicorn_config.py
RefactoringTool: ./deploy/lambda/alert_backups_missing.py
RefactoringTool: ./deploy/new_infrastructure_migration_scripts/copy_data_from_live_to_new.py
RefactoringTool: ./deploy_old/doaj_gunicorn_config.py
RefactoringTool: ./deploy_old/doaj_test_gunicorn_config.py
RefactoringTool: ./deploy_old/esbackup.py
RefactoringTool: ./deploy_old/mount_s3fs.py
RefactoringTool: ./deploy_old/lambda/alert_backups_missing.py
RefactoringTool: ./doajtest/bootstrap.py
RefactoringTool: ./doajtest/clonelive.py
RefactoringTool: ./doajtest/helpers.py
RefactoringTool: ./doajtest/make_matrix.py
RefactoringTool: ./doajtest/demo_scripts/async_workflow_notifications_demo.py
RefactoringTool: ./doajtest/fixtures/__init__.py
RefactoringTool: ./doajtest/fixtures/accounts.py
RefactoringTool: ./doajtest/fixtures/applications.py
RefactoringTool: ./doajtest/fixtures/article.py
RefactoringTool: ./doajtest/fixtures/background.py
RefactoringTool: ./doajtest/fixtures/bibjson.py
RefactoringTool: ./doajtest/fixtures/common.py
RefactoringTool: ./doajtest/fixtures/dois.py
RefactoringTool: ./doajtest/fixtures/editors.py
RefactoringTool: ./doajtest/fixtures/issns.py
RefactoringTool: ./doajtest/fixtures/journals.py
RefactoringTool: ./doajtest/fixtures/provenance.py
RefactoringTool: ./doajtest/fixtures/snapshots.py
RefactoringTool: ./doajtest/functional/bulk_api.py
RefactoringTool: ./doajtest/functional/oaipmh_client_test.py
RefactoringTool: ./doajtest/functional/stress_test.py
RefactoringTool: ./doajtest/mocks/bll_article.py
RefactoringTool: ./doajtest/mocks/model_Article.py
RefactoringTool: ./doajtest/mocks/model_Cache.py
RefactoringTool: ./doajtest/mocks/model_Journal.py
RefactoringTool: ./doajtest/mocks/models_Cache.py
RefactoringTool: ./doajtest/mocks/store.py
RefactoringTool: ./doajtest/unit/test_anon.py
RefactoringTool: ./doajtest/unit/test_api_account.py
RefactoringTool: ./doajtest/unit/test_api_bulk_application.py
RefactoringTool: ./doajtest/unit/test_api_bulk_article.py
RefactoringTool: ./doajtest/unit/test_api_crud_application.py
RefactoringTool: ./doajtest/unit/test_api_crud_article.py
RefactoringTool: ./doajtest/unit/test_api_crud_journal.py
RefactoringTool: ./doajtest/unit/test_api_crud_returnvalues.py
RefactoringTool: ./doajtest/unit/test_api_dataobj.py
RefactoringTool: ./doajtest/unit/test_api_dataobj_cast_functions.py
RefactoringTool: ./doajtest/unit/test_api_discovery.py
RefactoringTool: ./doajtest/unit/test_api_errors.py
RefactoringTool: ./doajtest/unit/test_article_cleanup_sync.py
RefactoringTool: ./doajtest/unit/test_article_match.py
RefactoringTool: ./doajtest/unit/test_background.py
RefactoringTool: ./doajtest/unit/test_bll_accept_application.py
RefactoringTool: ./doajtest/unit/test_bll_article_batch_create_article.py
RefactoringTool: ./doajtest/unit/test_bll_article_create_article.py
RefactoringTool: ./doajtest/unit/test_bll_article_discover_duplicates.py
RefactoringTool: ./doajtest/unit/test_bll_article_get_duplicates.py
RefactoringTool: ./doajtest/unit/test_bll_article_is_legitimate_owner.py
RefactoringTool: ./doajtest/unit/test_bll_article_issn_ownership_status.py
RefactoringTool: ./doajtest/unit/test_bll_authorisations.py
RefactoringTool: ./doajtest/unit/test_bll_delete_application.py
RefactoringTool: ./doajtest/unit/test_bll_getters.py
RefactoringTool: ./doajtest/unit/test_bll_journal_csv.py
RefactoringTool: ./doajtest/unit/test_bll_object_conversions.py
RefactoringTool: ./doajtest/unit/test_bll_reject_application.py
RefactoringTool: ./doajtest/unit/test_bll_update_request.py
RefactoringTool: ./doajtest/unit/test_contact.py
RefactoringTool: ./doajtest/unit/test_crosswalks_journal2questions.py
RefactoringTool: ./doajtest/unit/test_csv_wrapper.py
RefactoringTool: ./doajtest/unit/test_datasets.py
RefactoringTool: ./doajtest/unit/test_duplicate_report_script.py
RefactoringTool: ./doajtest/unit/test_es_snapshots_client.py
RefactoringTool: ./doajtest/unit/test_fc_assed_app_review.py
RefactoringTool: ./doajtest/unit/test_fc_assed_journal_review.py
RefactoringTool: ./doajtest/unit/test_fc_editor_app_review.py
RefactoringTool: ./doajtest/unit/test_fc_editor_journal_review.py
RefactoringTool: ./doajtest/unit/test_fc_maned_app_review.py
RefactoringTool: ./doajtest/unit/test_fc_maned_journal_review.py
RefactoringTool: ./doajtest/unit/test_fc_publisher_update_request.py
RefactoringTool: ./doajtest/unit/test_fc_readonly_journal.py
RefactoringTool: ./doajtest/unit/test_feed.py
RefactoringTool: ./doajtest/unit/test_formcontext.py
RefactoringTool: ./doajtest/unit/test_formcontext_emails.py
RefactoringTool: ./doajtest/unit/test_formrender.py
RefactoringTool: ./doajtest/unit/test_index_searchbox.py
RefactoringTool: ./doajtest/unit/test_jinja_template_filters.py
RefactoringTool: ./doajtest/unit/test_lib_normalise_doi.py
RefactoringTool: ./doajtest/unit/test_lib_normalise_url.py
RefactoringTool: ./doajtest/unit/test_lock.py
RefactoringTool: ./doajtest/unit/test_models.py
RefactoringTool: ./doajtest/unit/test_oaipmh.py
RefactoringTool: ./doajtest/unit/test_prune_marvel.py
RefactoringTool: ./doajtest/unit/test_query.py
RefactoringTool: ./doajtest/unit/test_query_filters.py
RefactoringTool: ./doajtest/unit/test_regexes.py
RefactoringTool: ./doajtest/unit/test_reporting.py
RefactoringTool: ./doajtest/unit/test_reserved_usernames.py
RefactoringTool: ./doajtest/unit/test_scripts_accounts_with_marketing_consent.py
RefactoringTool: ./doajtest/unit/test_sitemap.py
RefactoringTool: ./doajtest/unit/test_snapshot.py
RefactoringTool: ./doajtest/unit/test_snapshot_tasks.py
RefactoringTool: ./doajtest/unit/test_task_article_bulk_delete.py
RefactoringTool: ./doajtest/unit/test_task_journal_bulk_delete.py
RefactoringTool: ./doajtest/unit/test_task_journal_bulkedit.py
RefactoringTool: ./doajtest/unit/test_task_suggestion_bulkedit.py
RefactoringTool: ./doajtest/unit/test_tasks_ingestarticles.py
RefactoringTool: ./doajtest/unit/test_tasks_public_data_dump.py
RefactoringTool: ./doajtest/unit/test_tick.py
RefactoringTool: ./doajtest/unit/test_toc.py
RefactoringTool: ./doajtest/unit/test_util.py
RefactoringTool: ./doajtest/unit/test_withdraw_reinstate.py
RefactoringTool: ./doajtest/unit/test_workflow_emails.py
RefactoringTool: ./doajtest/unit/test_xwalk.py
RefactoringTool: ./portality/app.py
RefactoringTool: ./portality/app_email.py
RefactoringTool: ./portality/authorise.py
RefactoringTool: ./portality/background.py
RefactoringTool: ./portality/blog.py
RefactoringTool: ./portality/clcsv.py
RefactoringTool: ./portality/constants.py
RefactoringTool: ./portality/core.py
RefactoringTool: ./portality/dao.py
RefactoringTool: ./portality/datasets.py
RefactoringTool: ./portality/decorators.py
RefactoringTool: ./portality/error_handler.py
RefactoringTool: ./portality/lcc.py
RefactoringTool: ./portality/lock.py
RefactoringTool: ./portality/ordereddict.py
RefactoringTool: ./portality/regex.py
RefactoringTool: ./portality/settings.py
RefactoringTool: ./portality/store.py
RefactoringTool: ./portality/upgrade.py
RefactoringTool: ./portality/util.py
RefactoringTool: ./portality/api/v1/common.py
RefactoringTool: ./portality/api/v1/discovery.py
RefactoringTool: ./portality/api/v1/bulk/applications.py
RefactoringTool: ./portality/api/v1/bulk/articles.py
RefactoringTool: ./portality/api/v1/crud/applications.py
RefactoringTool: ./portality/api/v1/crud/articles.py
RefactoringTool: ./portality/api/v1/crud/journals.py
RefactoringTool: ./portality/api/v1/data_objects/application.py
RefactoringTool: ./portality/api/v1/data_objects/article.py
RefactoringTool: ./portality/api/v1/data_objects/common_journal_application.py
RefactoringTool: ./portality/api/v1/data_objects/journal.py
RefactoringTool: ./portality/bll/doaj.py
RefactoringTool: ./portality/bll/exceptions.py
RefactoringTool: ./portality/bll/services/application.py
RefactoringTool: ./portality/bll/services/article.py
RefactoringTool: ./portality/bll/services/authorisation.py
RefactoringTool: ./portality/bll/services/journal.py
RefactoringTool: ./portality/bll/services/query.py
RefactoringTool: ./portality/crosswalks/article_doaj_xml.py
RefactoringTool: ./portality/crosswalks/article_form.py
RefactoringTool: ./portality/crosswalks/exceptions.py
RefactoringTool: ./portality/crosswalks/journal_questions.py
RefactoringTool: ./portality/formcontext/choices.py
RefactoringTool: ./portality/formcontext/emails.py
RefactoringTool: ./portality/formcontext/fields.py
RefactoringTool: ./portality/formcontext/formcontext.py
RefactoringTool: ./portality/formcontext/formhelper.py
RefactoringTool: ./portality/formcontext/forms.py
RefactoringTool: ./portality/formcontext/render.py
RefactoringTool: ./portality/formcontext/validate.py
RefactoringTool: ./portality/formcontext/xwalk.py
RefactoringTool: ./portality/lib/analytics.py
RefactoringTool: ./portality/lib/anon.py
RefactoringTool: ./portality/lib/argvalidate.py
RefactoringTool: ./portality/lib/dataobj.py
RefactoringTool: ./portality/lib/dates.py
RefactoringTool: ./portality/lib/es_data_mapping.py
RefactoringTool: ./portality/lib/es_query_http.py
RefactoringTool: ./portality/lib/isolang.py
RefactoringTool: ./portality/lib/modeldoc.py
RefactoringTool: ./portality/lib/normalise.py
RefactoringTool: ./portality/lib/plugin.py
RefactoringTool: ./portality/lib/query_filters.py
RefactoringTool: ./portality/lib/report_to_csv.py
RefactoringTool: ./portality/lib/swagger.py
RefactoringTool: ./portality/migrate/1089_edit_field_by_query/regex_http_to_https.py
RefactoringTool: ./portality/migrate/1196_publisher_struct/operations.py
RefactoringTool: ./portality/migrate/1303_nonexistent_editors_assigned/nonexistent_editors_assigned.py
RefactoringTool: ./portality/migrate/1390_lcc_regen_subjects/operations.py
RefactoringTool: ./portality/migrate/1667_remove_email_from_articles/scrub_emails.py
RefactoringTool: ./portality/migrate/20180106_1463_ongoing_updates/operations.py
RefactoringTool: ./portality/migrate/20180106_1463_ongoing_updates/sync_journals_applications.py
RefactoringTool: ./portality/migrate/972_appl_form_changes/appl_form_changes.py
RefactoringTool: ./portality/migrate/continuations/clean_struct.py
RefactoringTool: ./portality/migrate/continuations/extract_continuations.py
RefactoringTool: ./portality/migrate/continuations/mappings.py
RefactoringTool: ./portality/migrate/continuations/restructure_archiving_policy.py
RefactoringTool: ./portality/migrate/continuations/test_migration.py
RefactoringTool: ./portality/migrate/delete_field_from_type/scrub_field.py
RefactoringTool: ./portality/migrate/licenses/missed_journals.py
RefactoringTool: ./portality/migrate/licenses/update_licenses.py
RefactoringTool: ./portality/migrate/p1p2/country_cleanup.py
RefactoringTool: ./portality/migrate/p1p2/emails.py
RefactoringTool: ./portality/migrate/p1p2/flushuploads.py
RefactoringTool: ./portality/migrate/p1p2/journalowners.py
RefactoringTool: ./portality/migrate/p1p2/journalrestructure.py
RefactoringTool: ./portality/migrate/p1p2/journals_and_suggestions_text_tweaks.py
RefactoringTool: ./portality/migrate/p1p2/loadlcc.py
RefactoringTool: ./portality/migrate/p1p2/publisheremails.py
RefactoringTool: ./portality/migrate/p1p2/remove-0000-issns.py
RefactoringTool: ./portality/migrate/p1p2/suggestionrestructure.py
RefactoringTool: ./portality/migrate/p1p2/uploadcorrections.py
RefactoringTool: ./portality/migrate/p1p2/uploadedfilenames.py
RefactoringTool: ./portality/migrate/p1p2/uploadedxml.py
RefactoringTool: ./portality/migrate/p1p2/userroles.py
RefactoringTool: ./portality/migrate/p2oe/apcdata.py
RefactoringTool: ./portality/migrate/p2oe/uncontinue.py
RefactoringTool: ./portality/migrate/st2cl/cluster.py
RefactoringTool: ./portality/migrate/st2cl/cluster2.py
RefactoringTool: ./portality/migrate/st2cl/emails.py
RefactoringTool: ./portality/migrate/st2cl/migrate.py
RefactoringTool: ./portality/migrate/st2cl/pages.py
RefactoringTool: ./portality/migrate/subjects/remove_duplicate_subjects.py
RefactoringTool: ./portality/migrate/tick/check_tick.py
RefactoringTool: ./portality/models/__init__.py
RefactoringTool: ./portality/models/account.py
RefactoringTool: ./portality/models/article.py
RefactoringTool: ./portality/models/atom.py
RefactoringTool: ./portality/models/background.py
RefactoringTool: ./portality/models/bibjson.py
RefactoringTool: ./portality/models/cache.py
RefactoringTool: ./portality/models/editors.py
RefactoringTool: ./portality/models/history.py
RefactoringTool: ./portality/models/journal.py
RefactoringTool: ./portality/models/lcc.py
RefactoringTool: ./portality/models/lock.py
RefactoringTool: ./portality/models/oaipmh.py
RefactoringTool: ./portality/models/openurl.py
RefactoringTool: ./portality/models/provenance.py
RefactoringTool: ./portality/models/search.py
RefactoringTool: ./portality/models/shared_structs.py
RefactoringTool: ./portality/models/suggestion.py
RefactoringTool: ./portality/models/uploads.py
RefactoringTool: ./portality/scripts/accounts_with_marketing_consent.py
RefactoringTool: ./portality/scripts/accounts_with_missing_api_role.py
RefactoringTool: ./portality/scripts/accounts_with_missing_passwords.py
RefactoringTool: ./portality/scripts/anon_export.py
RefactoringTool: ./portality/scripts/anon_import.py
RefactoringTool: ./portality/scripts/applicationrm.py
RefactoringTool: ./portality/scripts/article_cleanup_sync.py
RefactoringTool: ./portality/scripts/article_duplicate_analysis.py
RefactoringTool: ./portality/scripts/article_duplicate_report.py
RefactoringTool: ./portality/scripts/articledump.py
RefactoringTool: ./portality/scripts/articleload.py
RefactoringTool: ./portality/scripts/articlerm.py
RefactoringTool: ./portality/scripts/async_workflow_notifications.py
RefactoringTool: ./portality/scripts/change_application_status.py
RefactoringTool: ./portality/scripts/createuser.py
RefactoringTool: ./portality/scripts/generate_iso639b_language_code_schema.py
RefactoringTool: ./portality/scripts/history_dirs_reports.py
RefactoringTool: ./portality/scripts/history_records_analyse.py
RefactoringTool: ./portality/scripts/history_records_assemble.py
RefactoringTool: ./portality/scripts/inconsistent_journal_prov_application.py
RefactoringTool: ./portality/scripts/journalcsv.py
RefactoringTool: ./portality/scripts/manage_background_jobs.py
RefactoringTool: ./portality/scripts/missing_q14q18.py
RefactoringTool: ./portality/scripts/missing_quick_reject_emails.py
RefactoringTool: ./portality/scripts/news.py
RefactoringTool: ./portality/scripts/not_in_doaj_with_articles.py
RefactoringTool: ./portality/scripts/orphaned_datasets.py
RefactoringTool: ./portality/scripts/prepare_delete_request.py
RefactoringTool: ./portality/scripts/prune_es_backups.py
RefactoringTool: ./portality/scripts/prune_marvel.py
RefactoringTool: ./portality/scripts/public_data_dump.py
RefactoringTool: ./portality/scripts/publisher_email_in_doaj.py
RefactoringTool: ./portality/scripts/rejected_applications.py
RefactoringTool: ./portality/scripts/reports.py
RefactoringTool: ./portality/scripts/request_es_backup.py
RefactoringTool: ./portality/scripts/sitemap.py
RefactoringTool: ./portality/scripts/sync_doaj_records.py
RefactoringTool: ./portality/scripts/update_mapping_reindex.py
RefactoringTool: ./portality/tasks/article_bulk_delete.py
RefactoringTool: ./portality/tasks/article_cleanup_sync.py
RefactoringTool: ./portality/tasks/article_duplicate_report.py
RefactoringTool: ./portality/tasks/async_workflow_notifications.py
RefactoringTool: ./portality/tasks/check_latest_es_backup.py
RefactoringTool: ./portality/tasks/consumer_long_running.py
RefactoringTool: ./portality/tasks/consumer_main_queue.py
RefactoringTool: ./portality/tasks/ingestarticles.py
RefactoringTool: ./portality/tasks/journal_bulk_delete.py
RefactoringTool: ./portality/tasks/journal_bulk_edit.py
RefactoringTool: ./portality/tasks/journal_csv.py
RefactoringTool: ./portality/tasks/journal_in_out_doaj.py
RefactoringTool: ./portality/tasks/prune_es_backups.py
RefactoringTool: ./portality/tasks/public_data_dump.py
RefactoringTool: ./portality/tasks/read_news.py
RefactoringTool: ./portality/tasks/redis_huey.py
RefactoringTool: ./portality/tasks/reporting.py
RefactoringTool: ./portality/tasks/request_es_backup.py
RefactoringTool: ./portality/tasks/sitemap.py
RefactoringTool: ./portality/tasks/suggestion_bulk_edit.py
RefactoringTool: ./portality/ui/messages.py
RefactoringTool: ./portality/view/account.py
RefactoringTool: ./portality/view/admin.py
RefactoringTool: ./portality/view/api_v1.py
RefactoringTool: ./portality/view/atom.py
RefactoringTool: ./portality/view/doaj.py
RefactoringTool: ./portality/view/doajservices.py
RefactoringTool: ./portality/view/editor.py
RefactoringTool: ./portality/view/forms.py
RefactoringTool: ./portality/view/oaipmh.py
RefactoringTool: ./portality/view/openurl.py
RefactoringTool: ./portality/view/publisher.py
RefactoringTool: ./portality/view/query.py
RefactoringTool: ./portality/view/status.py
RefactoringTool: ./scratchpad/duplicate_articles/duplicate_articles.py
RefactoringTool: ./scratchpad/issue171/delete_articles.py
RefactoringTool: ./scratchpad/issue239/date_histogram.py
RefactoringTool: ./scratchpad/issue277/dump_articles.py
RefactoringTool: ./scratchpad/issue290/orphan_issns.py
RefactoringTool: ./scratchpad/sync/sync_from_remote.py
RefactoringTool: There were 2 errors:
RefactoringTool: Can't parse ./scratchpad/rate_limit/rate_test.py: ParseError: bad input: type=3, value=u"'https://doaj.org/api/v1/search/journals/issn:{issn}'", context=('', (14, 25))
RefactoringTool: Can't parse ./scratchpad/rate_limit/rate_test2.py: ParseError: bad input: type=3, value=u"'codes_555.csv'", context=('', (10, 11))
ng))
+print(missing)
 
 get_query = {
     "query" : {
@@ -66,7 +66,7 @@
         title = r.get("bibjson", {}).get("journal", {}).get("title")
         info.append((m, title))
 
-print info
+print(info)
 
 name_query = {
     "query" : {
@@ -92,6 +92,6 @@
     for r in res:
         issns = r.get("index", {}).get("issn", [])
         new_info.append((i, n, issns))
-        print i, n, "=>", issns
+        print(i, n, "=>", issns)
 
-print unfound
+print(unfound)
